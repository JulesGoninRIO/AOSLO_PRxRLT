{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3a471e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a40ef",
   "metadata": {},
   "source": [
    "### Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ce513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure the parent directory is in the system path for module imports\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "from pytest import param\n",
    "from zmq import has"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7054f",
   "metadata": {},
   "source": [
    "### Plot Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotx\n",
    "\n",
    "# plt.style.use('science')  # Use ggplot style for all plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)  # Default figure size\n",
    "plt.rcParams['figure.dpi'] = 300  # Default figure dpi\n",
    "plt.rcParams['font.size'] = 12  # Default font size\n",
    "plt.rcParams['lines.linewidth'] = 2  # Default line width\n",
    "plt.rcParams['axes.labelsize'] = 14  # Default label size\n",
    "plt.rcParams['axes.titlesize'] = 16  # Default title size\n",
    "plt.rcParams['xtick.labelsize'] = 12  # Default x-tick label size\n",
    "plt.rcParams['ytick.labelsize'] = 12  # Default y-tick label size\n",
    "plt.rcParams['legend.fontsize'] = 12  # Default legend font size\n",
    "plt.rcParams['figure.titlesize'] = 18  # Default figure title size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee85ec",
   "metadata": {},
   "source": [
    "## Gathering subjects' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad41af",
   "metadata": {},
   "source": [
    "### Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c67b8",
   "metadata": {},
   "source": [
    "data structure to store subject data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class SubjectData:\n",
    "    name: str = None\n",
    "    pid: str = None\n",
    "    nb: int = None\n",
    "    session: str = None\n",
    "\n",
    "    width_nas: float = None\n",
    "    width_tem: float = None\n",
    "    width_inf: float = None\n",
    "    width_sup: float = None\n",
    "    max_slope_nas: float = None\n",
    "    max_slope_tem: float = None\n",
    "    max_slope_inf: float = None\n",
    "    max_slope_sup: float = None\n",
    "\n",
    "    oct_bump_X: float = None\n",
    "    oct_bump_Y: float = None\n",
    "    oct_width_X: float = None\n",
    "    oct_width_Y: float = None\n",
    "    oct_max_slope: float = None\n",
    "    oct_depth: float = None\n",
    "    oct_flatness: float = None\n",
    "\n",
    "    age: float = None\n",
    "    axial_length: float = None\n",
    "    spherical_equiv: float = None\n",
    "    sex: int = None\n",
    "\n",
    "    eccs: np.ndarray = None\n",
    "    density_X: pd.Series = None\n",
    "    density_Y: pd.Series = None\n",
    "    density_fit_X: pd.Series = None\n",
    "    density_fit_Y: pd.Series = None\n",
    "\n",
    "    cvi_X: pd.Series = None\n",
    "    cvi_Y: pd.Series = None\n",
    "    gcl_ipl_X: pd.Series = None\n",
    "    gcl_ipl_Y: pd.Series = None\n",
    "    onl_X: pd.Series = None\n",
    "    onl_Y: pd.Series = None\n",
    "    inl_opl_X: pd.Series = None\n",
    "    inl_opl_Y: pd.Series = None\n",
    "    rnfl_X: pd.Series = None\n",
    "    rnfl_Y: pd.Series = None\n",
    "    chrd_X: pd.Series = None\n",
    "    chrd_Y: pd.Series = None\n",
    "    pr_rpe_X: pd.Series = None\n",
    "    pr_rpe_Y: pd.Series = None\n",
    "    os_X: pd.Series = None\n",
    "    os_Y: pd.Series = None\n",
    "\n",
    "    nb_cones: float = None\n",
    "    nb_cones_fit: float = None\n",
    "\n",
    "    width_gcl_X: float = None\n",
    "    width_gcl_Y: float = None\n",
    "    min_thick_gcl: float = None\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoveaParams:\n",
    "    \"\"\"Class for storing fovea 3D fitted parameters.\"\"\"\n",
    "    # Patient information\n",
    "    subject: str\n",
    "    patient_id: str\n",
    "    subject_folder: str\n",
    "    trial_name: str\n",
    "    age: Optional[int] = None\n",
    "\n",
    "    # Fitted parameters\n",
    "    A00: Optional[float] = None\n",
    "    A10: Optional[float] = None\n",
    "    A01: Optional[float] = None\n",
    "    A20: Optional[float] = None\n",
    "    A02: Optional[float] = None\n",
    "    A11: Optional[float] = None\n",
    "    foveal_depth: Optional[float] = None\n",
    "    foveal_center_X: Optional[float] = None\n",
    "    foveal_width_X: Optional[float] = None\n",
    "    foveal_center_Y: Optional[float] = None\n",
    "    foveal_width_Y: Optional[float] = None\n",
    "    foveal_max_slope: Optional[float] = None\n",
    "    foveal_flatness: Optional[float] = None\n",
    "    foveal_volume: Optional[float] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54482948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# here to avoid having to rerun the pipeline for -\n",
    "# all subjects everytime i want to test something on the model.\n",
    " \n",
    "# Since the list of subjects is ordered by strings , it goes from 10 to 100 to 103 etc...\n",
    "# which requires a bit of work to get the first 5 subjects\n",
    "\n",
    "#It will later be used to extract the first 5 subjects from the list of subject_data\n",
    "\n",
    "take_first_five = False\n",
    "first_five_subjects = [\"Subject10\",\"Subject100\",\"Subject101\",\"Subject104\",\"Subject105\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9893f9",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716aec0",
   "metadata": {},
   "source": [
    "#### Foveal Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1979fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fovea_data(base_path: str) -> List[FoveaParams]:\n",
    "    \"\"\"\n",
    "    Extract fovea parameters from CSV files with known structure.\n",
    "    \n",
    "    Args:\n",
    "        base_path: Path to the base directory containing subject folders\n",
    "        (subjfolder/trialfolder/layer_new/fovea_3d_fitted_params.csv)\n",
    "    \n",
    "    Returns:\n",
    "        List of FoveaParams objects, one for each found CSV file\n",
    "    \"\"\"\n",
    "    fovea_data = []\n",
    "    \n",
    "    # Get only the subject directories (directories starting with \"Subject\")\n",
    "    try:\n",
    "        subject_dirs = [d for d in os.listdir(base_path) \n",
    "                      if os.path.isdir(os.path.join(base_path, d)) and d.startswith(\"Subject\")]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Base path not found: {base_path}\")\n",
    "        return []\n",
    "    \n",
    "    # For each subject directory, get the session directories\n",
    "    for subject_dir in subject_dirs:\n",
    "        subject_path = os.path.join(base_path, subject_dir)\n",
    "        \n",
    "        # Extract subject number\n",
    "        import re\n",
    "        subject_match = re.search(r'Subject(\\d+)', subject_dir)\n",
    "        if not subject_match:\n",
    "            continue\n",
    "            \n",
    "        subject_num = subject_match.group(1)\n",
    "        patient_id = f\"{subject_num}\"\n",
    "        \n",
    "        try:\n",
    "            # Get session directories (directories starting with \"Session\")\n",
    "            session_dirs = [d for d in os.listdir(subject_path) \n",
    "                          if os.path.isdir(os.path.join(subject_path, d)) and d.startswith(\"Session\")]\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        \n",
    "        # For each session directory, check if the CSV file exists\n",
    "        for session_dir in session_dirs:\n",
    "            session_path = os.path.join(subject_path, session_dir)\n",
    "            csv_path = os.path.join(session_path, \"layer_new\", \"fovea_3d_fitted_params.csv\")\n",
    "            \n",
    "            # Check if the CSV file exists\n",
    "            if os.path.isfile(csv_path):\n",
    "                # try:\n",
    "                # Read CSV file\n",
    "                df = pd.read_csv(csv_path, sep=';', header=None, names=['param', 'value'])\n",
    "                \n",
    "                # Create basic FoveaParams object with patient info\n",
    "                fovea_obj = FoveaParams(\n",
    "                    patient_id=str(patient_id),\n",
    "                    subject=f\"Subject{patient_id}\",\n",
    "                    subject_folder=subject_dir,\n",
    "                    trial_name=session_dir\n",
    "                )\n",
    "                print(f\"Processing file: {csv_path} for patient {patient_id}\")\n",
    "                \n",
    "                # Fill in parameter values\n",
    "                for _, row in df.iterrows():\n",
    "                    param_name = row['param']\n",
    "                    param_value = row['value']\n",
    "                    print(f\"Processing {param_name} with value {param_value} for patient {patient_id}\")\n",
    "                    \n",
    "                    if param_value == \"params\":\n",
    "                        print(f\"Skipping parameter {param_name} for patient {patient_id} as it is 'params'\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if this parameter exists in our class\n",
    "                    if hasattr(fovea_obj, param_name) or hasattr(fovea_obj, f\"foveal_{param_name}\"):\n",
    "                        print(f\"Setting {param_name} for patient {patient_id}\")\n",
    "                        try:\n",
    "                            # Convert to float and set attribute\n",
    "                            print(f\"Trying to set {param_name} for patient {patient_id}\")\n",
    "                            setattr(fovea_obj, param_name, float(param_value))\n",
    "                            print(f\"Successfully set {param_name} for patient {patient_id}\")\n",
    "                        except:\n",
    "                            \n",
    "                            print(f\"Error setting {param_name} for patient {patient_id}: {param_value}\")\n",
    "\n",
    "                    if hasattr(fovea_obj, f\"foveal_{param_name}\"):\n",
    "                        try:\n",
    "                            print(f\"Trying to set foveal_{param_name} for patient {patient_id}\")\n",
    "                            setattr(fovea_obj, f\"foveal_{param_name}\", float(param_value))\n",
    "\n",
    "                        except:\n",
    "                            print(f\"Skipping parameter {param_name} for patient {patient_id} \")\n",
    "\n",
    "                            pass\n",
    "                    \n",
    "                fovea_data.append(fovea_obj)\n",
    "                # except Exception as e:\n",
    "                    # print(f\"Error processing file {csv_path}: {str(e)}\")\n",
    "    \n",
    "    return fovea_data\n",
    "\n",
    "def save_to_dataframe(fovea_data: List[FoveaParams], output_file: str = \"fovea_parameters.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert list of FoveaParams objects to a pandas DataFrame and save to CSV.\n",
    "    \n",
    "    Args:\n",
    "        fovea_data: List of FoveaParams objects\n",
    "        output_file: Path to save the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing all fovea data\n",
    "    \"\"\"\n",
    "    # Convert to list of dictionaries\n",
    "    data_dicts = [vars(f) for f in fovea_data]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_dicts)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586969b",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e436c87",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from src.cell.analysis.constants import MM_PER_DEGREE\n",
    "from src.cell.layer.helpers import gaussian_filter_nan\n",
    "from src.configs.parser import Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cd5bfd",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parser.initialize()\n",
    "\n",
    "subjects_sessions = [[int(n) for n in s.strip().split()] for s in open('../src/processed.txt').readlines()] \n",
    "\n",
    "\n",
    "try:\n",
    "    sheet = pd.ExcelFile(r'V:\\Studies\\AOSLO\\data\\cohorts\\AOSLO healthy\\DATA_HC+DM.xlsx').parse('Healthy', header=0, nrows=45, index_col=0)\n",
    "    sheet.index = sheet.index.map(lambda x: f'Subject{x}')\n",
    "    age_dict = ((sheet['Date of visit'] - sheet['DDN']).dt.days / 365).to_dict()\n",
    "    axial_dict = sheet['AL D (mm)'].where(sheet['Laterality'] == 'OD', sheet['AL G (mm)']).to_dict()\n",
    "    spherical_dict = sheet['Equi Sph D'].where(sheet['Laterality'] == 'OD', sheet['Equi Sph G']).to_dict()\n",
    "    sex_dict = sheet['Sexe'].map(lambda x: 1 if x == 'F' else 0).to_dict()\n",
    "except:\n",
    "    # if the excel file is not found, use a hardcoded dictionary\n",
    "    age_dict = {}\n",
    "base_path = Path(r'P:\\AOSLO\\_automation\\_PROCESSED\\Photoreceptors\\Healthy\\_Results')\n",
    "\n",
    "# look-up table for subject and session numbers\n",
    "\n",
    "\n",
    "# subject for which OCTs are tilted (white dot is not well aligned with PR+RPE peak)\n",
    "# see explanation in `PRxRLT_expmanual.ipynb`\n",
    "oct_to_exclude = {\n",
    "    13, 18, 20, 25, 26, 30, 35, 42, 46, 66, 100, 105,\n",
    "} \n",
    "\n",
    "\n",
    "subjects_data: List[SubjectData] = []\n",
    "for subject_n, session_n in subjects_sessions:\n",
    "    if subject_n in oct_to_exclude:\n",
    "        continue\n",
    "\n",
    "    sd = SubjectData()\n",
    "    sd.name = f'Subject{subject_n}'\n",
    "    sd.pid = f'AOHC_{subject_n}'\n",
    "    sd.nb = subject_n\n",
    "    sd.session = f'Session{session_n}'\n",
    "\n",
    "    #\n",
    "    path = base_path / sd.name / sd.session\n",
    "    print(f'Loading {sd.name} {sd.session}...')\n",
    "\n",
    "    # record subject's metadata from the excel sheet\n",
    "    sd.age = age_dict[sd.name]\n",
    "    sd.axial_length = axial_dict[sd.name]\n",
    "    sd.spherical_equiv = spherical_dict[sd.name]\n",
    "    sd.sex = sex_dict[sd.name]\n",
    "\n",
    "    # record foveal shape parameters (populated by `src/save_layer_features.ipynb`)\n",
    "    df_oct = pd.read_csv(path / Parser.get_layer_thickness_dir() / 'fovea_3d_fitted_params.csv', sep=';', index_col=0)\n",
    "    sd.oct_bump_X = df_oct.loc['A20', 'params']\n",
    "    sd.oct_bump_Y = df_oct.loc['A02', 'params']\n",
    "    sd.oct_width_X = df_oct.loc['width_X', 'params'] * np.sqrt(2 * 2.8) / MM_PER_DEGREE # in °\n",
    "    sd.oct_width_Y = df_oct.loc['width_Y', 'params'] * np.sqrt(2 * 2.8) / MM_PER_DEGREE # in °\n",
    "    sd.oct_max_slope = df_oct.loc['max_slope', 'params']\n",
    "    sd.oct_depth = df_oct.loc['depth', 'params'] # in mm\n",
    "    sd.oct_flatness = df_oct.loc['flatness', 'params']\n",
    "    # sd.oct_volume = df_oct.loc['volume', 'params']\n",
    "\n",
    "    # record cone density and fitted parameters (populated by `src/cell/analysis/density_analysis_pipeline_manager.py`)\n",
    "    df_density = pd.read_csv(path / Parser.get_density_analysis_dir() / 'densities.csv', sep=';', index_col=0)\n",
    "    df_raw_density_x = pd.read_csv(path / Parser.get_density_analysis_dir() / 'densities_raw_x.csv', sep=';', index_col=0)\n",
    "    df_raw_density_y = pd.read_csv(path / Parser.get_density_analysis_dir() / 'densities_raw_y.csv', sep=';', index_col=0)\n",
    "    \n",
    "    sd.width_nas = df_density['width_nasal'].iloc[0]\n",
    "    sd.width_tem = df_density['width_temporal'].iloc[0]\n",
    "    sd.width_inf = df_density['width_inferior'].iloc[0]\n",
    "    sd.width_sup = df_density['width_superior'].iloc[0]\n",
    "    sd.max_slope_nas = df_density['max_slope_nasal'].iloc[0]\n",
    "    sd.max_slope_tem = df_density['max_slope_temporal'].iloc[0]\n",
    "    sd.max_slope_inf = df_density['max_slope_inferior'].iloc[0]\n",
    "    sd.max_slope_sup = df_density['max_slope_superior'].iloc[0]\n",
    "    sd.density_X = df_density['dens_smthd_X']\n",
    "    sd.density_Y = df_density['dens_smthd_Y']\n",
    "    sd.density_fit_X = df_density['dens_fit_X']\n",
    "    sd.density_fit_Y = df_density['dens_fit_Y']\n",
    "    \n",
    "    sd.eccs = df_density.index.to_numpy()\n",
    "\n",
    "    # record layer thicknesses (populated by `src/save_layer_features.ipynb`)\n",
    "    df_thick = pd.read_csv(path / Parser.get_density_analysis_dir() / 'results.csv', sep=',', index_col=0, skiprows=1).query('-10 <= index <= 10')\n",
    "    sd.cvi_X = df_thick['CVI_X']\n",
    "    sd.cvi_Y = df_thick['CVI_Y']\n",
    "    sd.gcl_ipl_X = df_thick['GCL+IPL_X']\n",
    "    sd.gcl_ipl_Y = df_thick['GCL+IPL_Y']\n",
    "    sd.onl_X = df_thick['ONL_X']\n",
    "    sd.onl_Y = df_thick['ONL_Y']\n",
    "    sd.inl_opl_X = df_thick['INL+OPL_X']\n",
    "    sd.inl_opl_Y = df_thick['INL+OPL_Y']\n",
    "    sd.rnfl_X = df_thick['RNFL_X']\n",
    "    sd.rnfl_Y = df_thick['RNFL_Y']\n",
    "    sd.chrd_X = df_thick['Choroid_X']\n",
    "    sd.chrd_Y = df_thick['Choroid_Y']\n",
    "    sd.pr_rpe_X = df_thick['PhotoR+RPE_X']\n",
    "    sd.pr_rpe_Y = df_thick['PhotoR+RPE_Y']\n",
    "    sd.os_X = df_thick['OS_X']\n",
    "    sd.os_Y = df_thick['OS_Y']\n",
    "\n",
    "    subjects_data.append(sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b37ce",
   "metadata": {},
   "source": [
    "#### Populating Additional fields based on the previously gathered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_cones(ecc: np.ndarray, dens_X: pd.Series, dens_Y: pd.Series, radius: float, smoothen: bool = True) -> float:\n",
    "    \n",
    "    '''\n",
    "    Given the cone density profiles along the X and Y axes, compute the total number of cones within a disk of radius `radius` (in degree) centered at the fovea by linearly interpolating (radially) the density profiles and integrating over the disk.\n",
    "    '''\n",
    "    smthd_x = gaussian_filter_nan(dens_X, sigma=4) if smoothen else dens_X.to_numpy()\n",
    "    smthd_y = gaussian_filter_nan(dens_Y, sigma=4) if smoothen else dens_Y.to_numpy()\n",
    "   \n",
    "    x_amax = np.nanargmax(smthd_x)\n",
    "    p = np.polyfit(ecc[x_amax-2:x_amax+3], smthd_x[x_amax-2:x_amax+3], 2)\n",
    "    x_amax = -p[1] / (2 * p[0])\n",
    "\n",
    "    y_amax = np.nanargmax(smthd_y)\n",
    "    p = np.polyfit(ecc[y_amax-2:y_amax+3], smthd_y[y_amax-2:y_amax+3], 2)\n",
    "    y_amax = -p[1] / (2 * p[0])\n",
    "\n",
    "    R = np.linspace(0.0001, radius, 500) # radius in degrees\n",
    "    disk = np.r_[\n",
    "        np.interp(x_amax + R, ecc, smthd_x),\n",
    "        np.interp(x_amax - R, ecc, smthd_x),\n",
    "        np.interp(y_amax + R, ecc, smthd_y),\n",
    "        np.interp(y_amax - R, ecc, smthd_y)\n",
    "    ]\n",
    "    \n",
    "    norm_coef = MM_PER_DEGREE**2 * 2 * np.pi\n",
    "    # integrate cone density over disk to get total nb of cones\n",
    "    return norm_coef * np.trapz(np.nanmean(disk, axis=0) * R, R)\n",
    "\n",
    "RADIUS = 3.33 # degree\n",
    "for sd in subjects_data:\n",
    "    sd.nb_cones = get_nb_cones(sd.eccs, sd.density_X, sd.density_Y, radius = RADIUS)\n",
    "    sd.nb_cones_fit = get_nb_cones(sd.eccs, sd.density_fit_X, sd.density_fit_Y, radius = RADIUS, smoothen=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff70b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "def adjust_flat(gcl_data: np.ndarray, peak_left: int, peak_right: int) -> np.ndarray:\n",
    "    slope = (gcl_data[peak_right] - gcl_data[peak_left]) / (peak_right - peak_left)\n",
    "    transformed_gcl = gcl_data - slope * (np.arange(len(gcl_data)) - peak_left)\n",
    "    return transformed_gcl\n",
    "\n",
    "def get_gcl_width(gcl: pd.Series) -> Tuple[float, float]:\n",
    "    '''\n",
    "    Given the GCL+IPL thickness profile, compute the width of the pit as well as the minimum thickness of the layer. Here, the width of the pit is defined as the distance between the two points where the thickness is 20% of the depth of the pit. The depth of the pit is defined as the difference between the thickness surrounding the pit and the thickness at the pit's bottom.\n",
    "    '''\n",
    "    # name = gcl.name\n",
    "    gcl_to_plot = gcl.copy()\n",
    "    eccs = gcl[np.abs(gcl.index) <= 6].index.to_numpy()\n",
    "    gcl = gcl.interpolate(method='polynomial', order=1)[eccs].to_numpy()\n",
    "    # plt.plot(eccs, gcl, label=name)\n",
    "    smooth_param = 3\n",
    "    peak_left = peak_right = []\n",
    "    while not (len(peak_left) >= 1 and len(peak_right) >= 1) and smooth_param < 10:\n",
    "        smoothed_gcl = gaussian_filter_nan(gcl, smooth_param)\n",
    "        peaks = find_peaks(smoothed_gcl)[0]\n",
    "        peak_left  = [peak for peak in peaks if peak < len(smoothed_gcl) / 3]\n",
    "        peak_right = [peak for peak in peaks if peak > 2 * len(smoothed_gcl) / 3]\n",
    "        smooth_param += 1\n",
    "    assert len(peak_left) >= 1 and len(peak_right) >= 1, f'No peaks found for {gcl.name}'\n",
    "    peak_left = round(np.mean(peak_left))   \n",
    "    peak_right = round(np.mean(peak_right))\n",
    "    adjusted_gcl = adjust_flat(gcl, peak_left, peak_right)\n",
    "    smoothed_aj_gcl = gaussian_filter_nan(adjusted_gcl, 2)\n",
    "\n",
    "    y_min = np.nanmin(smoothed_aj_gcl[peak_left:peak_right])\n",
    "    y_target = y_min + (smoothed_aj_gcl[peak_left] - y_min) / 5\n",
    "    intercepts = np.where(np.diff(np.sign(smoothed_aj_gcl - y_target)))[0]\n",
    "    leftmost = eccs[intercepts[0]]\n",
    "    rightmost = eccs[intercepts[-1]+1]\n",
    "    width_pit_gcl = rightmost - leftmost\n",
    "\n",
    "    indicies = np.argpartition(gcl, 10)[:10]\n",
    "    p = np.polyfit(eccs[indicies], gcl[indicies], 2)\n",
    "    if p[0] == 0:\n",
    "    #     # gcl_to_plot.plot()\n",
    "    #     plt.plot(eccs, gcl, label='gcl')\n",
    "        plt.plot(np.sort(eccs[indicies]), np.polyval(p, np.sort(eccs[indicies])), '--')\n",
    "    min_thickness_gcl = np.polyval(p, -p[1] / (2 * p[0]))\n",
    "    return width_pit_gcl, min_thickness_gcl\n",
    "\n",
    "for sd in subjects_data:\n",
    "    width_gcl_x, min_thick_x = get_gcl_width(sd.gcl_ipl_X)\n",
    "    width_gcl_y, min_thick_y = get_gcl_width(sd.gcl_ipl_Y)\n",
    "    sd.width_gcl_X = width_gcl_x\n",
    "    sd.width_gcl_Y = width_gcl_y\n",
    "    sd.min_thick_gcl = min(min_thick_x, min_thick_y)\n",
    "    # print(f'{sd.name:>10}: {width_gcl_x:.2f}°, {depth_gcl_x:.4f}, {width_gcl_y:.2f}°, {depth_gcl_y:.4f}')\n",
    "    # plt.xlim(-6, 6)\n",
    "    # plt.legend()\n",
    "    # plt.title(sd.name)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "eccs = subjects_data[0].eccs\n",
    "layer_names = ['rnfl', 'gcl_ipl', 'inl_opl', 'onl', 'pr_rpe', 'os', 'chrd']\n",
    "names_r = {'rnfl': 'RNFL', 'gcl_ipl': 'GCL+IPL', 'inl_opl': 'INL+OPL', 'onl': 'ONL', 'pr_rpe': 'PhotoR+RPE', 'os': 'OS', 'chrd': 'Choroid', 'cones': 'Cone Density'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3fbae0",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b7a1e",
   "metadata": {},
   "source": [
    "### Function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c47904",
   "metadata": {},
   "source": [
    "##### Corr matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7eb104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "\n",
    "df = pd.DataFrame({k: [getattr(sd, k) for sd in subjects_data] for k,t in SubjectData.__annotations__.items() if t in (int, float)}, index=[sd.name for sd in subjects_data])\n",
    "print(df)\n",
    "def corr_sig(df: pd.DataFrame, drop=['nb']) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    cols = df.columns.drop(drop).to_list()\n",
    "    corr_matrix = np.zeros(shape=(len(cols), len(cols)))\n",
    "    p_matrix = np.ones_like(corr_matrix)\n",
    "    for col in cols:\n",
    "        for col2 in cols:\n",
    "            corr , p = scipy.stats.pearsonr(df[col],df[col2])\n",
    "            corr_matrix[cols.index(col),cols.index(col2)] = corr\n",
    "            p_matrix[cols.index(col),cols.index(col2)] = p\n",
    "    return corr_matrix, p_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e0f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cd_on_range(cone_density_fd, left, right):\n",
    "        range_eccs = np.argwhere((left <= eccs) & (eccs <= right)).flatten()\n",
    "        return np.mean(cone_density_fd[:, range_eccs], axis=1)\n",
    "    \n",
    "def plot_subjects_data(df):\n",
    "    \"\"\"\n",
    "    Plots the cone density values for each subject from the given DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame where each row corresponds to a subject, \n",
    "        and each column represents a specific eccentricity range.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Get the x-axis labels (eccentricity ranges)\n",
    "    x_labels = df.columns\n",
    "    x_values = np.arange(len(x_labels))  # Numerical representation for plotting\n",
    "\n",
    "    # Plot each subject's data in a different color\n",
    "    for subject_id in df.index:\n",
    "        plt.plot(x_values, df.loc[subject_id], marker='o', linestyle='-', label=f\"Subject {subject_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bbe752",
   "metadata": {},
   "source": [
    "##### Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065c253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.shared.helpers.direction import Direction\n",
    "\n",
    "\n",
    "def preprocess_functional_feature(data: np.ndarray, standardization: str = 'inter') -> np.ndarray:\n",
    "    '''\n",
    "    Preprocess a functional feature (functional feature such as cone density or layer thickness, for which the feature is a function of eccentricity) by (Z-)standardizing it.\n",
    "    Given `data` matrix should have shape (n_subjects, n_eccentricities).\n",
    "    \n",
    "    - For an intra-indivual analysis, use `standardization='intra'` to standardize within subjects (i.e. within each row). This removes inter-subject variability.\n",
    "    - For an inter-individual analysis, use `standardization='inter'` to standardize across subjects, eccentricity-wise (i.e. within each column). Removes eccentricity-level variability, focuses on between-patient trends\n",
    "    '''\n",
    "    if standardization == 'inter':\n",
    "        mean = np.mean(data, axis=0, keepdims=True)\n",
    "        std = np.std(data, axis=0, keepdims=True)\n",
    "        return (data - mean) / std\n",
    "    if standardization == 'intra':\n",
    "        mean = np.nanmean(data, axis=1, keepdims=True)\n",
    "        std = np.nanstd(data, axis=1, keepdims=True)\n",
    "        return (data - mean) / std\n",
    "    return data\n",
    "\n",
    "def preprocess_functional_data(direction: Direction, standardization: str = 'inter', toLog : bool = True) -> Dict[str, np.ndarray]:\n",
    "    '''\n",
    "    Preprocess functional data (e.g. cone density, layer thicknesses) for a given direction (X or Y) by (Z-)standardizing it.\n",
    "    '''\n",
    "\n",
    "    layer_fds = {\n",
    "        layer: preprocess_functional_feature(\n",
    "            np.array([getattr(s, f'{layer}_{direction.value}') for s in subjects_data]), standardization\n",
    "        )\n",
    "        for layer in layer_names\n",
    "    }\n",
    "    if toLog:\n",
    "        cone_density_fd = preprocess_functional_feature(\n",
    "            np.array([np.log(getattr(s, f'density_fit_{direction.value}')) for s in subjects_data]), standardization\n",
    "        )\n",
    "    else:\n",
    "        cone_density_fd = preprocess_functional_feature(\n",
    "            np.array([(getattr(s, f'density_fit_{direction.value}')) for s in subjects_data]), standardization\n",
    "        )\n",
    "\n",
    "    cone_density_nonfit = preprocess_functional_feature(\n",
    "            np.array([(getattr(s, f'density_{direction.value}')) for s in subjects_data]), standardization\n",
    "        )\n",
    "    # return {'cones': cone_density_fd, \"nonfit\": cone_density_nonfit, **layer_fds}\n",
    "    return {'cones': cone_density_fd, \"nonfit\": cone_density_nonfit, **layer_fds}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093acb0d",
   "metadata": {},
   "source": [
    "##### P-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "\n",
    "\n",
    "def kendall_pval(x,y):\n",
    "    return kendalltau(x,y)[1]\n",
    "\n",
    "def pearsonr_pval(x,y):\n",
    "    return pearsonr(x,y)[1]\n",
    "\n",
    "def spearmanr_pval(x,y):\n",
    "    return spearmanr(x,y, nan_policy = \"omit\")[1]\n",
    "\n",
    "\n",
    "def calculate_pvalues(df):\n",
    "    dfcols = pd.DataFrame(columns=df.columns)\n",
    "    pvalues = dfcols.transpose().join(dfcols, how='outer')\n",
    "    for r in df.columns:\n",
    "        for c in df.columns:\n",
    "            tmp = df[df[r].notnull() & df[c].notnull()]\n",
    "            pvalues[r][c] = round(spearmanr(tmp[r], tmp[c])[1], 4)  # Use spearmanr instead of pearsonr\n",
    "    return pvalues\n",
    "\n",
    "# Add significance asterisks\n",
    "def significance_marker(pval):\n",
    "    if pval < 0.001:\n",
    "        return \"***\"\n",
    "    elif pval < 0.01:\n",
    "        return \"**\"\n",
    "    elif pval < 0.05:\n",
    "        return \"*\"\n",
    "    else:\n",
    "        return \"\"  # No significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23798554",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, pv = corr_sig(df)\n",
    "sig_mask = pv < 0.05\n",
    "with(plt.style.context(matplotx.styles.pacoty)):\n",
    "    plt.figure(figsize=(14, 10), dpi=400)\n",
    "    # sns.heatmap(corr, mask=~sig_mask, annot=True, cmap='coolwarm', annot_kws={\"fontsize\":8}, xticklabels=df.columns, yticklabels=df.columns)\n",
    "    sns.heatmap(df.corr(), annot=True, cmap='coolwarm', annot_kws={\"fontsize\":8})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e44c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for direction in Direction:\n",
    "    cone_density_fd = preprocess_functional_data(direction, standardization='none', toLog=False)['cones']\n",
    "\n",
    "    pids = np.array([s.nb for s in subjects_data])\n",
    "    \n",
    "    _df = pd.DataFrame({\n",
    "        'Perifovea,\\n Temporal': get_cd_on_range(cone_density_fd, -10, -4.16),\n",
    "        'Parafovea,\\n Temporal': get_cd_on_range(cone_density_fd, -4.16, -2.8),\n",
    "        'Fovea,\\n temporal': get_cd_on_range(cone_density_fd,-2.8, -0.8),\n",
    "        'FAZ,\\n temporal': get_cd_on_range(cone_density_fd,-0.8, -0.6),\n",
    "        'Foveola': get_cd_on_range(cone_density_fd,-0.6, 0.6),\n",
    "        'FAZ,\\n nasal': get_cd_on_range(cone_density_fd,0.6, 0.8),\n",
    "        'Fovea,\\n nasal': get_cd_on_range(cone_density_fd,0.8, 2.8),\n",
    "        'Parafovea,\\n nasal': get_cd_on_range(cone_density_fd, 2.8, 4.16),\n",
    "        'Perifovea,\\n nasal': get_cd_on_range(cone_density_fd, 4.16, 10),\n",
    "\n",
    "    }, index=pids).sort_values(by='Foveola')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pvalues = calculate_pvalues(_df)\n",
    "\n",
    "    # Create a new annotation matrix based on p-values\n",
    "    annot = _df.corr(method=\"spearman\").applymap(lambda x: f\"{x:.2f}\")  # Spearman correlation values\n",
    "    for r in _df.columns:\n",
    "        for c in _df.columns:\n",
    "            pval = pvalues[r][c]\n",
    "            annot[r][c] += f\"\\n{significance_marker(pval)}\"\n",
    "\n",
    "    # Plot the heatmap with annotations including significance\n",
    "    sns.set_theme(font_scale=0.8)\n",
    "    mask = np.triu(np.ones_like(_df.corr(), dtype=bool))\n",
    "    for k in range(mask.shape[0]):\n",
    "        mask[k, k] = False\n",
    "    sns.heatmap(_df.corr(method=\"spearman\"), annot=annot, fmt=\"\", cmap='coolwarm', square=True, center=0, mask=mask,  annot_kws={'fontsize': 12, 'fontstyle': 'italic', 'color':'w', 'alpha': 1.0,\n",
    "                       })\n",
    "    plt.title(f'Correlation matrix of Cone Density at different eccentricities, {direction.value}-axis', fontsize=14)\n",
    "    plt.grid\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    # sns.set_theme(font_scale=0.8)\n",
    "    # sns.heatmap(_df.corr(method = spearmanr_pval), annot=True, fmt=\".6f\", cmap='coolwarm', square=True, center=0)\n",
    "    # plt.title(f'Correlation matrix of Cone Density at different eccentricities, {direction.value}-axis', fontsize=14)\n",
    "    # plt.show()\n",
    "\n",
    "    # Call the function to plot the data\n",
    "    plot_subjects_data(_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cone_density_fd_X = preprocess_functional_data(Direction.X, standardization='none', toLog = False)['cones']\n",
    "cone_density_fd_Y = preprocess_functional_data(Direction.Y, standardization='none', toLog = False)['cones']\n",
    "\n",
    "def get_cd_on_range(left, right, direction: Direction):\n",
    "    range_eccs = np.argwhere((left <= eccs) & (eccs <= right)).flatten()\n",
    "    return np.mean((cone_density_fd_X if direction == Direction.X else cone_density_fd_Y)[:, range_eccs], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Fovea/Perifovea etc equivalent for x/y axis\n",
    "\n",
    "_df = pd.DataFrame({\n",
    "        'X-axis, Perifoveal Temporal': get_cd_on_range(-10, -4.16, Direction.X),\n",
    "        'X-axis, Parafoveal Temporal': get_cd_on_range(-4.16, -2.8, Direction.X),\n",
    "        'X-axis, Foveal Temporal': get_cd_on_range(-2.8, -0.8, Direction.X),\n",
    "        'X-axis, FAZ Temporal': get_cd_on_range(-0.8, -0.6, Direction.X),\n",
    "        'X-axis, Foveola': get_cd_on_range(-0.6, 0.6, Direction.X),\n",
    "        'X-axis, FAZ Nasal': get_cd_on_range(0.6, 0.8, Direction.X),\n",
    "        'X-axis, Foveal Nasal': get_cd_on_range(0.8, 2.8, Direction.X),\n",
    "        'X-axis, Parafoveal Nasal': get_cd_on_range(2.8, 4.16, Direction.X),\n",
    "        'X-axis, Perifoveal Nasal': get_cd_on_range(4.16, 10, Direction.X),\n",
    "        'Y-axis, Perifoveal Superior': get_cd_on_range(-10, -4.16, Direction.Y),\n",
    "        'Y-axis, Parafoveal Superior': get_cd_on_range(-4.16, -2.8, Direction.Y),\n",
    "        'Y-axis, Foveal Superior': get_cd_on_range(-2.8, -0.8, Direction.Y),\n",
    "        'Y-axis, FAZ Superior': get_cd_on_range(-0.8, -0.6, Direction.Y),\n",
    "        'Y-axis, Foveola': get_cd_on_range(-0.6, 0.6, Direction.Y),\n",
    "        'Y-axis, FAZ Inferior': get_cd_on_range(0.6, 0.8, Direction.Y),\n",
    "        'Y-axis, Foveal Inferior': get_cd_on_range(0.8, 2.8, Direction.Y),\n",
    "        'Y-axis, Parafoveal Inferior': get_cd_on_range(2.8, 4.16, Direction.Y),\n",
    "        'Y-axis, Perifoveal Inferior': get_cd_on_range(4.16, 10, Direction.Y),\n",
    "})     \n",
    "\n",
    "\n",
    "sns.set_theme(font_scale=0.6)\n",
    "sns.heatmap(_df.corr(method='pearson'), annot=True, fmt=\".2f\", cmap='coolwarm', square=True, center=0)\n",
    "plt.title(f'Correlation matrix of Cone Density at different eccentricities.', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09446d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key1 = 'Y-axis, Foveola'\n",
    "key2 = 'Y-axis, Perifoveal Inferior'\n",
    "\n",
    "# Select the last two entries\n",
    "y_data = _df[key1]\n",
    "x_data = _df[key2]\n",
    "\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x_data, y_data, color='darkblue', label='Data Points')\n",
    "\n",
    "# Fit a linear model (y = mx + b)\n",
    "coeffs = np.polyfit(x_data, y_data, 1)  # Linear fit (degree 1)\n",
    "linear_fit = np.poly1d(coeffs)  # Create a polynomial from the coefficients\n",
    "\n",
    "# Plot the fitted line\n",
    "x_fit = np.linspace(x_data.min(), x_data.max(), 100)\n",
    "y_fit = linear_fit(x_fit)\n",
    "plt.plot(x_fit, y_fit, color='red', label=f'Fit: y = {coeffs[0]:.2f}x + {coeffs[1]:.2f}')\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel(key1)\n",
    "plt.xlabel(key2)\n",
    "plt.title(f'Scatter Plot: {key1} vs {key2}')\n",
    "\n",
    "# Show grid (or not)\n",
    "plt.grid(False)\n",
    "\n",
    "# Display the plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82f329",
   "metadata": {},
   "source": [
    "### Correlation with foveal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71021a2a",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3fd508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from turtle import color\n",
    "from cv2 import mean\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "from scipy import integrate, interpolate, stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ddac95",
   "metadata": {},
   "source": [
    "#### Function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c1bb8e",
   "metadata": {},
   "source": [
    "##### Integration/CD parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9980dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_cone_density_circle(eccs_in_MM: np.array,\n",
    "                                  cone_density_fd_X: np.array,\n",
    "                                  cone_density_fd_Y: np.array,\n",
    "                                  radius: float = 1.0,\n",
    "                                  num_r: int = 400,\n",
    "                                  num_theta: int = 400,\n",
    "                                  exclude_center: bool = False):\n",
    "    \"\"\"\n",
    "    Integrates cone density over a circular region by interpolating between the horizontal (X)\n",
    "    and vertical (Y) density measurements for each subject.\n",
    "    \n",
    "    Also computes:\n",
    "      - The total integrated density over the circle (a single value per subject).\n",
    "      - The maximum density along the horizontal and vertical meridians.\n",
    "      - The cumulative integrated density as a function of radius (in mm).\n",
    "    \n",
    "    Parameters:\n",
    "        eccs_in_MM (np.array): 1D array of eccentricities (radial positions in MM).\n",
    "        cone_density_fd_X (np.array): 2D array of cone densities along the horizontal meridian.\n",
    "                                      Shape: (n_subjects, len(eccs_in_MM)).\n",
    "        cone_density_fd_Y (np.array): 2D array of cone densities along the vertical meridian.\n",
    "                                      Shape: (n_subjects, len(eccs_in_MM)).\n",
    "        radius (float): Maximum radius (in MM) for the circular integration.\n",
    "        num_r (int): Number of radial grid points.\n",
    "        num_theta (int): Number of angular grid points.\n",
    "        exclude_center (bool): If True, excludes the central region (sets r_min to 0.3 MM).\n",
    "        \n",
    "    Returns:\n",
    "        mean_int (float): Mean integrated density across subjects.\n",
    "        std_int (float): Standard deviation of the integrated densities.\n",
    "        min_int (float): Minimum integrated density.\n",
    "        max_int (float): Maximum integrated density.\n",
    "        cov_int (float): Coefficient of Variation (std/mean * 100).\n",
    "        int_results (np.array): Array of integrated density values (one per subject).\n",
    "        max_x_results (np.array): Array of maximum densities along the X meridian per subject.\n",
    "        max_y_results (np.array): Array of maximum densities along the Y meridian per subject.\n",
    "        r_grid (np.array): The radial grid used for integration.\n",
    "        cumulative_integrations (np.array): 2D array (n_subjects x num_r) of cumulative integrated\n",
    "                                             density as a function of radius.\n",
    "    \"\"\"\n",
    "    # Set the lower bound for integration in r\n",
    "    r_min = 0.3 if exclude_center else 0.0\n",
    "    \n",
    "    # Create the polar grid for integration:\n",
    "    r = np.linspace(r_min, radius, num_r)\n",
    "    theta = np.linspace(0, 2 * np.pi, num_theta)\n",
    "    \n",
    "    int_results = []\n",
    "    max_x_results = []\n",
    "    max_y_results = []\n",
    "    cumulative_integrations_list = []\n",
    "    n_subjects = cone_density_fd_X.shape[0]\n",
    "    \n",
    "    # Loop over each subject (each row in the data)\n",
    "    for i in range(n_subjects):\n",
    "        # Create interpolation functions for the horizontal and vertical densities\n",
    "        f_x = interpolate.interp1d(eccs_in_MM, cone_density_fd_X[i, :],\n",
    "                                   bounds_error=False, fill_value=\"extrapolate\")\n",
    "        f_y = interpolate.interp1d(eccs_in_MM, cone_density_fd_Y[i, :],\n",
    "                                   bounds_error=False, fill_value=\"extrapolate\")\n",
    "        \n",
    "        # Evaluate the interpolated densities on the radial grid\n",
    "        density_x = f_x(r)  # shape: (num_r,)\n",
    "        density_y = f_y(r)  # shape: (num_r,)\n",
    "        \n",
    "        # Compute maximum density along each meridian within the integration region\n",
    "        max_x = np.max(density_x)\n",
    "        max_y = np.max(density_y)\n",
    "        max_x_results.append(max_x)\n",
    "        max_y_results.append(max_y)\n",
    "        \n",
    "        # Compute the density field on the polar grid.\n",
    "        # For each (r, theta) point, combine the two measurements as:\n",
    "        # density(r, theta) = density_x(r)*cos²(theta) + density_y(r)*sin²(theta)\n",
    "        density_field = (density_x[:, None] * np.cos(theta)**2 +\n",
    "                         density_y[:, None] * np.sin(theta)**2)\n",
    "        \n",
    "        # Multiply by the Jacobian (r) to account for the area element in polar coordinates\n",
    "        density_field_weighted = density_field * r[:, None]\n",
    "        \n",
    "        # Integrate first over theta (axis=1) then over r using Simpson’s rule.\n",
    "        integral_theta = integrate.simpson(density_field_weighted, x=theta, axis=1)\n",
    "        integrated_density = integrate.simpson(integral_theta, x=r)\n",
    "        int_results.append(integrated_density)\n",
    "        \n",
    "        # Compute cumulative integration as a function of r using cumulative trapezoidal rule.\n",
    "        # This gives the integrated cone density from r_min up to each r value.\n",
    "        cumulative_integration = integrate.cumulative_trapezoid(integral_theta, r, initial=0)\n",
    "        cumulative_integrations_list.append(cumulative_integration)\n",
    "    \n",
    "    # Convert lists to NumPy arrays for further statistics\n",
    "    int_results = np.array(int_results)\n",
    "    max_x_results = np.array(max_x_results)\n",
    "    max_y_results = np.array(max_y_results)\n",
    "    cumulative_integrations = np.array(cumulative_integrations_list)\n",
    "    \n",
    "    # Compute the integration metrics for the total integrated density\n",
    "    mean_int = np.mean(int_results)\n",
    "    std_int = np.std(int_results)\n",
    "    min_int = np.min(int_results)\n",
    "    max_int = np.max(int_results)\n",
    "    cov_int = (std_int / mean_int * 100) if mean_int != 0 else np.nan\n",
    "    \n",
    "    return (mean_int, std_int, min_int, max_int, cov_int, int_results,\n",
    "            max_x_results, max_y_results, r, cumulative_integrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e5476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_peak_width(density_values, positions, threshold_percent=50):\n",
    "    \"\"\"\n",
    "    Calculate the width of a peak at a specified percentage of its maximum value.\n",
    "    \n",
    "    Args:\n",
    "        density_values: Array of density values\n",
    "        positions: Array of corresponding positions\n",
    "        threshold_percent: Percentage of maximum to measure width at (default: 50 for FWHM)\n",
    "        \n",
    "    Returns:\n",
    "        width: The width at the specified threshold\n",
    "    \"\"\"\n",
    "    if len(density_values) == 0 or len(positions) == 0:\n",
    "        return None\n",
    "    \n",
    "\n",
    "    density_values = np.array(density_values)\n",
    "    positions = np.array(positions)\n",
    "    \n",
    "\n",
    "    # Find the maximum value and its index\n",
    "    max_val = np.max(density_values)\n",
    "    max_idx = np.argmax(density_values)\n",
    "    \n",
    "    # Calculate threshold value (e.g., 20% of maximum)\n",
    "    threshold = max_val * (threshold_percent / 100.0)\n",
    "    \n",
    "    # Find indices where values are closest to threshold\n",
    "    # For left side (before max)\n",
    "    left_indices = np.where(density_values[:max_idx] <= threshold)[0]\n",
    "    left_idx = left_indices[-1] if len(left_indices) > 0 else 0\n",
    "    \n",
    "    # For right side (after max)\n",
    "    right_indices = np.where(density_values[max_idx:] <= threshold)[0]\n",
    "    right_idx = right_indices[0] + max_idx if len(right_indices) > 0 else len(density_values) - 1\n",
    "    \n",
    "    # Use interpolation to find more precise positions\n",
    "    # For left side\n",
    "    if left_idx < max_idx - 1:\n",
    "        # Get points for interpolation\n",
    "        x1, y1 = positions[left_idx], density_values[left_idx]\n",
    "        x2, y2 = positions[left_idx + 1], density_values[left_idx + 1]\n",
    "        \n",
    "        # Linear interpolation to find position at threshold\n",
    "        if y2 - y1 != 0:  # Avoid division by zero\n",
    "            left_pos = x1 + (threshold - y1) * (x2 - x1) / (y2 - y1)\n",
    "        else:\n",
    "            left_pos = x1\n",
    "    else:\n",
    "        left_pos = positions[left_idx]\n",
    "    \n",
    "    # For right side\n",
    "    if right_idx > max_idx:\n",
    "        # Get points for interpolation\n",
    "        x1, y1 = positions[right_idx - 1], density_values[right_idx - 1]\n",
    "        x2, y2 = positions[right_idx], density_values[right_idx]\n",
    "        \n",
    "        # Linear interpolation to find position at threshold\n",
    "        if y2 - y1 != 0:  # Avoid division by zero\n",
    "            right_pos = x1 + (threshold - y1) * (x2 - x1) / (y2 - y1)\n",
    "        else:\n",
    "            right_pos = x1\n",
    "    else:\n",
    "        right_pos = positions[right_idx]\n",
    "    \n",
    "    # Calculate width\n",
    "    width = abs(right_pos - left_pos)\n",
    "    \n",
    "    return width #, right_pos, left_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3b588a",
   "metadata": {},
   "source": [
    "##### Foveal Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c43cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FoveaParams:\n",
    "    \"\"\"Class for storing fovea 3D fitted parameters.\"\"\"\n",
    "    # Patient information\n",
    "    subject: str\n",
    "    patient_id: str\n",
    "    subject_folder: str\n",
    "    trial_name: str\n",
    "    age: Optional[int] = None\n",
    "\n",
    "    # Fitted parameters\n",
    "    A00: Optional[float] = None\n",
    "    A10: Optional[float] = None\n",
    "    A01: Optional[float] = None\n",
    "    A20: Optional[float] = None\n",
    "    A02: Optional[float] = None\n",
    "    A11: Optional[float] = None\n",
    "    foveal_depth: Optional[float] = None\n",
    "    foveal_center_X: Optional[float] = None\n",
    "    foveal_width_X: Optional[float] = None\n",
    "    foveal_center_Y: Optional[float] = None\n",
    "    foveal_width_Y: Optional[float] = None\n",
    "    foveal_max_slope: Optional[float] = None\n",
    "    foveal_flatness: Optional[float] = None\n",
    "    foveal_volume: Optional[float] = None\n",
    "\n",
    "def extract_fovea_data(base_path: str) -> List[FoveaParams]:\n",
    "    \"\"\"\n",
    "    Extract fovea parameters from CSV files with known structure.\n",
    "    \n",
    "    Args:\n",
    "        base_path: Path to the base directory containing subject folders\n",
    "        (subjfolder/trialfolder/layer_new/fovea_3d_fitted_params.csv)\n",
    "    \n",
    "    Returns:\n",
    "        List of FoveaParams objects, one for each found CSV file\n",
    "    \"\"\"\n",
    "    fovea_data = []\n",
    "    \n",
    "    # Get only the subject directories (directories starting with \"Subject\")\n",
    "    try:\n",
    "        subject_dirs = [d for d in os.listdir(base_path) \n",
    "                      if os.path.isdir(os.path.join(base_path, d)) and d.startswith(\"Subject\")]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Base path not found: {base_path}\")\n",
    "        return []\n",
    "    \n",
    "    # For each subject directory, get the session directories\n",
    "    for subject_dir in subject_dirs:\n",
    "        subject_path = os.path.join(base_path, subject_dir)\n",
    "        \n",
    "        # Extract subject number\n",
    "        import re\n",
    "        subject_match = re.search(r'Subject(\\d+)', subject_dir)\n",
    "        if not subject_match:\n",
    "            continue\n",
    "            \n",
    "        subject_num = subject_match.group(1)\n",
    "        patient_id = f\"{subject_num}\"\n",
    "        \n",
    "        try:\n",
    "            # Get session directories (directories starting with \"Session\")\n",
    "            session_dirs = [d for d in os.listdir(subject_path) \n",
    "                          if os.path.isdir(os.path.join(subject_path, d)) and d.startswith(\"Session\")]\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        \n",
    "        # For each session directory, check if the CSV file exists\n",
    "        for session_dir in session_dirs:\n",
    "            session_path = os.path.join(subject_path, session_dir)\n",
    "            csv_path = os.path.join(session_path, \"layer_new\", \"fovea_3d_fitted_params.csv\")\n",
    "            \n",
    "            # Check if the CSV file exists\n",
    "            if os.path.isfile(csv_path):\n",
    "                # try:\n",
    "                # Read CSV file\n",
    "                df = pd.read_csv(csv_path, sep=';', header=None, names=['param', 'value'])\n",
    "                \n",
    "                # Create basic FoveaParams object with patient info\n",
    "                fovea_obj = FoveaParams(\n",
    "                    patient_id=str(patient_id),\n",
    "                    subject=f\"Subject{patient_id}\",\n",
    "                    subject_folder=subject_dir,\n",
    "                    trial_name=session_dir\n",
    "                )\n",
    "                print(f\"Processing file: {csv_path} for patient {patient_id}\")\n",
    "                \n",
    "                # Fill in parameter values\n",
    "                for _, row in df.iterrows():\n",
    "                    param_name = row['param']\n",
    "                    param_value = row['value']\n",
    "                    print(f\"Processing {param_name} with value {param_value} for patient {patient_id}\")\n",
    "                    \n",
    "                    if param_value == \"params\":\n",
    "                        print(f\"Skipping parameter {param_name} for patient {patient_id} as it is 'params'\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if this parameter exists in our class\n",
    "                    if hasattr(fovea_obj, param_name) or hasattr(fovea_obj, f\"foveal_{param_name}\"):\n",
    "                        print(f\"Setting {param_name} for patient {patient_id}\")\n",
    "                        try:\n",
    "                            # Convert to float and set attribute\n",
    "                            print(f\"Trying to set {param_name} for patient {patient_id}\")\n",
    "                            setattr(fovea_obj, param_name, float(param_value))\n",
    "                            print(f\"Successfully set {param_name} for patient {patient_id}\")\n",
    "                        except:\n",
    "                            \n",
    "                            print(f\"Error setting {param_name} for patient {patient_id}: {param_value}\")\n",
    "\n",
    "                    if hasattr(fovea_obj, f\"foveal_{param_name}\"):\n",
    "                        try:\n",
    "                            print(f\"Trying to set foveal_{param_name} for patient {patient_id}\")\n",
    "                            setattr(fovea_obj, f\"foveal_{param_name}\", float(param_value))\n",
    "\n",
    "                        except:\n",
    "                            print(f\"Skipping parameter {param_name} for patient {patient_id} \")\n",
    "\n",
    "                            pass\n",
    "                    \n",
    "                fovea_data.append(fovea_obj)\n",
    "                # except Exception as e:\n",
    "                    # print(f\"Error processing file {csv_path}: {str(e)}\")\n",
    "    \n",
    "    return fovea_data\n",
    "\n",
    "def save_to_dataframe(fovea_data: List[FoveaParams], output_file: str = \"fovea_parameters.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert list of FoveaParams objects to a pandas DataFrame and save to CSV.\n",
    "    \n",
    "    Args:\n",
    "        fovea_data: List of FoveaParams objects\n",
    "        output_file: Path to save the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing all fovea data\n",
    "    \"\"\"\n",
    "    # Convert to list of dictionaries\n",
    "    data_dicts = [vars(f) for f in fovea_data]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_dicts)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283a032",
   "metadata": {},
   "source": [
    "##### Foveal Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1395d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fovea_volume(params: FoveaParams, radius_start: float, radius_end: float, num_points: int = 1000) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the volume of the fovea between radius_start and radius_end.\n",
    "    \n",
    "    Parameters:\n",
    "    - params: FoveaParams object containing the fitted parameters\n",
    "    - radius_start: Inner radius of the circular region\n",
    "    - radius_end: Outer radius of the circular region\n",
    "    - num_points: Number of points for numerical integration\n",
    "    \n",
    "    Returns:\n",
    "    - volume: Volume of the fovea in the specified region\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Calculating fovea volume for subject: {params.subject}, from radius {radius_start} to {radius_end} from center ({params.foveal_center_X}, {params.foveal_center_Y})\")\n",
    "    # Define the height function based on the model parameters\n",
    "    def height_function(x, y):\n",
    "        # 2D parabola part\n",
    "        parabola = (params.A00 + params.A10*x + params.A01*y - \n",
    "                   params.A20*(x**2) - params.A02*(y**2) + params.A11*x*y)\n",
    "        \n",
    "        # 2D Gaussian part\n",
    "        x_term = ((x - params.foveal_center_X)**2) / (2 * (params.foveal_width_X**2))\n",
    "        y_term = ((y - params.foveal_center_Y)**2) / (2 * (params.foveal_width_Y**2))\n",
    "        gaussian = params.foveal_depth * np.exp(-(x_term + y_term))\n",
    "        \n",
    "        return parabola - gaussian\n",
    "    \n",
    "    # Use polar coordinates for integration over circular regions\n",
    "    def integrand(r, theta):\n",
    "        x = r * np.cos(theta) + params.foveal_center_X  # x-coordinate relative to center\n",
    "        y = r * np.sin(theta) + params.foveal_center_Y  # y-coordinate relative to center\n",
    "        return height_function(x, y) * r  # r is the Jacobian for polar coordinates\n",
    "    \n",
    "    # Perform the double integration over r and theta\n",
    "    volume, _ = integrate.dblquad(\n",
    "        integrand, \n",
    "        0, 2*np.pi,  # theta limits: 0 to 2π\n",
    "        lambda _: radius_start, lambda _: radius_end  # r limits: radius_start to radius_end\n",
    "    )\n",
    "    \n",
    "    return volume\n",
    "\n",
    "def calculate_all_regional_volumes(fovea_data: List[FoveaParams], retinal_regions: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate volumes for all subjects across all specified retinal regions,\n",
    "    measuring from the center (0) to the absolute boundary of each region.\n",
    "    \n",
    "    Parameters:\n",
    "    - subjects_data: List of FoveaParams objects\n",
    "    - retinal_regions: List of dictionaries defining regions\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with volumes organized by subject and region\n",
    "    \"\"\"\n",
    "    foveal_integration_data = {}\n",
    "    \n",
    "    for subject in fovea_data:\n",
    "        print(f\"Calculating volumes for subject: {subject.subject}\")\n",
    "        foveal_integration_data[subject.subject] = {}\n",
    "\n",
    "        foveal_integration_data[subject.subject][\"total\"] = {\n",
    "                'foveal_volume': subject.volume,\n",
    "                'foveal_radius': np.inf\n",
    "            }\n",
    "        \n",
    "        for region in retinal_regions:\n",
    "            # For each region, we want to calculate from 0 to the maximum absolute radius\n",
    "            region_radius = max(abs(region['start']), abs(region['end']))\n",
    "            \n",
    "            # For regions with infinite bounds, set a practical limit\n",
    "            if region_radius == np.inf:\n",
    "                region_radius = 3.0  # Practical limit for \"infinite\" radius\n",
    "            \n",
    "            # Calculate volume from center (0) to region_radius\n",
    "            volume = calculate_fovea_volume(subject, 0, region_radius)\n",
    "            \n",
    "            # Store the result\n",
    "            foveal_integration_data[subject.subject][region['name']] = {\n",
    "                'foveal_volume': volume,\n",
    "                'foveal_radius': region_radius\n",
    "            }\n",
    "    \n",
    "    return foveal_integration_data\n",
    "\n",
    "# Function to visualize the model and regions\n",
    "def visualize_fovea_model(params: FoveaParams, retinal_regions: List[Dict]):\n",
    "    \"\"\"Create a visualization of the fovea model with marked regions\"\"\"\n",
    "    # Create a grid of points\n",
    "    x = np.linspace(params.foveal_center_X - 2, params.foveal_center_X + 2, 100)\n",
    "    y = np.linspace(params.foveal_center_Y - 2, params.foveal_center_Y + 2, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Calculate Z values\n",
    "    parabola = (params.A00 + params.A10*X + params.A01*Y - \n",
    "               params.A20*(X**2) - params.A02*(Y**2) + params.A11*X*Y)\n",
    "    \n",
    "    x_term = ((X - params.foveal_center_X)**2) / (2 * (params.foveal_width_X**2))\n",
    "    y_term = ((Y - params.foveal_center_Y)**2) / (2 * (params.foveal_width_Y**2))\n",
    "    gaussian = params.foveal_depth * np.exp(-(x_term + y_term))\n",
    "    \n",
    "    Z = parabola - gaussian\n",
    "    \n",
    "    # Create 3D plot\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "    ax1.set_zlabel('Z')\n",
    "    ax1.set_title('3D Fovea Model')\n",
    "    \n",
    "    # Add a 2D cross-section\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    center_idx = len(x) // 2\n",
    "    cross_section = Z[:, center_idx]\n",
    "    ax2.plot(y, cross_section)\n",
    "    \n",
    "    # Mark the regions\n",
    "    colors = ['red', 'green', 'blue', 'purple', 'orange', 'brown', 'pink']\n",
    "    unique_regions = set()\n",
    "    for i, region in enumerate(retinal_regions):\n",
    "        if region['name'] not in unique_regions:\n",
    "            unique_regions.add(region['name'])\n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            # Handle infinite bounds for visualization\n",
    "            start = region['start']\n",
    "            end = region['end']\n",
    "            \n",
    "            if start == -np.inf:\n",
    "                start = -3.0\n",
    "            if end == np.inf:\n",
    "                end = 3.0\n",
    "                \n",
    "            # Mark region on cross-section\n",
    "            y_region = np.linspace(params.center_Y + start, params.center_Y + end, 100)\n",
    "            idx_start = np.abs(y - (params.center_Y + start)).argmin()\n",
    "            idx_end = np.abs(y - (params.center_Y + end)).argmin()\n",
    "            \n",
    "            ax2.axvspan(params.center_Y + start, params.center_Y + end, \n",
    "                        alpha=0.2, color=color, label=region['name'])\n",
    "    \n",
    "    ax2.set_xlabel('Y position')\n",
    "    ax2.set_ylabel('Z height')\n",
    "    ax2.set_title('Cross-section with Retinal Regions')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_directional_slopes(params, distance=0.1):\n",
    "    \"\"\"\n",
    "    Calculate slopes in four cardinal directions from the foveal model.\n",
    "    \n",
    "    Args:\n",
    "        params: FoveaParams object with fitted parameters\n",
    "        distance: Distance from center to calculate slope (in mm)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Slopes in four directions\n",
    "    \"\"\"\n",
    "    \n",
    "    def height_function(x, y):\n",
    "        # 2D parabola part\n",
    "        parabola = (params.A00 + params.A10*x + params.A01*y - \n",
    "                   params.A20*(x**2) - params.A02*(y**2) + params.A11*x*y)\n",
    "        \n",
    "        # 2D Gaussian part\n",
    "        x_term = ((x - params.foveal_center_X)**2) / (2 * (params.foveal_width_X**2))\n",
    "        y_term = ((y - params.foveal_center_Y)**2) / (2 * (params.foveal_width_Y**2))\n",
    "        gaussian = params.foveal_depth * np.exp(-(x_term + y_term))\n",
    "        \n",
    "        return parabola - gaussian\n",
    "    \n",
    "    # Calculate heights at center and at distance in each direction\n",
    "    center_height = height_function(params.foveal_center_X, params.foveal_center_Y)\n",
    "    \n",
    "    # Temporal (y=0, x<0) - negative x direction\n",
    "    temporal_height = height_function(params.foveal_center_X - distance, params.foveal_center_Y)\n",
    "    temporal_slope = (temporal_height - center_height) / distance\n",
    "    \n",
    "    # Nasal (y=0, x>0) - positive x direction  \n",
    "    nasal_height = height_function(params.foveal_center_X + distance, params.foveal_center_Y)\n",
    "    nasal_slope = (nasal_height - center_height) / distance\n",
    "    \n",
    "    # Superior (y<0, x=0) - negative y direction\n",
    "    superior_height = height_function(params.foveal_center_X, params.foveal_center_Y - distance)\n",
    "    superior_slope = (superior_height - center_height) / distance\n",
    "    \n",
    "    # Inferior (y>0, x=0) - positive y direction\n",
    "    inferior_height = height_function(params.foveal_center_X, params.foveal_center_Y + distance)\n",
    "    inferior_slope = (inferior_height - center_height) / distance\n",
    "    \n",
    "    return {\n",
    "        'foveal_temporal_slope': temporal_slope,\n",
    "        'foveal_nasal_slope': nasal_slope,\n",
    "        'foveal_superior_slope': superior_slope,\n",
    "        'foveal_inferior_slope': inferior_slope\n",
    "    }\n",
    "\n",
    "def create_foveal_integration_scatterplots(fovea_params_list, integration_data, save_path=None):\n",
    "    \"\"\"\n",
    "    Create scatter plots of foveal parameters vs integration parameters.\n",
    "    \n",
    "    Args:\n",
    "        fovea_params_list: List of FoveaParams objects\n",
    "        integration_data: Dictionary with integration data for each subject\n",
    "        save_path: Path to save plots (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    if save_path:\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Convert fovea data to DataFrame\n",
    "    fovea_df = pd.DataFrame([vars(f) for f in fovea_params_list])\n",
    "    \n",
    "    # Calculate directional slopes for each subject\n",
    "    directional_data = []\n",
    "    for params in fovea_params_list:\n",
    "        if all(hasattr(params, attr) and getattr(params, attr) is not None \n",
    "               for attr in ['A00', 'foveal_center_X', 'foveal_center_Y', 'foveal_width_X', 'foveal_width_Y', 'foveal_depth']):\n",
    "            slopes = calculate_directional_slopes(params)\n",
    "            slopes['subject'] = params.subject\n",
    "            directional_data.append(slopes)\n",
    "    \n",
    "    slopes_df = pd.DataFrame(directional_data)\n",
    "    \n",
    "    # Merge with fovea parameters\n",
    "    extended_fovea_df = pd.merge(fovea_df, slopes_df, on='subject', how='left')\n",
    "    \n",
    "    # Convert integration data to DataFrame with proper numeric conversion\n",
    "    integration_rows = []\n",
    "    for subject, data in integration_data.items():\n",
    "        row = {'subject': subject}\n",
    "        for key, value in data.items():\n",
    "            # Handle numpy arrays - take first element if array\n",
    "            if isinstance(value, np.ndarray):\n",
    "                if len(value) > 0:\n",
    "                    row[key] = float(value[0]) if not np.isnan(value[0]) else np.nan\n",
    "                else:\n",
    "                    row[key] = np.nan\n",
    "            # Handle other numeric types\n",
    "            elif isinstance(value, (int, float, np.integer, np.floating)):\n",
    "                row[key] = float(value)\n",
    "            # Handle strings that might be numeric\n",
    "            elif isinstance(value, str):\n",
    "                try:\n",
    "                    row[key] = float(value)\n",
    "                except ValueError:\n",
    "                    row[key] = np.nan\n",
    "            else:\n",
    "                row[key] = np.nan\n",
    "        integration_rows.append(row)\n",
    "    \n",
    "    integration_df = pd.DataFrame(integration_rows)\n",
    "    \n",
    "    # Ensure all integration columns (except subject) are numeric\n",
    "    for col in integration_df.columns:\n",
    "        if col != 'subject':\n",
    "            integration_df[col] = pd.to_numeric(integration_df[col], errors='coerce')\n",
    "    \n",
    "    # Merge datasets\n",
    "    merged_df = pd.merge(extended_fovea_df, integration_df, on='subject', how='inner')\n",
    "    \n",
    "    print(f\"Successfully merged data for {len(merged_df)} subjects\")\n",
    "    \n",
    "    # Define foveal parameters to plot\n",
    "    foveal_params = [\n",
    "        'foveal_max_slope', 'foveal_temporal_slope', 'foveal_nasal_slope', 'foveal_superior_slope', 'foveal_inferior_slope',\n",
    "        'A00', 'A10', 'A01', 'A20', 'A02', 'A11', 'foveal_depth', \n",
    "        'foveal_center_X', 'foveal_width_X', 'foveal_center_Y', 'foveal_width_Y', 'foveal_flatness', 'volume'\n",
    "    ]\n",
    "    \n",
    "    # Get integration parameter names (exclude subject)\n",
    "    integration_params = [col for col in integration_df.columns if col != 'subject']\n",
    "    \n",
    "    print(f\"Foveal parameters: {foveal_params}\")\n",
    "    print(f\"Integration parameters: {integration_params}\")\n",
    "    \n",
    "    # Create scatter plots\n",
    "    for foveal_param in foveal_params:\n",
    "        if foveal_param not in merged_df.columns:\n",
    "            print(f\"Skipping {foveal_param} - not found in data\")\n",
    "            continue\n",
    "            \n",
    "        # Check if we have valid data for this parameter\n",
    "        valid_data = merged_df[foveal_param].dropna()\n",
    "        if len(valid_data) < 5:\n",
    "            print(f\"Skipping {foveal_param} - insufficient data ({len(valid_data)} points)\")\n",
    "            continue\n",
    "        \n",
    "        # Create subplots for all integration parameters\n",
    "        n_params = len(integration_params)\n",
    "        n_cols = 3\n",
    "        n_rows = (n_params + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        fig.suptitle(f'{foveal_param.replace(\"_\", \" \").title()} vs Integration Parameters', \n",
    "                     fontsize=16, y=0.995)\n",
    "        \n",
    "        # Flatten axes array for easier indexing\n",
    "        if n_rows == 1:\n",
    "            axes = [axes] if n_cols == 1 else axes\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        for i, integration_param in enumerate(integration_params):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Get valid data for both parameters\n",
    "            valid_mask = merged_df[foveal_param].notna() & merged_df[integration_param].notna()\n",
    "            x_data = merged_df.loc[valid_mask, foveal_param]\n",
    "            y_data = merged_df.loc[valid_mask, integration_param]\n",
    "            \n",
    "            # Ensure data is numeric\n",
    "            try:\n",
    "                x_data = pd.to_numeric(x_data, errors='coerce').dropna()\n",
    "                y_data = pd.to_numeric(y_data, errors='coerce').dropna()\n",
    "                \n",
    "                # Re-align the data after dropping NaNs\n",
    "                common_indices = x_data.index.intersection(y_data.index)\n",
    "                x_data = x_data.loc[common_indices]\n",
    "                y_data = y_data.loc[common_indices]\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting data to numeric for {foveal_param} vs {integration_param}: {e}\")\n",
    "                ax.text(0.5, 0.5, f'Data conversion error', \n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title(integration_param)\n",
    "                continue\n",
    "            \n",
    "            if len(x_data) < 3:\n",
    "                ax.text(0.5, 0.5, f'Insufficient data\\n({len(x_data)} points)', \n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title(integration_param)\n",
    "                continue\n",
    "            \n",
    "            # Create scatter plot\n",
    "            ax.scatter(x_data, y_data, alpha=0.6, s=50)\n",
    "            \n",
    "            # Calculate correlation\n",
    "            try:\n",
    "                r, p = stats.spearmanr(x_data, y_data)\n",
    "                \n",
    "                # Add trend line if correlation is significant\n",
    "                trend_line_label = \"\"\n",
    "                # if p < 0.05:\n",
    "                z = np.polyfit(x_data, y_data, 1)\n",
    "                p_line = np.poly1d(z)\n",
    "                beta = z[0]  # slope coefficient\n",
    "                intercept = z[1]  # intercept\n",
    "                \n",
    "                x_sorted = x_data.sort_values()\n",
    "                ax.plot(x_sorted, p_line(x_sorted), \n",
    "                        \"r--\", alpha=0.8, linewidth=2, \n",
    "                        label=f'β = {beta:.3f}')\n",
    "                \n",
    "                # Add legend\n",
    "                ax.legend(loc='best', fontsize=8, frameon=True, \n",
    "                            fancybox=True, shadow=True)\n",
    "                \n",
    "                # Format title with correlation info\n",
    "                significance = \"\"\n",
    "                if p < 0.001:\n",
    "                    significance = \"***\"\n",
    "                elif p < 0.01:\n",
    "                    significance = \"**\"\n",
    "                elif p < 0.05:\n",
    "                    significance = \"*\"\n",
    "                \n",
    "                title = f'{integration_param}\\nr={r:.3f}{significance} (n={len(x_data)})'\n",
    "                ax.set_title(title, fontsize=10)\n",
    "                \n",
    "            except Exception as e:\n",
    "                ax.set_title(f'{integration_param}\\n(correlation error)')\n",
    "                print(f\"Error calculating correlation for {foveal_param} vs {integration_param}: {e}\")\n",
    "            \n",
    "            # Set axis labels\n",
    "            ax.set_xlabel(foveal_param.replace('_', ' ').title())\n",
    "            ax.set_ylabel(integration_param.replace('_', ' ').title())\n",
    "            \n",
    "            # Add grid\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for j in range(len(integration_params), len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for suptitle\n",
    "        \n",
    "        # Save plot if path is provided\n",
    "        if save_path:\n",
    "            filename = f\"{foveal_param}_vs_integration_parameters.png\"\n",
    "            filepath = os.path.join(save_path, filename)\n",
    "            plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved: {filepath}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "def create_summary_correlation_matrix(fovea_params_list, integration_data, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a summary correlation matrix focusing on key relationships.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert and merge data (same as above)\n",
    "    fovea_df = pd.DataFrame([vars(f) for f in fovea_params_list])\n",
    "    \n",
    "    # Calculate directional slopes\n",
    "    directional_data = []\n",
    "    for params in fovea_params_list:\n",
    "        if all(hasattr(params, attr) and getattr(params, attr) is not None \n",
    "               for attr in ['A00', 'center_X', 'center_Y', 'width_X', 'width_Y', 'depth']):\n",
    "            slopes = calculate_directional_slopes(params)\n",
    "            slopes['subject'] = params.subject\n",
    "            directional_data.append(slopes)\n",
    "    \n",
    "    slopes_df = pd.DataFrame(directional_data)\n",
    "    extended_fovea_df = pd.merge(fovea_df, slopes_df, on='subject', how='left')\n",
    "    \n",
    "    integration_df = pd.DataFrame([\n",
    "        {'subject': subject, **data} for subject, data in integration_data.items()\n",
    "    ])\n",
    "    \n",
    "    merged_df = pd.merge(extended_fovea_df, integration_df, on='subject', how='inner')\n",
    "    \n",
    "    # Select key parameters for correlation matrix\n",
    "    key_foveal_params = ['max_slope', 'temporal_slope', 'nasal_slope', \n",
    "                        'superior_slope', 'inferior_slope', 'depth', 'volume']\n",
    "    integration_params = [col for col in integration_df.columns if col != 'subject']\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    selected_columns = [col for col in key_foveal_params + integration_params \n",
    "                       if col in merged_df.columns]\n",
    "    \n",
    "    correlation_matrix = merged_df[selected_columns].corr(method='spearman')\n",
    "    \n",
    "    # Plot correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    # Create heatmap\n",
    "    import seaborn as sns\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=0.5, fmt='.2f',\n",
    "                annot_kws={'size': 8})\n",
    "    \n",
    "    plt.title('Correlation Matrix: Foveal Parameters vs Integration Parameters', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        filepath = os.path.join(save_path, \"foveal_integration_correlation_matrix.png\")\n",
    "        plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved correlation matrix: {filepath}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72373868",
   "metadata": {},
   "source": [
    "##### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f3599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_foveal_volumes_with_cone_density(foveal_volumes, integration_data):\n",
    "    \"\"\"\n",
    "    Create a correlation matrix between foveal volumes and cone density integration data.\n",
    "    \n",
    "    Args:\n",
    "        foveal_volumes: Dictionary with foveal volume data\n",
    "        integration_data: Dictionary with cone density integration data\n",
    "    \n",
    "    Returns:\n",
    "        correlation_df: DataFrame with correlation coefficients\n",
    "        merged_df: The merged data DataFrame\n",
    "        annot: DataFrame with formatted annotations\n",
    "    \"\"\"\n",
    "    print(f\"Starting correlation analysis between foveal volumes and cone density...\")\n",
    "    \n",
    "    # Step 1: Create a DataFrame from foveal volumes\n",
    "    print(f\"Processing foveal volume data for {len(foveal_volumes)} subjects\")\n",
    "    \n",
    "    foveal_data = []\n",
    "    for subject_id, regions in foveal_volumes.items():\n",
    "        # Create a row for this subject\n",
    "        row = {'subject': subject_id}\n",
    "        \n",
    "\n",
    "        print(f\"Processing subject {subject_id} with regions: {list(regions.keys())}\")\n",
    "        # Add volume data for each region\n",
    "        print (f\"Regions found: {list(regions.items())}\")\n",
    "        for region_name, data in regions.items():\n",
    "            print(f\"Processing region {region_name} with data: {data}\")\n",
    "            if isinstance(data, dict) and 'foveal_volume' in data:\n",
    "                \n",
    "                row[f\"foveal_volume_{region_name}\"] = data['foveal_volume']\n",
    "        \n",
    "        foveal_data.append(row)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    foveal_df = pd.DataFrame(foveal_data)\n",
    "    print(f\"Created foveal volume DataFrame with {len(foveal_df)} rows and {len(foveal_df.columns)} columns\")\n",
    "    \n",
    "    # Print the first few rows to verify\n",
    "    print(\"\\nSample of foveal volume data (first 3 rows):\")\n",
    "    print(foveal_df.head(3))\n",
    "    \n",
    "    # Print unique subjects in foveal data\n",
    "    print(f\"\\nFound {len(foveal_df['subject'].unique())} unique subjects in foveal data:\")\n",
    "    print(foveal_df['subject'].unique()[:5], \"...\" if len(foveal_df['subject'].unique()) > 5 else \"\")\n",
    "    \n",
    "    # Step 2: Create a DataFrame for cone density data\n",
    "    print(f\"\\nProcessing cone density integration data for {len(integration_data)} subjects\")\n",
    "    \n",
    "    density_data = []\n",
    "    for subject_id, regions in integration_data.items():\n",
    "        # Create a row for this subject\n",
    "        row = {'subject': subject_id}\n",
    "        \n",
    "        # Add region data\n",
    "        for region, value in regions.items():\n",
    "            if region not in [\"max_X\", \"max_Y\"]:  # Skip these special keys\n",
    "                row[f\"cone_density_{region}\"] = value[0] if isinstance(value, np.ndarray) else value\n",
    "        \n",
    "        # Add max X and Y if available\n",
    "        if \"max_X\" in regions:\n",
    "            row[\"max_X\"] = regions[\"max_X\"]\n",
    "        if \"max_Y\" in regions:\n",
    "            row[\"max_Y\"] = regions[\"max_Y\"]\n",
    "            \n",
    "        density_data.append(row)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    density_df = pd.DataFrame(density_data)\n",
    "    print(f\"Created cone density DataFrame with {len(density_df)} rows and {len(density_df.columns)} columns\")\n",
    "    \n",
    "    # Print the first few rows to verify\n",
    "    print(\"\\nSample of cone density data (first 3 rows):\")\n",
    "    print(density_df.head(3))\n",
    "    \n",
    "    # Print unique subjects in density data\n",
    "    print(f\"\\nFound {len(density_df['subject'].unique())} unique subjects in density data:\")\n",
    "    print(density_df['subject'].unique()[:5], \"...\" if len(density_df['subject'].unique()) > 5 else \"\")\n",
    "    \n",
    "    # Step 3: Merge the DataFrames on subject\n",
    "    print(\"\\nMerging datasets on 'subject' column...\")\n",
    "    merged_df = pd.merge(foveal_df, density_df, on='subject', how='inner')\n",
    "    print(f\"Merged DataFrame has {len(merged_df)} rows\")\n",
    "    \n",
    "    # Check if we have any data after merging\n",
    "    if len(merged_df) == 0:\n",
    "        print(\"ERROR: No matching subjects found between datasets!\")\n",
    "        print(\"Check that subject names match exactly between the two datasets.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Print the subjects that were successfully matched\n",
    "    print(f\"Successfully matched {len(merged_df['subject'].unique())} subjects:\")\n",
    "    print(merged_df['subject'].unique()[:5], \"...\" if len(merged_df['subject'].unique()) > 5 else \"\")\n",
    "    \n",
    "    # Step 4: Select only numeric columns for correlation\n",
    "    print(\"\\nSelecting numeric columns for correlation...\")\n",
    "    \n",
    "    # Get all columns except 'subject'\n",
    "    numeric_cols = [col for col in merged_df.columns if col != 'subject']\n",
    "    print(f\"Numeric columns: {numeric_cols}\")\n",
    "    \n",
    "    # Select columns for correlation\n",
    "    numeric_df = merged_df[numeric_cols]\n",
    "    print(f\"Selected {len(numeric_df.columns)} numeric columns for correlation\")\n",
    "    \n",
    "    # Check for and report missing data\n",
    "    missing_data = numeric_df.isna().sum()\n",
    "    if missing_data.sum() > 0:\n",
    "        print(\"\\nWARNING: Missing data detected in these columns:\")\n",
    "        print(missing_data[missing_data > 0])\n",
    "    \n",
    "    # Step 5: Calculate correlation matrix\n",
    "    print(\"\\nCalculating correlation matrix...\")\n",
    "    \n",
    "    correlation_df = numeric_df.corr(method=\"spearman\")\n",
    "    \n",
    "    # Using your existing function for p-values\n",
    "    pvalues = calculate_pvalues(correlation_df)\n",
    "    print(f\"Calculated p-values for correlation matrix\")\n",
    "    \n",
    "    # Create annotation matrix with your existing approach\n",
    "    annot = correlation_df.applymap(lambda x: f\"{x:.2f}\")  # Format correlation values\n",
    "    \n",
    "    for r in correlation_df.columns:\n",
    "        for c in correlation_df.columns:\n",
    "            pval = pvalues.loc[r, c]\n",
    "            # annot.loc[r, c] += f\"\\n{significance_marker(pval)}\"\n",
    "            annot.loc[r, c] += f\"\\n(p={pval:.3f})\"\n",
    "    \n",
    "    return correlation_df, merged_df, annot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e722da6e",
   "metadata": {},
   "source": [
    "##### Correlation Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlation_matrix(fovea_params_list, region_dict):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Create a correlation matrix between fovea parameters and region density data.\n",
    "    \n",
    "    Args:\n",
    "        fovea_params_list: List of FoveaParams objects\n",
    "        region_dict: Dictionary with subject data for different regions\n",
    "    \n",
    "    Returns:\n",
    "        correlation_df: DataFrame with correlation coefficients\n",
    "        merged_df: The merged data DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"Starting correlation analysis...\")\n",
    "    \n",
    "    # Step 1: Create a DataFrame from fovea parameters\n",
    "    print(f\"Processing {len(fovea_params_list)} fovea parameter objects\")\n",
    "    fovea_df = pd.DataFrame([vars(f) for f in fovea_params_list])\n",
    "    print(f\"Created fovea DataFrame with {len(fovea_df)} rows and {len(fovea_df.columns)} columns\")\n",
    "    \n",
    "    # Print the first few rows to verify\n",
    "    print(\"\\nSample of fovea data (first 3 rows):\")\n",
    "    print(fovea_df[['subject', 'patient_id', 'A00', 'foveal_depth', 'foveal_volume']].head(3))\n",
    "    \n",
    "    # Print unique subjects in fovea data\n",
    "    print(f\"\\nFound {len(fovea_df['subject'].unique())} unique subjects in fovea data:\")\n",
    "    print(fovea_df['subject'].unique()[:5], \"...\" if len(fovea_df['subject'].unique()) > 5 else \"\")\n",
    "    \n",
    "    # Step 2: Extract the numeric parameters from fovea_df\n",
    "    param_cols = ['A00', 'A10', 'A01', 'A20', 'A02', 'A11', 'foveal_depth', \n",
    "                  'foveal_center_X', 'foveal_width_X', 'foveal_center_Y', 'foveal_width_Y', \n",
    "                  'foveal_max_slope', 'foveal_flatness', 'volume']\n",
    "    \n",
    "    # Check which parameters are present\n",
    "    present_params = [col for col in param_cols if col in fovea_df.columns]\n",
    "    print(f\"\\nFound {len(present_params)} parameters in fovea data: {present_params}\")\n",
    "    \n",
    "    # Step 3: Create a DataFrame for region data\n",
    "    print(f\"\\nProcessing region dictionary with {len(region_dict)} subjects\")\n",
    "    region_data = []\n",
    "    \n",
    "    for subj_id in region_dict:\n",
    "        # Create a row for this subject\n",
    "        row = {'subject': subj_id}\n",
    "        \n",
    "        # Add region data\n",
    "        for region, value_array in region_dict[subj_id].items():\n",
    "            # Use the first value if it's an array\n",
    "            row[f\"{region}\"] = value_array[0] if isinstance(value_array, np.ndarray) else value_array\n",
    "        \n",
    "        region_data.append(row)\n",
    "        print(f\"Processed subject {subj_id} with {len(row)} region values\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    region_df = pd.DataFrame(region_data)\n",
    "    print(f\"Created region DataFrame with {len(region_df)} rows and {len(region_df.columns)} columns\")\n",
    "    \n",
    "    # Print the first few rows to verify\n",
    "    print(\"\\nSample of region data (first 3 rows):\")\n",
    "    print(region_df.head(3))\n",
    "    \n",
    "    # Print unique subjects in region data\n",
    "    print(f\"\\nFound {len(region_df['subject'].unique())} unique subjects in region data:\")\n",
    "    print(region_df['subject'].unique()[:5], \"...\" if len(region_df['subject'].unique()) > 5 else \"\")\n",
    "    \n",
    "    # Step 4: Merge the DataFrames on subject\n",
    "    print(\"\\nMerging datasets on 'subject' column...\")\n",
    "    merged_df = pd.merge(fovea_df, region_df, on='subject', how='inner')\n",
    "    print(f\"Merged DataFrame has {len(merged_df)} rows\")\n",
    "    \n",
    "    # Check if we have any data after merging\n",
    "    if len(merged_df) == 0:\n",
    "        print(\"ERROR: No matching subjects found between datasets!\")\n",
    "        print(\"Check that subject names match exactly between the two datasets.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Print the subjects that were successfully matched\n",
    "    print(f\"Successfully matched {len(merged_df['subject'].unique())} subjects:\")\n",
    "    print(merged_df['subject'].unique()[:5], \"...\" if len(merged_df['subject'].unique()) > 5 else \"\")\n",
    "    \n",
    "    # Step 5: Select only numeric columns for correlation\n",
    "    print(\"\\nSelecting numeric columns for correlation...\")\n",
    "    \n",
    "    # Get region columns (excluding subject)\n",
    "    region_cols = [col for col in region_df.columns if col != 'subject']\n",
    "    print(f\"Region columns: {region_cols}\")\n",
    "    \n",
    "    # Get numeric fovea parameter columns that actually have data\n",
    "    valid_param_cols = [col for col in param_cols if col in merged_df.columns and not merged_df[col].isna().all()]\n",
    "    print(f\"Valid parameter columns: {valid_param_cols}\")\n",
    "    \n",
    "    # Select columns for correlation\n",
    "    numeric_df = merged_df[valid_param_cols + region_cols]\n",
    "    print(f\"Selected {len(numeric_df.columns)} numeric columns for correlation\")\n",
    "    \n",
    "    # Check for and report missing data\n",
    "    missing_data = numeric_df.isna().sum()\n",
    "    if missing_data.sum() > 0:\n",
    "        print(\"\\nWARNING: Missing data detected in these columns:\")\n",
    "        print(missing_data[missing_data > 0])\n",
    "    \n",
    "    # Step 6: Calculate correlation matrix\n",
    "    print(\"\\nCalculating correlation matrix...\")\n",
    "    \n",
    "    correlation_df = numeric_df.corr(method=\"spearman\")\n",
    "    \n",
    "    pvalues = calculate_pvalues(correlation_df)\n",
    "    \n",
    "    print(f\"Calculated p-values for correlation matrix\")\n",
    "    # print(\"DEBUG: p-values matrix:\", pvalues)\n",
    "\n",
    "    print(f\"Created correlation matrix with shape {correlation_df.shape}\")\n",
    "\n",
    "    annot = correlation_df.applymap(lambda x: f\"{x:.2f}\")  # Spearman correlation values\n",
    "\n",
    "    for r in correlation_df.columns:\n",
    "        # print(\"DEBUG: r\", r)\n",
    "        for c in correlation_df.columns:\n",
    "            # print(\"DEBUG: c\", c)\n",
    "            # print(\"DEBUG: creating annotation for columns\",r , \"and\", c)\n",
    "            pval = pvalues[r][c]\n",
    "            # print(\"DEBUG: pval\", pval)\n",
    "            annot[r][c] += f\"\\n{significance_marker(pval)}\"\n",
    "    \n",
    "    return correlation_df, merged_df, annot\n",
    "\n",
    "def plot_correlation_matrix(corr_df, annot,  title=\"Correlation Matrix\"):\n",
    "    \"\"\"Plot a correlation matrix heatmap.\"\"\"\n",
    "    print(f\"\\nPlotting correlation matrix: {title}\")\n",
    "\n",
    "    # print(f\"DEBUG: Correlation DataFrame shape: {corr_df.shape}\")\n",
    "    # print(f\"DEBUG: Correlation Annotation columns: {annot.shape}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
    "    \n",
    "    # Draw heatmap with mask and correct aspect ratio\n",
    "    sns.heatmap(corr_df, mask=mask, annot=annot, cmap=\"coolwarm\", fmt =\"s\",\n",
    "                vmin=-1, vmax=1, center=0, square=True, linewidths=.5,\n",
    "                 annot_kws={ 'fontstyle': 'italic', 'color':'w', 'alpha': 1.0,\n",
    "                })\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Plot complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25ea17",
   "metadata": {},
   "source": [
    "##### Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d971e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_foveal_shape_and_cone_density(fovea_data, retinal_regions, integration_data):\n",
    "    \"\"\"\n",
    "    Main function to run the complete analysis workflow.\n",
    "    \n",
    "    Args:\n",
    "        fvea_data: List of FoveaParams objects\n",
    "        retinal_regions: List of dictionaries defining regions\n",
    "        integration_data: Dictionary with cone density integration data\n",
    "        \n",
    "    Returns:\n",
    "        correlation_df: The correlation matrix\n",
    "        merged_df: The merged data for further analysis\n",
    "        foveal_volumes: The calculated foveal volumes\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate foveal volumes\n",
    "    print(\"Step 1: Calculating foveal volumes for all subjects and regions...\")\n",
    "    foveal_volumes = calculate_all_regional_volumes(fovea_data, retinal_regions)\n",
    "    \n",
    "    # Step 2: Correlate with cone density data\n",
    "    print(\"\\nStep 2: Correlating foveal volumes with cone density data...\")\n",
    "    corr_df, merged_df, annot = correlate_foveal_volumes_with_cone_density(\n",
    "        foveal_volumes, integration_data)\n",
    "    \n",
    "    # Step 3: Plot correlation matrix using your existing function\n",
    "    print(\"\\nStep 3: Plotting correlation matrix...\")\n",
    "    plot_correlation_matrix(corr_df, annot = annot,\n",
    "                           title=\"Correlation between Foveal Volume and Cone Density\")\n",
    "\n",
    "    \n",
    "    return corr_df, merged_df, foveal_volumes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0865c47b",
   "metadata": {},
   "source": [
    "#### Fovea-CD correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45fcce",
   "metadata": {},
   "source": [
    "##### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc1a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define retinal regions (for integration)\n",
    "retinal_regions = [\n",
    "    {'name' : 'Perifovea', 'start':1, 'end': np.inf },\n",
    "    {'name': 'Parafovea', 'start': 0.75, 'end': 1 },\n",
    "    {'name': 'Fovea', 'start': 0.175, 'end': 0.75 },\n",
    "    {'name': 'Faz', 'start': 0.175, 'end': 0.25},\n",
    "    {'name': 'Foveola', 'start': 0, 'end': 0.175},\n",
    "    \n",
    "]\n",
    "\n",
    "integration_data = {}\n",
    "\n",
    "for sd in subjects_data: \n",
    "    integration_data[sd.name] = {}\n",
    "\n",
    "eccs_in_MM = eccs * MM_PER_DEGREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b6936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints the retinal thresholds in deg\n",
    "\n",
    "for region in retinal_regions:\n",
    "    print(f\"Region: {region['name']}, Start: {region['start']/MM_PER_DEGREE} deg, End: {region['end']/MM_PER_DEGREE} deg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69c573",
   "metadata": {},
   "source": [
    "##### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047fb3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fovea\n",
    "\n",
    "\n",
    "base_path = r\"P:\\AOSLO\\_automation\\_PROCESSED\\Photoreceptors\\Healthy\\_Results\"\n",
    "\n",
    "fovea_data = extract_fovea_data(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c964bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CD (Integration) data\n",
    "\n",
    "for sd in subjects_data:\n",
    "    sd_name = sd.name\n",
    "    # Find the corresponding fovea data object\n",
    "    # fovea_obj = next((f for f in foveal_data if f.subject == sd_name), None)\n",
    "    for region in retinal_regions:\n",
    "        if region['end'] >0:\n",
    "            print (f\"Integrating for subject {sd.name} in Region: {region['name']}, Start: {region['start']}, End: {region['end']}\")\n",
    "            if region['end'] == np.inf:\n",
    "                print(f\"changing radius to 3.00 for {region['name']}\")\n",
    "                radius = 3.0\n",
    "            else:\n",
    "                radius = region['end']\n",
    "            (mean_int, std_int, min_int, max_int, cov_int, \n",
    "            int_results, max_x_results, \n",
    "            max_y_results, r_grid, \n",
    "            cumulative_integrations) = integrate_cone_density_circle(\n",
    "                                                                    eccs_in_MM, np.array(sd.density_fit_X).reshape(1, -1), np.array(sd.density_fit_Y).reshape(1, -1),\n",
    "                                                                    radius=radius , exclude_center=False\n",
    "                                                                    )\n",
    "\n",
    "            integration_data[sd_name][f\"n_cones_at_{region['name']}\"] = int_results\n",
    "            integration_data[sd_name][\"max_X\"] = max_x_results\n",
    "            integration_data[sd_name][\"max_Y\"] = max_y_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de29f7b",
   "metadata": {},
   "source": [
    "##### Filling in extra data fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc8528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate peak widths at 20% or more threshold for each subject\n",
    "\n",
    "threshold_percents = [20]  # Customizable threshold\n",
    "for threshold_percent in threshold_percents:\n",
    "    for sd in subjects_data:\n",
    "        sd_name = sd.name\n",
    "        \n",
    "        # Make sure the subject exists in integration_data\n",
    "        if sd_name not in integration_data:\n",
    "            integration_data[sd_name] = {}\n",
    "        \n",
    "        # Calculate and store peak width at 20% threshold\n",
    "        x_values = np.array(sd.density_fit_X)\n",
    "        y_values = np.array(sd.density_fit_Y)\n",
    "        \n",
    "        # Create position arrays\n",
    "        x_positions = eccs_in_MM\n",
    "        y_positions = eccs_in_MM\n",
    "        \n",
    "        # Use your existing peak calculation function (assuming it's called calculate_peak_width)\n",
    "        width_x = calculate_peak_width(x_values, x_positions, threshold_percent=threshold_percent)\n",
    "        width_y = calculate_peak_width(y_values, y_positions, threshold_percent=threshold_percent)\n",
    "        \n",
    "        # Store the width measurements in integration_data\n",
    "        integration_data[sd_name][f\"peak_width_X_{threshold_percent}p\"] = width_x\n",
    "        integration_data[sd_name][f\"peak_width_Y_{threshold_percent}p\"] = width_y\n",
    "\n",
    "        # Store the peak positions if needed\n",
    "        # integration_data[sd_name][f\"peak_position_right_X_{threshold_percent}p\"] = posrightx\n",
    "        # integration_data[sd_name][f\"peak_position_left_X_{threshold_percent}p\"] = posleftx\n",
    "        # integration_data[sd_name][f\"peak_position_right_Y_{threshold_percent}p\"] = posrighty\n",
    "        # integration_data[sd_name][f\"peak_position_left_Y_{threshold_percent}p\"] = poslefty\n",
    "        \n",
    "        # # Store Alex's peak width calculations\n",
    "\n",
    "        # integration_data[sd_name][f\"peak_width_X_alex\"] = sd.width_nas + sd.width_tem\n",
    "        # integration_data[sd_name][f\"peak_width_Y_alex\"] = sd.width_sup + sd.width_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9edaa1b",
   "metadata": {},
   "source": [
    "##### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b6a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to compare peak widths for the x and y directions\n",
    "# They should be teh same, as they are inferring the CD for the same point\n",
    "# at x=0 an d y= 0\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "labels = []\n",
    "\n",
    "for sd in subjects_data:\n",
    "    subject_name = sd.name\n",
    "    if subject_name in integration_data:\n",
    "\n",
    "        x_max = np.nanmax(sd.density_fit_X)\n",
    "        \n",
    "        # Get width from integration_data\n",
    "        # width_integration = integration_data[subject_name]['peak_width_Y_20p']\n",
    "        y_max = np.nanmax(sd.density_fit_Y)\n",
    "        \n",
    "        \n",
    "        x_data.append(x_max)\n",
    "        y_data.append(y_max)\n",
    "        labels.append(subject_name)\n",
    "\n",
    "# Create scatter plot\n",
    "plt.scatter(x_data, y_data, alpha=0.7, s=80)\n",
    "\n",
    "# Add labels to each point\n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(label.replace('Subject', ''), (x_data[i], y_data[i]), \n",
    "                 fontsize=8, alpha=0.7, \n",
    "                 xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "# Add regression line\n",
    "if len(x_data) > 1:\n",
    "    z = np.polyfit(x_data, y_data, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(min(x_data), max(x_data), 100)\n",
    "    plt.plot(x_line, p(x_line), 'r--', alpha=0.8)\n",
    "    \n",
    "    # Calculate correlation and add to plot\n",
    "    r, p_val = scipy.stats.pearsonr(x_data, y_data)\n",
    "    plt.text(0.05, 0.95, f'r = {r:.2f}, p = {p_val:.4f}\\ny = {z[0]:.2f}x + {z[1]:.2f}', \n",
    "             transform=plt.gca().transAxes, fontsize=10, \n",
    "             verticalalignment='top', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "plt.xlabel('Peak CD on x axis (cones/mm²)')\n",
    "plt.ylabel('Peak CD on y axis (cones/mm²)')\n",
    "plt.title('Comparison of peak CD measurmeents in x and y directions')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2653c815",
   "metadata": {},
   "source": [
    "##### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b2939",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting analysis with:\")\n",
    "print(f\"- {len(fovea_data)} fovea parameter objects\")\n",
    "print(f\"- {len(integration_data)} subjects in integration data dictionary\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix, merged_data, annot = create_correlation_matrix(fovea_data, integration_data)\n",
    "\n",
    "if corr_matrix is not None:\n",
    "    # Plot full correlation matrix\n",
    "    plot_correlation_matrix(corr_matrix, annot=annot, title = \"Full Correlation Matrix\")\n",
    "    \n",
    "    # For a focused view, plot correlations between fovea params and regions only\n",
    "    print(\"\\nCreating focused correlation matrix (fovea params vs. region metrics)...\")\n",
    "    fovea_params = ['A00', 'A10', 'A01', 'A20', 'A02', 'A11', 'foveal_depth', \n",
    "                    'foveal_center_X', 'foveal_width_X', 'foveal_center_Y', 'foveal_width_Y', \n",
    "                    'foveal_max_slope', 'foveal_flatness', 'foveal_volume']\n",
    "    region_metrics = ['n_cones_at_Perifovea', 'n_cones_at_Parafovea', 'n_cones_at_Fovea', 'n_cones_at_Foveola', 'max_cone_density_X', 'max_cone_density_Y']\n",
    "    \n",
    "    # Filter for only valid columns that exist in the data\n",
    "    valid_fovea_params = [p for p in fovea_params if p in corr_matrix.index]\n",
    "    valid_region_metrics = [r for r in region_metrics if r in corr_matrix.columns]\n",
    "    \n",
    "    print(f\"Using {len(valid_fovea_params)} fovea parameters and {len(valid_region_metrics)} region metrics\")\n",
    "    \n",
    "    # Get only the correlations between parameters and regions\n",
    "\n",
    "    \n",
    "    focused_corr = corr_matrix.loc[valid_fovea_params, valid_region_metrics]\n",
    "    focused_annot = annot.loc[valid_fovea_params, valid_region_metrics]\n",
    "\n",
    "\n",
    "    plot_correlation_matrix(focused_corr, focused_annot, \"Fovea Parameters vs Region Metrics\")\n",
    "    \n",
    "    # Print descriptive statistics for the merged data\n",
    "    print(\"\\nDescriptive Statistics:\")\n",
    "    print(merged_data[valid_fovea_params + valid_region_metrics].describe())\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "else:\n",
    "    print(\"\\nERROR: Could not create correlation matrix due to no matching data.\")\n",
    "    print(\"Please check that subject identifiers match between the two datasets.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aoslo12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

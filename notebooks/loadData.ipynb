{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3a471e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a40ef",
   "metadata": {},
   "source": [
    "### Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ce513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "from pytest import param\n",
    "from zmq import has"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7054f",
   "metadata": {},
   "source": [
    "### Plot Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotx\n",
    "\n",
    "# plt.style.use('science')  # Use ggplot style for all plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)  # Default figure size\n",
    "plt.rcParams['figure.dpi'] = 300  # Default figure dpi\n",
    "plt.rcParams['font.size'] = 12  # Default font size\n",
    "plt.rcParams['lines.linewidth'] = 2  # Default line width\n",
    "plt.rcParams['axes.labelsize'] = 14  # Default label size\n",
    "plt.rcParams['axes.titlesize'] = 16  # Default title size\n",
    "plt.rcParams['xtick.labelsize'] = 12  # Default x-tick label size\n",
    "plt.rcParams['ytick.labelsize'] = 12  # Default y-tick label size\n",
    "plt.rcParams['legend.fontsize'] = 12  # Default legend font size\n",
    "plt.rcParams['figure.titlesize'] = 18  # Default figure title size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee85ec",
   "metadata": {},
   "source": [
    "## Gathering subjects' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad41af",
   "metadata": {},
   "source": [
    "### Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c67b8",
   "metadata": {},
   "source": [
    "data structure to store subject data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class SubjectData:\n",
    "    name: str = None\n",
    "    pid: str = None\n",
    "    nb: int = None\n",
    "    session: str = None\n",
    "\n",
    "    width_nas: float = None\n",
    "    width_tem: float = None\n",
    "    width_inf: float = None\n",
    "    width_sup: float = None\n",
    "    max_slope_nas: float = None\n",
    "    max_slope_tem: float = None\n",
    "    max_slope_inf: float = None\n",
    "    max_slope_sup: float = None\n",
    "\n",
    "    oct_bump_X: float = None\n",
    "    oct_bump_Y: float = None\n",
    "    oct_width_X: float = None\n",
    "    oct_width_Y: float = None\n",
    "    oct_max_slope: float = None\n",
    "    oct_depth: float = None\n",
    "    oct_flatness: float = None\n",
    "\n",
    "    age: float = None\n",
    "    axial_length: float = None\n",
    "    spherical_equiv: float = None\n",
    "    sex: int = None\n",
    "\n",
    "    eccs: np.ndarray = None\n",
    "    density_X: pd.Series = None\n",
    "    density_Y: pd.Series = None\n",
    "    density_fit_X: pd.Series = None\n",
    "    density_fit_Y: pd.Series = None\n",
    "\n",
    "    cvi_X: pd.Series = None\n",
    "    cvi_Y: pd.Series = None\n",
    "    gcl_ipl_X: pd.Series = None\n",
    "    gcl_ipl_Y: pd.Series = None\n",
    "    onl_X: pd.Series = None\n",
    "    onl_Y: pd.Series = None\n",
    "    inl_opl_X: pd.Series = None\n",
    "    inl_opl_Y: pd.Series = None\n",
    "    rnfl_X: pd.Series = None\n",
    "    rnfl_Y: pd.Series = None\n",
    "    chrd_X: pd.Series = None\n",
    "    chrd_Y: pd.Series = None\n",
    "    pr_rpe_X: pd.Series = None\n",
    "    pr_rpe_Y: pd.Series = None\n",
    "    os_X: pd.Series = None\n",
    "    os_Y: pd.Series = None\n",
    "\n",
    "    nb_cones: float = None\n",
    "    nb_cones_fit: float = None\n",
    "\n",
    "    width_gcl_X: float = None\n",
    "    width_gcl_Y: float = None\n",
    "    min_thick_gcl: float = None\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoveaParams:\n",
    "    \"\"\"Class for storing fovea 3D fitted parameters.\"\"\"\n",
    "    # Patient information\n",
    "    subject: str\n",
    "    patient_id: str\n",
    "    subject_folder: str\n",
    "    trial_name: str\n",
    "    age: Optional[int] = None\n",
    "\n",
    "    # Fitted parameters\n",
    "    A00: Optional[float] = None\n",
    "    A10: Optional[float] = None\n",
    "    A01: Optional[float] = None\n",
    "    A20: Optional[float] = None\n",
    "    A02: Optional[float] = None\n",
    "    A11: Optional[float] = None\n",
    "    foveal_depth: Optional[float] = None\n",
    "    foveal_center_X: Optional[float] = None\n",
    "    foveal_width_X: Optional[float] = None\n",
    "    foveal_center_Y: Optional[float] = None\n",
    "    foveal_width_Y: Optional[float] = None\n",
    "    foveal_max_slope: Optional[float] = None\n",
    "    foveal_flatness: Optional[float] = None\n",
    "    foveal_volume: Optional[float] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54482948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# here to avoid having to rerun the pipeline for -\n",
    "# all subjects everytime i want to test something on the model.\n",
    " \n",
    "# Since the list of subjects is ordered by strings , it goes from 10 to 100 to 103 etc...\n",
    "# which requires a bit of work to get the first 5 subjects\n",
    "\n",
    "#It will later be used to extract the first 5 subjects from the list of subject_data\n",
    "\n",
    "take_first_five = False\n",
    "first_five_subjects = [\"Subject10\",\"Subject100\",\"Subject101\",\"Subject104\",\"Subject105\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9893f9",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716aec0",
   "metadata": {},
   "source": [
    "#### Foveal Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1979fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fovea_data(base_path: str) -> List[FoveaParams]:\n",
    "    \"\"\"\n",
    "    Extract fovea parameters from CSV files with known structure.\n",
    "    \n",
    "    Args:\n",
    "        base_path: Path to the base directory containing subject folders\n",
    "        (subjfolder/trialfolder/layer_new/fovea_3d_fitted_params.csv)\n",
    "    \n",
    "    Returns:\n",
    "        List of FoveaParams objects, one for each found CSV file\n",
    "    \"\"\"\n",
    "    fovea_data = []\n",
    "    \n",
    "    # Get only the subject directories (directories starting with \"Subject\")\n",
    "    try:\n",
    "        subject_dirs = [d for d in os.listdir(base_path) \n",
    "                      if os.path.isdir(os.path.join(base_path, d)) and d.startswith(\"Subject\")]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Base path not found: {base_path}\")\n",
    "        return []\n",
    "    \n",
    "    # For each subject directory, get the session directories\n",
    "    for subject_dir in subject_dirs:\n",
    "        subject_path = os.path.join(base_path, subject_dir)\n",
    "        \n",
    "        # Extract subject number\n",
    "        import re\n",
    "        subject_match = re.search(r'Subject(\\d+)', subject_dir)\n",
    "        if not subject_match:\n",
    "            continue\n",
    "            \n",
    "        subject_num = subject_match.group(1)\n",
    "        patient_id = f\"{subject_num}\"\n",
    "        \n",
    "        try:\n",
    "            # Get session directories (directories starting with \"Session\")\n",
    "            session_dirs = [d for d in os.listdir(subject_path) \n",
    "                          if os.path.isdir(os.path.join(subject_path, d)) and d.startswith(\"Session\")]\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        \n",
    "        # For each session directory, check if the CSV file exists\n",
    "        for session_dir in session_dirs:\n",
    "            session_path = os.path.join(subject_path, session_dir)\n",
    "            csv_path = os.path.join(session_path, \"layer_new\", \"fovea_3d_fitted_params.csv\")\n",
    "            \n",
    "            # Check if the CSV file exists\n",
    "            if os.path.isfile(csv_path):\n",
    "                # try:\n",
    "                # Read CSV file\n",
    "                df = pd.read_csv(csv_path, sep=';', header=None, names=['param', 'value'])\n",
    "                \n",
    "                # Create basic FoveaParams object with patient info\n",
    "                fovea_obj = FoveaParams(\n",
    "                    patient_id=str(patient_id),\n",
    "                    subject=f\"Subject{patient_id}\",\n",
    "                    subject_folder=subject_dir,\n",
    "                    trial_name=session_dir\n",
    "                )\n",
    "                print(f\"Processing file: {csv_path} for patient {patient_id}\")\n",
    "                \n",
    "                # Fill in parameter values\n",
    "                for _, row in df.iterrows():\n",
    "                    param_name = row['param']\n",
    "                    param_value = row['value']\n",
    "                    print(f\"Processing {param_name} with value {param_value} for patient {patient_id}\")\n",
    "                    \n",
    "                    if param_value == \"params\":\n",
    "                        print(f\"Skipping parameter {param_name} for patient {patient_id} as it is 'params'\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if this parameter exists in our class\n",
    "                    if hasattr(fovea_obj, param_name) or hasattr(fovea_obj, f\"foveal_{param_name}\"):\n",
    "                        print(f\"Setting {param_name} for patient {patient_id}\")\n",
    "                        try:\n",
    "                            # Convert to float and set attribute\n",
    "                            print(f\"Trying to set {param_name} for patient {patient_id}\")\n",
    "                            setattr(fovea_obj, param_name, float(param_value))\n",
    "                            print(f\"Successfully set {param_name} for patient {patient_id}\")\n",
    "                        except:\n",
    "                            \n",
    "                            print(f\"Error setting {param_name} for patient {patient_id}: {param_value}\")\n",
    "\n",
    "                    if hasattr(fovea_obj, f\"foveal_{param_name}\"):\n",
    "                        try:\n",
    "                            print(f\"Trying to set foveal_{param_name} for patient {patient_id}\")\n",
    "                            setattr(fovea_obj, f\"foveal_{param_name}\", float(param_value))\n",
    "\n",
    "                        except:\n",
    "                            print(f\"Skipping parameter {param_name} for patient {patient_id} \")\n",
    "\n",
    "                            pass\n",
    "                    \n",
    "                fovea_data.append(fovea_obj)\n",
    "                # except Exception as e:\n",
    "                    # print(f\"Error processing file {csv_path}: {str(e)}\")\n",
    "    \n",
    "    return fovea_data\n",
    "\n",
    "def save_to_dataframe(fovea_data: List[FoveaParams], output_file: str = \"fovea_parameters.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert list of FoveaParams objects to a pandas DataFrame and save to CSV.\n",
    "    \n",
    "    Args:\n",
    "        fovea_data: List of FoveaParams objects\n",
    "        output_file: Path to save the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing all fovea data\n",
    "    \"\"\"\n",
    "    # Convert to list of dictionaries\n",
    "    data_dicts = [vars(f) for f in fovea_data]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_dicts)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586969b",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b37ce",
   "metadata": {},
   "source": [
    "#### Populating Additional fields based on the previously gathered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_cones(ecc: np.ndarray, dens_X: pd.Series, dens_Y: pd.Series, radius: float, smoothen: bool = True) -> float:\n",
    "    \n",
    "    '''\n",
    "    Given the cone density profiles along the X and Y axes, compute the total number of cones within a disk of radius `radius` (in degree) centered at the fovea by linearly interpolating (radially) the density profiles and integrating over the disk.\n",
    "    '''\n",
    "    smthd_x = gaussian_filter_nan(dens_X, sigma=4) if smoothen else dens_X.to_numpy()\n",
    "    smthd_y = gaussian_filter_nan(dens_Y, sigma=4) if smoothen else dens_Y.to_numpy()\n",
    "   \n",
    "    x_amax = np.nanargmax(smthd_x)\n",
    "    p = np.polyfit(ecc[x_amax-2:x_amax+3], smthd_x[x_amax-2:x_amax+3], 2)\n",
    "    x_amax = -p[1] / (2 * p[0])\n",
    "\n",
    "    y_amax = np.nanargmax(smthd_y)\n",
    "    p = np.polyfit(ecc[y_amax-2:y_amax+3], smthd_y[y_amax-2:y_amax+3], 2)\n",
    "    y_amax = -p[1] / (2 * p[0])\n",
    "\n",
    "    R = np.linspace(0.0001, radius, 500) # radius in degrees\n",
    "    disk = np.r_[\n",
    "        np.interp(x_amax + R, ecc, smthd_x),\n",
    "        np.interp(x_amax - R, ecc, smthd_x),\n",
    "        np.interp(y_amax + R, ecc, smthd_y),\n",
    "        np.interp(y_amax - R, ecc, smthd_y)\n",
    "    ]\n",
    "    \n",
    "    norm_coef = MM_PER_DEGREE**2 * 2 * np.pi\n",
    "    # integrate cone density over disk to get total nb of cones\n",
    "    return norm_coef * np.trapz(np.nanmean(disk, axis=0) * R, R)\n",
    "\n",
    "RADIUS = 3.33 # degree\n",
    "for sd in subjects_data:\n",
    "    sd.nb_cones = get_nb_cones(sd.eccs, sd.density_X, sd.density_Y, radius = RADIUS)\n",
    "    sd.nb_cones_fit = get_nb_cones(sd.eccs, sd.density_fit_X, sd.density_fit_Y, radius = RADIUS, smoothen=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff70b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "def adjust_flat(gcl_data: np.ndarray, peak_left: int, peak_right: int) -> np.ndarray:\n",
    "    slope = (gcl_data[peak_right] - gcl_data[peak_left]) / (peak_right - peak_left)\n",
    "    transformed_gcl = gcl_data - slope * (np.arange(len(gcl_data)) - peak_left)\n",
    "    return transformed_gcl\n",
    "\n",
    "def get_gcl_width(gcl: pd.Series) -> Tuple[float, float]:\n",
    "    '''\n",
    "    Given the GCL+IPL thickness profile, compute the width of the pit as well as the minimum thickness of the layer. Here, the width of the pit is defined as the distance between the two points where the thickness is 20% of the depth of the pit. The depth of the pit is defined as the difference between the thickness surrounding the pit and the thickness at the pit's bottom.\n",
    "    '''\n",
    "    # name = gcl.name\n",
    "    gcl_to_plot = gcl.copy()\n",
    "    eccs = gcl[np.abs(gcl.index) <= 6].index.to_numpy()\n",
    "    gcl = gcl.interpolate(method='polynomial', order=1)[eccs].to_numpy()\n",
    "    # plt.plot(eccs, gcl, label=name)\n",
    "    smooth_param = 3\n",
    "    peak_left = peak_right = []\n",
    "    while not (len(peak_left) >= 1 and len(peak_right) >= 1) and smooth_param < 10:\n",
    "        smoothed_gcl = gaussian_filter_nan(gcl, smooth_param)\n",
    "        peaks = find_peaks(smoothed_gcl)[0]\n",
    "        peak_left  = [peak for peak in peaks if peak < len(smoothed_gcl) / 3]\n",
    "        peak_right = [peak for peak in peaks if peak > 2 * len(smoothed_gcl) / 3]\n",
    "        smooth_param += 1\n",
    "    assert len(peak_left) >= 1 and len(peak_right) >= 1, f'No peaks found for {gcl.name}'\n",
    "    peak_left = round(np.mean(peak_left))   \n",
    "    peak_right = round(np.mean(peak_right))\n",
    "    adjusted_gcl = adjust_flat(gcl, peak_left, peak_right)\n",
    "    smoothed_aj_gcl = gaussian_filter_nan(adjusted_gcl, 2)\n",
    "\n",
    "    y_min = np.nanmin(smoothed_aj_gcl[peak_left:peak_right])\n",
    "    y_target = y_min + (smoothed_aj_gcl[peak_left] - y_min) / 5\n",
    "    intercepts = np.where(np.diff(np.sign(smoothed_aj_gcl - y_target)))[0]\n",
    "    leftmost = eccs[intercepts[0]]\n",
    "    rightmost = eccs[intercepts[-1]+1]\n",
    "    width_pit_gcl = rightmost - leftmost\n",
    "\n",
    "    indicies = np.argpartition(gcl, 10)[:10]\n",
    "    p = np.polyfit(eccs[indicies], gcl[indicies], 2)\n",
    "    if p[0] == 0:\n",
    "    #     # gcl_to_plot.plot()\n",
    "    #     plt.plot(eccs, gcl, label='gcl')\n",
    "        plt.plot(np.sort(eccs[indicies]), np.polyval(p, np.sort(eccs[indicies])), '--')\n",
    "    min_thickness_gcl = np.polyval(p, -p[1] / (2 * p[0]))\n",
    "    return width_pit_gcl, min_thickness_gcl\n",
    "\n",
    "for sd in subjects_data:\n",
    "    width_gcl_x, min_thick_x = get_gcl_width(sd.gcl_ipl_X)\n",
    "    width_gcl_y, min_thick_y = get_gcl_width(sd.gcl_ipl_Y)\n",
    "    sd.width_gcl_X = width_gcl_x\n",
    "    sd.width_gcl_Y = width_gcl_y\n",
    "    sd.min_thick_gcl = min(min_thick_x, min_thick_y)\n",
    "    # print(f'{sd.name:>10}: {width_gcl_x:.2f}째, {depth_gcl_x:.4f}, {width_gcl_y:.2f}째, {depth_gcl_y:.4f}')\n",
    "    # plt.xlim(-6, 6)\n",
    "    # plt.legend()\n",
    "    # plt.title(sd.name)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6a1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Subject10 Session279...\n",
      "Loading Subject11 Session241...\n",
      "Loading Subject12 Session239...\n",
      "Loading Subject15 Session254...\n",
      "Loading Subject16 Session246...\n",
      "Loading Subject19 Session248...\n",
      "Loading Subject24 Session298...\n",
      "Loading Subject27 Session270...\n",
      "Loading Subject28 Session251...\n",
      "Loading Subject29 Session258...\n",
      "Loading Subject33 Session272...\n",
      "Loading Subject34 Session274...\n",
      "Loading Subject36 Session284...\n",
      "Loading Subject37 Session285...\n",
      "Loading Subject38 Session288...\n",
      "Loading Subject39 Session290...\n",
      "Loading Subject40 Session292...\n",
      "Loading Subject41 Session294...\n",
      "Loading Subject47 Session311...\n",
      "Loading Subject48 Session314...\n",
      "Loading Subject49 Session316...\n",
      "Loading Subject51 Session323...\n",
      "Loading Subject53 Session334...\n",
      "Loading Subject55 Session337...\n",
      "Loading Subject91 Session428...\n",
      "Loading Subject92 Session431...\n",
      "Loading Subject93 Session434...\n",
      "Loading Subject94 Session436...\n",
      "Loading Subject98 Session450...\n",
      "Loading Subject101 Session470...\n",
      "Loading Subject104 Session492...\n",
      "Loading Subject107 Session503...\n",
      "Loading Subject108 Session506...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from src.cell.analysis.constants import MM_PER_DEGREE\n",
    "from src.cell.layer.helpers import gaussian_filter_nan\n",
    "from src.configs.parser import Parser\n",
    "\n",
    "Parser.initialize()\n",
    "\n",
    "subjects_sessions = [[int(n) for n in s.strip().split()] for s in open('src/processed.txt').readlines()] \n",
    "\n",
    "\n",
    "try:\n",
    "    sheet = pd.ExcelFile(r'V:\\Studies\\AOSLO\\data\\cohorts\\AOSLO healthy\\DATA_HC+DM.xlsx').parse('Healthy', header=0, nrows=45, index_col=0)\n",
    "    sheet.index = sheet.index.map(lambda x: f'Subject{x}')\n",
    "    age_dict = ((sheet['Date of visit'] - sheet['DDN']).dt.days / 365).to_dict()\n",
    "    axial_dict = sheet['AL D (mm)'].where(sheet['Laterality'] == 'OD', sheet['AL G (mm)']).to_dict()\n",
    "    spherical_dict = sheet['Equi Sph D'].where(sheet['Laterality'] == 'OD', sheet['Equi Sph G']).to_dict()\n",
    "    sex_dict = sheet['Sexe'].map(lambda x: 1 if x == 'F' else 0).to_dict()\n",
    "except:\n",
    "    # if the excel file is not found, use a hardcoded dictionary\n",
    "    age_dict = {}\n",
    "base_path = Path(r'P:\\AOSLO\\_automation\\_PROCESSED\\Photoreceptors\\Healthy\\_Results')\n",
    "\n",
    "# look-up table for subject and session numbers\n",
    "\n",
    "\n",
    "# subject for which OCTs are tilted (white dot is not well aligned with PR+RPE peak)\n",
    "# see explanation in `PRxRLT_expmanual.ipynb`\n",
    "oct_to_exclude = {\n",
    "    13, 18, 20, 25, 26, 30, 35, 42, 46, 66, 100, 105,\n",
    "} \n",
    "\n",
    "\n",
    "subjects_data: List[SubjectData] = []\n",
    "for subject_n, session_n in subjects_sessions:\n",
    "    if subject_n in oct_to_exclude:\n",
    "        continue\n",
    "\n",
    "    sd = SubjectData()\n",
    "    sd.name = f'Subject{subject_n}'\n",
    "    sd.pid = f'AOHC_{subject_n}'\n",
    "    sd.nb = subject_n\n",
    "    sd.session = f'Session{session_n}'\n",
    "\n",
    "    #\n",
    "    path = base_path / sd.name / sd.session\n",
    "    print(f'Loading {sd.name} {sd.session}...')\n",
    "\n",
    "    # record subject's metadata from the excel sheet\n",
    "    sd.age = age_dict[sd.name]\n",
    "    sd.axial_length = axial_dict[sd.name]\n",
    "    sd.spherical_equiv = spherical_dict[sd.name]\n",
    "    sd.sex = sex_dict[sd.name]\n",
    "\n",
    "    # record foveal shape parameters (populated by `src/save_layer_features.ipynb`)\n",
    "    df_oct = pd.read_csv(path / Parser.get_layer_thickness_dir() / 'fovea_3d_fitted_params.csv', sep=';', index_col=0)\n",
    "    sd.oct_bump_X = df_oct.loc['A20', 'params']\n",
    "    sd.oct_bump_Y = df_oct.loc['A02', 'params']\n",
    "    sd.oct_width_X = df_oct.loc['width_X', 'params'] * np.sqrt(2 * 2.8) / MM_PER_DEGREE # in 째\n",
    "    sd.oct_width_Y = df_oct.loc['width_Y', 'params'] * np.sqrt(2 * 2.8) / MM_PER_DEGREE # in 째\n",
    "    sd.oct_max_slope = df_oct.loc['max_slope', 'params']\n",
    "    sd.oct_depth = df_oct.loc['depth', 'params'] # in mm\n",
    "    sd.oct_flatness = df_oct.loc['flatness', 'params']\n",
    "    # sd.oct_volume = df_oct.loc['volume', 'params']\n",
    "\n",
    "    # record cone density and fitted parameters (populated by `src/cell/analysis/density_analysis_pipeline_manager.py`)\n",
    "    df_density = pd.read_csv(path / Parser.get_density_analysis_dir() / 'densities.csv', sep=';', index_col=0)\n",
    "    df_raw_density_x = pd.read_csv(path / Parser.get_density_analysis_dir() / 'densities_raw_x.csv', sep=';', index_col=0)\n",
    "    df_raw_density_y = pd.read_csv(path / Parser.get_density_analysis_dir() / 'densities_raw_y.csv', sep=';', index_col=0)\n",
    "    \n",
    "    sd.width_nas = df_density['width_nasal'].iloc[0]\n",
    "    sd.width_tem = df_density['width_temporal'].iloc[0]\n",
    "    sd.width_inf = df_density['width_inferior'].iloc[0]\n",
    "    sd.width_sup = df_density['width_superior'].iloc[0]\n",
    "    sd.max_slope_nas = df_density['max_slope_nasal'].iloc[0]\n",
    "    sd.max_slope_tem = df_density['max_slope_temporal'].iloc[0]\n",
    "    sd.max_slope_inf = df_density['max_slope_inferior'].iloc[0]\n",
    "    sd.max_slope_sup = df_density['max_slope_superior'].iloc[0]\n",
    "    sd.density_X = df_density['dens_smthd_X']\n",
    "    sd.density_Y = df_density['dens_smthd_Y']\n",
    "    sd.density_fit_X = df_density['dens_fit_X']\n",
    "    sd.density_fit_Y = df_density['dens_fit_Y']\n",
    "    \n",
    "    sd.eccs = df_density.index.to_numpy()\n",
    "\n",
    "    # record layer thicknesses (populated by `src/save_layer_features.ipynb`)\n",
    "    df_thick = pd.read_csv(path / Parser.get_density_analysis_dir() / 'results.csv', sep=',', index_col=0, skiprows=1).query('-10 <= index <= 10')\n",
    "    sd.cvi_X = df_thick['CVI_X']\n",
    "    sd.cvi_Y = df_thick['CVI_Y']\n",
    "    sd.gcl_ipl_X = df_thick['GCL+IPL_X']\n",
    "    sd.gcl_ipl_Y = df_thick['GCL+IPL_Y']\n",
    "    sd.onl_X = df_thick['ONL_X']\n",
    "    sd.onl_Y = df_thick['ONL_Y']\n",
    "    sd.inl_opl_X = df_thick['INL+OPL_X']\n",
    "    sd.inl_opl_Y = df_thick['INL+OPL_Y']\n",
    "    sd.rnfl_X = df_thick['RNFL_X']\n",
    "    sd.rnfl_Y = df_thick['RNFL_Y']\n",
    "    sd.chrd_X = df_thick['Choroid_X']\n",
    "    sd.chrd_Y = df_thick['Choroid_Y']\n",
    "    sd.pr_rpe_X = df_thick['PhotoR+RPE_X']\n",
    "    sd.pr_rpe_Y = df_thick['PhotoR+RPE_Y']\n",
    "    sd.os_X = df_thick['OS_X']\n",
    "    sd.os_Y = df_thick['OS_Y']\n",
    "\n",
    "    subjects_data.append(sd)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')  # Use ggplot style for all plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)  # Default figure size\n",
    "plt.rcParams['figure.dpi'] = 300  # Default figure dpi\n",
    "plt.rcParams['font.size'] = 12  # Default font size\n",
    "plt.rcParams['lines.linewidth'] = 2  # Default line width\n",
    "plt.rcParams['axes.labelsize'] = 14  # Default label size\n",
    "plt.rcParams['axes.titlesize'] = 16  # Default title size\n",
    "plt.rcParams['xtick.labelsize'] = 12  # Default x-tick label size\n",
    "plt.rcParams['ytick.labelsize'] = 12  # Default y-tick label size\n",
    "plt.rcParams['legend.fontsize'] = 12  # Default legend font size\n",
    "plt.rcParams['figure.titlesize'] = 18  # Default figure title size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering subjects' data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data structure to store subject data; feel free to add more fields if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class SubjectData:\n",
    "    name: str = None\n",
    "    pid: str = None\n",
    "    nb: int = None\n",
    "    session: str = None\n",
    "\n",
    "    width_nas: float = None\n",
    "    width_tem: float = None\n",
    "    width_inf: float = None\n",
    "    width_sup: float = None\n",
    "    max_slope_nas: float = None\n",
    "    max_slope_tem: float = None\n",
    "    max_slope_inf: float = None\n",
    "    max_slope_sup: float = None\n",
    "\n",
    "    oct_bump_X: float = None\n",
    "    oct_bump_Y: float = None\n",
    "    oct_width_X: float = None\n",
    "    oct_width_Y: float = None\n",
    "    oct_max_slope: float = None\n",
    "    oct_depth: float = None\n",
    "    oct_flatness: float = None\n",
    "\n",
    "    age: float = None\n",
    "    axial_length: float = None\n",
    "    spherical_equiv: float = None\n",
    "    sex: int = None\n",
    "\n",
    "    eccs: np.ndarray = None\n",
    "    density_X: pd.Series = None\n",
    "    density_Y: pd.Series = None\n",
    "    density_fit_X: pd.Series = None\n",
    "    density_fit_Y: pd.Series = None\n",
    "\n",
    "    cvi_X: pd.Series = None\n",
    "    cvi_Y: pd.Series = None\n",
    "    gcl_ipl_X: pd.Series = None\n",
    "    gcl_ipl_Y: pd.Series = None\n",
    "    onl_X: pd.Series = None\n",
    "    onl_Y: pd.Series = None\n",
    "    inl_opl_X: pd.Series = None\n",
    "    inl_opl_Y: pd.Series = None\n",
    "    rnfl_X: pd.Series = None\n",
    "    rnfl_Y: pd.Series = None\n",
    "    chrd_X: pd.Series = None\n",
    "    chrd_Y: pd.Series = None\n",
    "    pr_rpe_X: pd.Series = None\n",
    "    pr_rpe_Y: pd.Series = None\n",
    "    os_X: pd.Series = None\n",
    "    os_Y: pd.Series = None\n",
    "\n",
    "    nb_cones: float = None\n",
    "    nb_cones_fit: float = None\n",
    "\n",
    "    width_gcl_X: float = None\n",
    "    width_gcl_Y: float = None\n",
    "    min_thick_gcl: float = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gian Notes \n",
    "\n",
    "ONL we have mostly Muller Cells \n",
    "More cones present -> Superior in the PhotoR: the cones are standing right up, the shape might be thicker because of the infrastructure change\n",
    "Whz would the INL be more thick at the Superior and thinner at the Inferior "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "populate the data structure by gathering data from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# here to avoid having to rerun the pipeline for -\n",
    "# all subjects everytime i want to test something on the model.\n",
    " \n",
    "# Since the list of subjects is ordered by strings , it goes from 10 to 100 to 103 etc...\n",
    "# which requires a bit of work to get the first 5 subjects\n",
    "\n",
    "#It will later be used to extract the first 5 subjects from the list of subject_data\n",
    "\n",
    "take_first_five = False\n",
    "first_five_subjects = [\"Subject10\",\"Subject100\",\"Subject101\",\"Subject104\",\"Subject105\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from src.cell.analysis.constants import MM_PER_DEGREE\n",
    "from src.cell.layer.helpers import gaussian_filter_nan\n",
    "from src.configs.parser import Parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Parser.initialize()\n",
    "\n",
    "sheet = pd.ExcelFile(r'V:\\Studies\\AOSLO\\data\\cohorts\\AOSLO healthy\\DATA_HC+DM.xlsx').parse('Healthy', header=0, nrows=45, index_col=0)\n",
    "sheet.index = sheet.index.map(lambda x: f'Subject{x}')\n",
    "age_dict = ((sheet['Date of visit'] - sheet['DDN']).dt.days / 365).to_dict()\n",
    "axial_dict = sheet['AL D (mm)'].where(sheet['Laterality'] == 'OD', sheet['AL G (mm)']).to_dict()\n",
    "spherical_dict = sheet['Equi Sph D'].where(sheet['Laterality'] == 'OD', sheet['Equi Sph G']).to_dict()\n",
    "sex_dict = sheet['Sexe'].map(lambda x: 1 if x == 'F' else 0).to_dict()\n",
    "\n",
    "base_path = Path(r'P:\\AOSLO\\_automation\\_PROCESSED\\Photoreceptors\\Healthy\\_Results')\n",
    "\n",
    "# look-up table for subject and session numbers\n",
    "subjects_sessions = [[int(n) for n in s.strip().split()] for s in open(r'P:\\AOSLO\\_automation\\_PROCESSED\\Photoreceptors\\Healthy\\processed.txt').readlines()] \n",
    "\n",
    "# subject for which OCTs are tilted (white dot is not well aligned with PR+RPE peak)\n",
    "# see explanation in `PRxRLT_expmanual.ipynb`\n",
    "oct_to_exclude = {\n",
    "    13, 18, 20, 25, 26, 30, 35, 42, 46, 66, 100, 105,\n",
    "} \n",
    "\n",
    "\n",
    "subjects_data: List[SubjectData] = []\n",
    "for subject_n, session_n in subjects_sessions:\n",
    "    if subject_n in oct_to_exclude:\n",
    "        continue\n",
    "\n",
    "    sd = SubjectData()\n",
    "    sd.name = f'Subject{subject_n}'\n",
    "    sd.pid = f'AOHC_{subject_n}'\n",
    "    sd.nb = subject_n\n",
    "    sd.session = f'Session{session_n}'\n",
    "\n",
    "    #\n",
    "    path = base_path / sd.name / sd.session\n",
    "    print(f'Loading {sd.name} {sd.session}...')\n",
    "\n",
    "    # record subject's metadata from the excel sheet\n",
    "    sd.age = age_dict[sd.name]\n",
    "    sd.axial_length = axial_dict[sd.name]\n",
    "    sd.spherical_equiv = spherical_dict[sd.name]\n",
    "    sd.sex = sex_dict[sd.name]\n",
    "\n",
    "    # record foveal shape parameters (populated by `src/save_layer_features.ipynb`)\n",
    "    df_oct = pd.read_csv(path / Parser.get_layer_thickness_dir() / 'fovea_3d_fitted_params.csv', sep=';', index_col=0)\n",
    "    sd.oct_bump_X = df_oct.loc['A20', 'params']\n",
    "    sd.oct_bump_Y = df_oct.loc['A02', 'params']\n",
    "    sd.oct_width_X = df_oct.loc['width_X', 'params'] * np.sqrt(2 * 2.8) / MM_PER_DEGREE # in °\n",
    "    sd.oct_width_Y = df_oct.loc['width_Y', 'params'] * np.sqrt(2 * 2.8) / MM_PER_DEGREE # in °\n",
    "    sd.oct_max_slope = df_oct.loc['max_slope', 'params']\n",
    "    sd.oct_depth = df_oct.loc['depth', 'params'] # in mm\n",
    "    sd.oct_flatness = df_oct.loc['flatness', 'params']\n",
    "    # sd.oct_volume = df_oct.loc['volume', 'params']\n",
    "\n",
    "    # record cone density and fitted parameters (populated by `src/cell/analysis/density_analysis_pipeline_manager.py`)\n",
    "    df_density = pd.read_csv(path / Parser.get_density_analysis_dir() / 'densities_test2907.csv', sep=';', index_col=0)\n",
    "    df_raw_density_x = pd.read_csv(path / Parser.get_density_analysis_dir() / 'densities_raw_x.csv', sep=';', index_col=0)\n",
    "    df_raw_density_y = pd.read_csv(path / Parser.get_density_analysis_dir() / 'densities_raw_y.csv', sep=';', index_col=0)\n",
    "    \n",
    "    sd.width_nas = df_density['width_nasal'].iloc[0]\n",
    "    sd.width_tem = df_density['width_temporal'].iloc[0]\n",
    "    sd.width_inf = df_density['width_inferior'].iloc[0]\n",
    "    sd.width_sup = df_density['width_superior'].iloc[0]\n",
    "    sd.max_slope_nas = df_density['max_slope_nasal'].iloc[0]\n",
    "    sd.max_slope_tem = df_density['max_slope_temporal'].iloc[0]\n",
    "    sd.max_slope_inf = df_density['max_slope_inferior'].iloc[0]\n",
    "    sd.max_slope_sup = df_density['max_slope_superior'].iloc[0]\n",
    "    sd.density_X = df_density['dens_smthd_X']\n",
    "    sd.density_Y = df_density['dens_smthd_Y']\n",
    "    sd.density_fit_X = df_density['dens_fit_X']\n",
    "    sd.density_fit_Y = df_density['dens_fit_Y']\n",
    "    \n",
    "    sd.eccs = df_density.index.to_numpy()\n",
    "\n",
    "    # record layer thicknesses (populated by `src/save_layer_features.ipynb`)\n",
    "    df_thick = pd.read_csv(path / Parser.get_density_analysis_dir() / 'results.csv', sep=',', index_col=0, skiprows=1).query('-10 <= index <= 10')\n",
    "    sd.cvi_X = df_thick['CVI_X']\n",
    "    sd.cvi_Y = df_thick['CVI_Y']\n",
    "    sd.gcl_ipl_X = df_thick['GCL+IPL_X']\n",
    "    sd.gcl_ipl_Y = df_thick['GCL+IPL_Y']\n",
    "    sd.onl_X = df_thick['ONL_X']\n",
    "    sd.onl_Y = df_thick['ONL_Y']\n",
    "    sd.inl_opl_X = df_thick['INL+OPL_X']\n",
    "    sd.inl_opl_Y = df_thick['INL+OPL_Y']\n",
    "    sd.rnfl_X = df_thick['RNFL_X']\n",
    "    sd.rnfl_Y = df_thick['RNFL_Y']\n",
    "    sd.chrd_X = df_thick['Choroid_X']\n",
    "    sd.chrd_Y = df_thick['Choroid_Y']\n",
    "    sd.pr_rpe_X = df_thick['PhotoR+RPE_X']\n",
    "    sd.pr_rpe_Y = df_thick['PhotoR+RPE_Y']\n",
    "    sd.os_X = df_thick['OS_X']\n",
    "    sd.os_Y = df_thick['OS_Y']\n",
    "\n",
    "    subjects_data.append(sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics / Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells are used to visualize the data just extracted, mainly in terms of densities and layer thicknesses, to serve as a first analysis and as debugging for the data extracted from the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots density for a subject\n",
    "\n",
    "from sympy import plot\n",
    "\n",
    "\n",
    "def plot_density(subject_data: SubjectData):\n",
    "    plt.figure()\n",
    "    plt.plot(subject_data.eccs, subject_data.density_X, label='X')\n",
    "    plt.plot(subject_data.eccs, subject_data.density_Y, label='Y')\n",
    "    # plt.plot(subject_data.eccs, subject_data.density_fit_X, label='X fit')\n",
    "    # plt.plot(subject_data.eccs, subject_data.density_fit_Y, label='Y fit')\n",
    "    plt.xlabel('Eccentricity (°)')\n",
    "    plt.ylabel('Density (cells/deg²)')\n",
    "    plt.title(subject_data.name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_density(subjects_data[0])\n",
    "\n",
    "#plots thickness for a subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots density for a subject\n",
    "\n",
    "from sympy import plot\n",
    "\n",
    "\n",
    "def plot_density(subject_data: SubjectData):\n",
    "    plt.figure()\n",
    "    plt.plot(subject_data.eccs, subject_data.os_X, label='X')\n",
    "\n",
    "    # plt.plot(subject_data.eccs, subject_data.density_fit_X, label='X fit')\n",
    "\n",
    "    plt.xlabel('Eccentricity (°)')\n",
    "    plt.ylabel('Density (cells/deg²)')\n",
    "    plt.title(subject_data.name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#searches fo a \n",
    "plot_density(subjects_data[-2])\n",
    "\n",
    "#plots thickness for a subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def animate_and_save_images(subjects_data):\n",
    "    # Sort subjects by the peak density (largest value in density_X) in descending order,\n",
    "\n",
    "    # using np.nanmax so that NaNs are ignored.\n",
    "    subjects_sorted = sorted(subjects_data, key=lambda s: np.nanmax(s.os_X), reverse=True)\n",
    "    \n",
    "    # Compute global limits while filtering out NaNs.\n",
    "    all_eccs = np.concatenate([s.eccs for s in subjects_sorted])\n",
    "    all_eccs = all_eccs[~np.isnan(all_eccs)]\n",
    "    \n",
    "    all_density = np.concatenate([s.density_fit_X for s in subjects_sorted])\n",
    "    all_density = all_density[~np.isnan(all_density)]\n",
    "    \n",
    "    all_os = np.concatenate([s.os_X for s in subjects_sorted])\n",
    "    all_os = all_os[~np.isnan(all_os)]\n",
    "    \n",
    "    # X limits (eccentricity) based on all valid points.\n",
    "    x_min, x_max = np.min(all_eccs), np.max(all_eccs)\n",
    "    x_margin = (x_max - x_min) * 0.1\n",
    "    \n",
    "    # Y limits for density.\n",
    "    y_density_min, y_density_max = np.min(all_density), np.max(all_density)\n",
    "    y_density_margin = (y_density_max - y_density_min) * 0.1\n",
    "    \n",
    "    # Y limits for OS.\n",
    "    y_os_min, y_os_max = np.min(all_os), np.max(all_os)\n",
    "    y_os_margin = (y_os_max - y_os_min) * 0.1\n",
    "    \n",
    "    # Create output folder for images.\n",
    "    output_folder = \"subject_images\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    #counting the subject opsition for saving \n",
    "    subject_rank = 1\n",
    "    # Create a figure with two subplots (side by side).\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    def update(frame):\n",
    "        subject = subjects_sorted[frame]\n",
    "        # Clear both axes.\n",
    "        ax1.cla()\n",
    "        ax2.cla()\n",
    "        \n",
    "        # --- Density plot ---\n",
    "        valid_idx_density = ~np.isnan(subject.eccs) & ~np.isnan(subject.density_fit_X)\n",
    "        eccs_density = subject.eccs[valid_idx_density]\n",
    "        density = subject.density_fit_X[valid_idx_density]\n",
    "        \n",
    "        ax1.set_xlim(x_min - x_margin, x_max + x_margin)\n",
    "        ax1.set_ylim(y_density_min - y_density_margin, y_density_max + y_density_margin)\n",
    "        ax1.plot(eccs_density, density, label='Density', marker='o')\n",
    "        ax1.set_xlabel(\"Eccentricity (°)\")\n",
    "        ax1.set_ylabel(\"Density (cells/deg²)\")\n",
    "        ax1.set_title(f\"{subject.name} - Density\")\n",
    "        ax1.legend()\n",
    "        \n",
    "        # --- OS (thickness) plot ---\n",
    "        valid_idx_os = ~np.isnan(subject.eccs) & ~np.isnan(subject.os_X)\n",
    "        eccs_os = subject.eccs[valid_idx_os]\n",
    "        os_vals = subject.os_X[valid_idx_os]\n",
    "        \n",
    "        ax2.set_xlim(x_min - x_margin, x_max + x_margin)\n",
    "        ax2.set_ylim(y_os_min - y_os_margin, y_os_max + y_os_margin)\n",
    "        ax2.plot(eccs_os, os_vals, label='OS Thickness', color='orange', marker='o')\n",
    "        ax2.set_xlabel(\"Eccentricity (°)\")\n",
    "        ax2.set_ylabel(\"OS Thickness (µm)\")\n",
    "        ax2.set_title(f\"{subject.name} - OS\")\n",
    "        ax2.legend()\n",
    "        \n",
    "        # Set a suptitle for the whole figure.\n",
    "        fig.suptitle(f\"Subject {frame+1}/{len(subjects_sorted)}: {subject.name}\", fontsize=16)\n",
    "        \n",
    "        # Save the current figure to an image file using the subject's name.\n",
    "        image_filename = os.path.join(output_folder, f\"{frame}-{subject.name}.png\")\n",
    "        fig.savefig(image_filename)\n",
    "    \n",
    "    # Create the animation; adjust fps and interval as needed.\n",
    "    ani = animation.FuncAnimation(fig, update, frames=len(subjects_sorted), interval=2000, repeat=True)\n",
    "    \n",
    "    # Save the animation as a GIF file (requires Pillow installed).\n",
    "    ani.save(\"subjects_animation_os.gif\", writer=\"pillow\", fps=2)\n",
    "    plt.show()\n",
    "\n",
    "# Usage:\n",
    "# animate_and_save_images(subjects_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import svgpathtools\n",
    "\n",
    "\n",
    "def debug_visualize_segmentation(oct_file, seg_file, dims, bscan_id):\n",
    "    \"\"\"\n",
    "    Display the original B-scan and overlay the segmentation polygon from the SVG file.\n",
    "    \"\"\"\n",
    "    # Load the OCT image\n",
    "    oct_img = np.array(PIL.Image.open(oct_file))\n",
    "    \n",
    "    # Get the paths from the SVG (returns a list of path objects)\n",
    "    svg_paths = svgpathtools.svg2paths(str(seg_file))\n",
    "    svg_layers = svg_paths[0]  # if there is only one group of paths\n",
    "    \n",
    "    # Each path is made of line segments. We'll collect the points in a list to plot.\n",
    "    x_coords, z_coords = [], []\n",
    "    for line in svg_layers:\n",
    "        start = line.start\n",
    "        # Convert complex coordinate to real, imag\n",
    "        # dims['x'] is horizontal, dims['z'] is vertical in your code\n",
    "        # So, line.start.real -> x, line.start.imag -> z\n",
    "        x_coords.append(start.real)\n",
    "        z_coords.append(start.imag)\n",
    "    \n",
    "    # Display\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.imshow(oct_img, cmap='gray', origin='upper')\n",
    "    ax.scatter(x_coords, z_coords, s=5, c='red', marker='o')\n",
    "    ax.set_title(f\"B-scan {bscan_id} with raw SVG points overlay\")\n",
    "    ax.invert_yaxis()  # if needed, depending on coordinate system\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subjects_data[0].density_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def goodness_of_fit(subject_data: SubjectData, min_ecc, max_ecc):\n",
    "    \"\"\"\n",
    "    Computes R² for model fit within a specified eccentricity range.\n",
    "    \n",
    "    Parameters:\n",
    "        subject_data: An object containing `eccs`, `density_X`, `density_fit_X`, `density_Y`, and `density_fit_Y`.\n",
    "        min_ecc: Minimum eccentricity for filtering.\n",
    "        max_ecc: Maximum eccentricity for filtering.\n",
    "    \n",
    "    Returns:\n",
    "        r2_X: R² for X densities within the eccentricity range.\n",
    "        r2_Y: R² for Y densities within the eccentricity range.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply mask and remove NaNs\n",
    "    valid_mask_X =  ~np.isnan(subject_data.density_X) & ~np.isnan(subject_data.density_fit_X)\n",
    "    valid_mask_Y =  ~np.isnan(subject_data.density_Y) & ~np.isnan(subject_data.density_fit_Y)\n",
    "\n",
    "    density_X_valid = subject_data.density_X[valid_mask_X]\n",
    "    density_fit_X_valid = subject_data.density_fit_X[valid_mask_X]\n",
    "\n",
    "    density_Y_valid = subject_data.density_Y[valid_mask_Y]\n",
    "    density_fit_Y_valid = subject_data.density_fit_Y[valid_mask_Y]\n",
    "\n",
    "    # Compute R² only if we have at least two valid values\n",
    "    r2_X = r2_score(density_X_valid, density_fit_X_valid) if len(density_X_valid) > 1 else np.nan\n",
    "    r2_Y = r2_score(density_Y_valid, density_fit_Y_valid) if len(density_Y_valid) > 1 else np.nan\n",
    "\n",
    "    return r2_X, r2_Y\n",
    "\n",
    "# Example usage for all subjects with an eccentricity range of 0 to 10\n",
    "min_ecc, max_ecc = 0, 10\n",
    "r2s = [goodness_of_fit(sd, min_ecc, max_ecc) for sd in subjects_data]\n",
    "\n",
    "print(r2s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the mean of the r2s in probably the least efficient way possible\n",
    "#i swear i don't usually code like this\n",
    "\n",
    "mean_r2_X, mean_r2_Y = (np.mean([r2[0] for r2 in r2s]), np.mean([r2[1] for r2 in r2s]))\n",
    "\n",
    "print(mean_r2_X, mean_r2_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional fields based on the previously gathered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_cones(ecc: np.ndarray, dens_X: pd.Series, dens_Y: pd.Series, radius: float, smoothen: bool = True) -> float:\n",
    "    \n",
    "    '''\n",
    "    Given the cone density profiles along the X and Y axes, compute the total number of cones within a disk of radius `radius` (in degree) centered at the fovea by linearly interpolating (radially) the density profiles and integrating over the disk.\n",
    "    '''\n",
    "    smthd_x = gaussian_filter_nan(dens_X, sigma=4) if smoothen else dens_X.to_numpy()\n",
    "    smthd_y = gaussian_filter_nan(dens_Y, sigma=4) if smoothen else dens_Y.to_numpy()\n",
    "   \n",
    "    x_amax = np.nanargmax(smthd_x)\n",
    "    p = np.polyfit(ecc[x_amax-2:x_amax+3], smthd_x[x_amax-2:x_amax+3], 2)\n",
    "    x_amax = -p[1] / (2 * p[0])\n",
    "\n",
    "    y_amax = np.nanargmax(smthd_y)\n",
    "    p = np.polyfit(ecc[y_amax-2:y_amax+3], smthd_y[y_amax-2:y_amax+3], 2)\n",
    "    y_amax = -p[1] / (2 * p[0])\n",
    "\n",
    "    R = np.linspace(0.0001, radius, 500) # radius in degrees\n",
    "    disk = np.r_[\n",
    "        np.interp(x_amax + R, ecc, smthd_x),\n",
    "        np.interp(x_amax - R, ecc, smthd_x),\n",
    "        np.interp(y_amax + R, ecc, smthd_y),\n",
    "        np.interp(y_amax - R, ecc, smthd_y)\n",
    "    ]\n",
    "    \n",
    "    norm_coef = MM_PER_DEGREE**2 * 2 * np.pi\n",
    "    # integrate cone density over disk to get total nb of cones\n",
    "    return norm_coef * np.trapz(np.nanmean(disk, axis=0) * R, R)\n",
    "\n",
    "RADIUS = 3 # degree\n",
    "for sd in subjects_data:\n",
    "    sd.nb_cones = get_nb_cones(sd.eccs, sd.density_X, sd.density_Y, radius = RADIUS)\n",
    "    sd.nb_cones_fit = get_nb_cones(sd.eccs, sd.density_fit_X, sd.density_fit_Y, radius = RADIUS, smoothen=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "def adjust_flat(gcl_data: np.ndarray, peak_left: int, peak_right: int) -> np.ndarray:\n",
    "    slope = (gcl_data[peak_right] - gcl_data[peak_left]) / (peak_right - peak_left)\n",
    "    transformed_gcl = gcl_data - slope * (np.arange(len(gcl_data)) - peak_left)\n",
    "    return transformed_gcl\n",
    "\n",
    "def get_gcl_width(gcl: pd.Series) -> Tuple[float, float]:\n",
    "    '''\n",
    "    Given the GCL+IPL thickness profile, compute the width of the pit as well as the minimum thickness of the layer. \n",
    "    Here, the width of the pit is defined as the distance between the two points where the thickness is 20% of the depth of the pit. The depth of the pit is defined as the difference between the thickness surrounding the pit and the thickness at the pit's bottom.\n",
    "    '''\n",
    "    # name = gcl.name\n",
    "    gcl_to_plot = gcl.copy()\n",
    "    eccs = gcl[np.abs(gcl.index) <= 6].index.to_numpy()\n",
    "    gcl = gcl.interpolate(method='polynomial', order=1)[eccs].to_numpy()\n",
    "    # plt.plot(eccs, gcl, label=name)\n",
    "    smooth_param = 3\n",
    "    peak_left = peak_right = []\n",
    "    while not (len(peak_left) >= 1 and len(peak_right) >= 1) and smooth_param < 10:\n",
    "        smoothed_gcl = gaussian_filter_nan(gcl, smooth_param)\n",
    "        peaks = find_peaks(smoothed_gcl)[0]\n",
    "        peak_left  = [peak for peak in peaks if peak < len(smoothed_gcl) / 3]\n",
    "        peak_right = [peak for peak in peaks if peak > 2 * len(smoothed_gcl) / 3]\n",
    "        smooth_param += 1\n",
    "    assert len(peak_left) >= 1 and len(peak_right) >= 1, f'No peaks found for {gcl.name}'\n",
    "    peak_left = round(np.mean(peak_left))   \n",
    "    peak_right = round(np.mean(peak_right))\n",
    "    adjusted_gcl = adjust_flat(gcl, peak_left, peak_right)\n",
    "    smoothed_aj_gcl = gaussian_filter_nan(adjusted_gcl, 2)\n",
    "\n",
    "    y_min = np.nanmin(smoothed_aj_gcl[peak_left:peak_right])\n",
    "    y_target = y_min + (smoothed_aj_gcl[peak_left] - y_min) / 5\n",
    "    intercepts = np.where(np.diff(np.sign(smoothed_aj_gcl - y_target)))[0]\n",
    "    leftmost = eccs[intercepts[0]]\n",
    "    rightmost = eccs[intercepts[-1]+1]\n",
    "    width_pit_gcl = rightmost - leftmost\n",
    "\n",
    "    indicies = np.argpartition(gcl, 10)[:10]\n",
    "    p = np.polyfit(eccs[indicies], gcl[indicies], 2)\n",
    "    if p[0] == 0:\n",
    "    #     # gcl_to_plot.plot()\n",
    "    #     plt.plot(eccs, gcl, label='gcl')\n",
    "        plt.plot(np.sort(eccs[indicies]), np.polyval(p, np.sort(eccs[indicies])), '--')\n",
    "    min_thickness_gcl = np.polyval(p, -p[1] / (2 * p[0]))\n",
    "    return width_pit_gcl, min_thickness_gcl\n",
    "\n",
    "for sd in subjects_data:\n",
    "    width_gcl_x, min_thick_x = get_gcl_width(sd.gcl_ipl_X)\n",
    "    width_gcl_y, min_thick_y = get_gcl_width(sd.gcl_ipl_Y)\n",
    "    sd.width_gcl_X = width_gcl_x\n",
    "    sd.width_gcl_Y = width_gcl_y\n",
    "    sd.min_thick_gcl = min(min_thick_x, min_thick_y)\n",
    "    # print(f'{sd.name:>10}: {width_gcl_x:.2f}°, {depth_gcl_x:.4f}, {width_gcl_y:.2f}°, {depth_gcl_y:.4f}')\n",
    "    # plt.xlim(-6, 6)\n",
    "    # plt.legend()\n",
    "    # plt.title(sd.name)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting the first 5 subjects from the list of subjects_data\n",
    "\n",
    "if take_first_five:\n",
    "    first_five_subjects_data = [sd for sd in subjects_data if sd.name in first_five_subjects]\n",
    "\n",
    "    #storing the old subjects_data in a new variable to avoid overwriting the old one\n",
    "    old_subjects_data = subjects_data\n",
    "\n",
    "    #reassigning the subjects_data variable to the first_five_subjects_data\n",
    "    subjects_data = first_five_subjects_data\n",
    "\n",
    "    # keep in mind that now we have only five subjects in the subjects_data list, which is gonna \n",
    "    # mess with the statistics calculations later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "\n",
    "df = pd.DataFrame({k: [getattr(sd, k) for sd in subjects_data] for k,t in SubjectData.__annotations__.items() if t in (int, float)}, index=[sd.name for sd in subjects_data])\n",
    "\n",
    "def corr_sig(df: pd.DataFrame, drop=['nb']) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    cols = df.columns.drop(drop).to_list()\n",
    "    corr_matrix = np.zeros(shape=(len(cols), len(cols)))\n",
    "    p_matrix = np.ones_like(corr_matrix)\n",
    "    for col in cols:\n",
    "        for col2 in cols:\n",
    "            corr , p = scipy.stats.pearsonr(df[col],df[col2])\n",
    "            corr_matrix[cols.index(col),cols.index(col2)] = corr\n",
    "            p_matrix[cols.index(col),cols.index(col2)] = p\n",
    "    return corr_matrix, p_matrix\n",
    "\n",
    "corr, pv = corr_sig(df)\n",
    "sig_mask = pv < 0.05\n",
    "plt.figure(figsize=(14, 10), dpi=400)\n",
    "# sns.heatmap(corr, mask=~sig_mask, annot=True, cmap='coolwarm', annot_kws={\"fontsize\":8}, xticklabels=df.columns, yticklabels=df.columns)\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', annot_kws={\"fontsize\":8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eccs = subjects_data[0].eccs\n",
    "layer_names = ['rnfl', 'gcl_ipl', 'inl_opl', 'onl', 'pr_rpe', 'os', 'chrd']\n",
    "names_r = {'rnfl': 'RNFL', 'gcl_ipl': 'GCL+IPL', 'inl_opl': 'INL+OPL', 'onl': 'ONL', 'pr_rpe': 'PhotoR+RPE', 'os': 'OS', 'chrd': 'Choroid', 'cones': 'Cone Density'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.shared.helpers.direction import Direction\n",
    "\n",
    "\n",
    "def preprocess_functional_feature(data: np.ndarray, standardization: str = 'inter') -> np.ndarray:\n",
    "    '''\n",
    "    Preprocess a functional feature (functional feature such as cone density or layer thickness, for which the feature is a function of eccentricity) by (Z-)standardizing it.\n",
    "    Given `data` matrix should have shape (n_subjects, n_eccentricities).\n",
    "    \n",
    "    - For an intra-indivual analysis, use `standardization='intra'` to standardize within subjects (i.e. within each row). This removes inter-subject variability.\n",
    "    - For an inter-individual analysis, use `standardization='inter'` to standardize across subjects, eccentricity-wise (i.e. within each column). Removes eccentricity-level variability, focuses on between-patient trends\n",
    "    '''\n",
    "    if standardization == 'inter':\n",
    "        mean = np.mean(data, axis=0, keepdims=True)\n",
    "        std = np.std(data, axis=0, keepdims=True)\n",
    "        return (data - mean) / std\n",
    "    if standardization == 'intra':\n",
    "        mean = np.nanmean(data, axis=1, keepdims=True)\n",
    "        std = np.nanstd(data, axis=1, keepdims=True)\n",
    "        return (data - mean) / std\n",
    "    return data\n",
    "\n",
    "def preprocess_functional_data(direction: Direction, standardization: str = 'inter', toLog : bool = True) -> Dict[str, np.ndarray]:\n",
    "    '''\n",
    "    Preprocess functional data (e.g. cone density, layer thicknesses) for a given direction (X or Y) by (Z-)standardizing it.\n",
    "    '''\n",
    "\n",
    "    layer_fds = {\n",
    "        layer: preprocess_functional_feature(\n",
    "            np.array([getattr(s, f'{layer}_{direction.value}') for s in subjects_data]), standardization\n",
    "        )\n",
    "        for layer in layer_names\n",
    "    }\n",
    "    if toLog:\n",
    "        cone_density_fd = preprocess_functional_feature(\n",
    "            np.array([np.log(getattr(s, f'density_fit_{direction.value}')) for s in subjects_data]), standardization\n",
    "        )\n",
    "    else:\n",
    "        cone_density_fd = preprocess_functional_feature(\n",
    "            np.array([(getattr(s, f'density_fit_{direction.value}')) for s in subjects_data]), standardization\n",
    "        )\n",
    "\n",
    "    cone_density_nonfit = preprocess_functional_feature(\n",
    "            np.array([(getattr(s, f'density_{direction.value}')) for s in subjects_data]), standardization\n",
    "        )\n",
    "    # return {'cones': cone_density_fd, \"nonfit\": cone_density_nonfit, **layer_fds}\n",
    "    return {'cones': cone_density_fd, \"nonfit\": cone_density_nonfit, **layer_fds}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cone density specific analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def kendall_pval(x,y):\n",
    "    return kendalltau(x,y)[1]\n",
    "\n",
    "def pearsonr_pval(x,y):\n",
    "    return pearsonr(x,y)[1]\n",
    "\n",
    "def spearmanr_pval(x,y):\n",
    "    return spearmanr(x,y, nan_policy = \"omit\")[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.630 /MM_PER_DEGREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.070 / MM_PER_DEGREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subjects_data:\n",
    "    print(s.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for direction in Direction:\n",
    "    cone_density_fd = preprocess_functional_data(direction, standardization='none', toLog=False)['nonfit']\n",
    "\n",
    "    def get_cd_on_range(left, right):\n",
    "        range_eccs = np.argwhere((left <= eccs) & (eccs <= right)).flatten()\n",
    "        return np.mean(cone_density_fd[:, range_eccs], axis=1)\n",
    "    \n",
    "    def plot_subjects_data(df):\n",
    "        \"\"\"\n",
    "        Plots the cone density values for each subject from the given DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "        - df: pandas DataFrame where each row corresponds to a subject, \n",
    "            and each column represents a specific eccentricity range.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Get the x-axis labels (eccentricity ranges)\n",
    "        x_labels = df.columns\n",
    "        x_values = np.arange(len(x_labels))  # Numerical representation for plotting\n",
    "\n",
    "        # Plot each subject's data in a different color\n",
    "        for subject_id in df.index:\n",
    "            plt.plot(x_values, df.loc[subject_id], marker='o', linestyle='-', label=f\"Subject {subject_id}\")\n",
    "\n",
    "        # Formatting the plot\n",
    "        plt.xticks(ticks=x_values, labels=x_labels, rotation=90)  # Rotate x-axis labels for readability\n",
    "        plt.xlabel(\"Eccentricity Range (°)\")\n",
    "        plt.ylabel(\"Cone Density\")\n",
    "        plt.title(\"Cone Density Across Eccentricity Ranges for Each Subject\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "    pids = np.array([s.nb for s in subjects_data])\n",
    "    # Define the start and end of the range\n",
    "\n",
    "    start_val = -10\n",
    "    end_val = 10   # This will create bins up to 10.0° (last bin: 9.9° to 10.0°)\n",
    "    step = 2\n",
    "\n",
    "    # Generate bin edges from start_val to end_val (excluding the final edge)\n",
    "    bin_starts = np.arange(start_val, end_val, step)\n",
    "\n",
    "    # Create a dictionary with column names and corresponding data from get_cd_on_range\n",
    "    data = {\n",
    "        f\"{bin_start:.1f}° to {bin_start + step:.1f}°\": get_cd_on_range(bin_start, bin_start + step)\n",
    "        for bin_start in bin_starts\n",
    "    }\n",
    "\n",
    "    # Create the DataFrame with your patient IDs (pids) as the index\n",
    "    _df = pd.DataFrame(data, index=pids)\n",
    "\n",
    "    # Optionally, sort the DataFrame by one of the bins (e.g., the bin covering 0.0° to 0.1°)\n",
    "    print (_df.keys())\n",
    "    _df = _df.sort_values(by=\"-2.0° to 0.0°\")\n",
    "\n",
    "\n",
    "    sns.set_theme(font_scale=0.8)\n",
    "    sns.heatmap(_df.corr(method = \"pearson\"), annot=True, fmt=\".2f\", cmap='coolwarm', square=True, center=0, annot_kws={\"size\": 5}, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title(f'Correlation matrix of Cone Density (Not fitted) at different eccentricities, {direction.value}-axis', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Call the function to plot the data\n",
    "    plot_subjects_data(_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for direction in Direction:\n",
    "    cone_density_fd = preprocess_functional_data(direction, standardization='none', toLog=False)['cones']\n",
    "\n",
    "    def get_cd_on_range(left, right):\n",
    "        range_eccs = np.argwhere((left <= eccs) & (eccs <= right)).flatten()\n",
    "        return np.mean(cone_density_fd[:, range_eccs], axis=1)\n",
    "    \n",
    "    def plot_subjects_data(df):\n",
    "        \"\"\"\n",
    "        Plots the cone density values for each subject from the given DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "        - df: pandas DataFrame where each row corresponds to a subject, \n",
    "            and each column represents a specific eccentricity range.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Get the x-axis labels (eccentricity ranges)\n",
    "        x_labels = df.columns\n",
    "        x_values = np.arange(len(x_labels))  # Numerical representation for plotting\n",
    "\n",
    "        # Plot each subject's data in a different color\n",
    "        for subject_id in df.index:\n",
    "            plt.plot(x_values, df.loc[subject_id], marker='o', linestyle='-', label=f\"Subject {subject_id}\")\n",
    "\n",
    "        # Formatting the plot\n",
    "        plt.xticks(ticks=x_values, labels=x_labels, rotation=90)  # Rotate x-axis labels for readability\n",
    "        plt.xlabel(\"Eccentricity Range (°)\")\n",
    "        plt.ylabel(\"Cone Density\")\n",
    "        plt.title(\"Cone Density Across Eccentricity Ranges for Each Subject\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "    pids = np.array([s.nb for s in subjects_data])\n",
    "    _df = pd.DataFrame({\n",
    "        '-10° to -6°': get_cd_on_range(-10, -6),\n",
    "        '-6° to -3°': get_cd_on_range(-6, -3),\n",
    "        '-3° to -0.5°': get_cd_on_range(-3, -0.5),\n",
    "        '-0.5° to 0.5°': get_cd_on_range(-0.5, 0.5),\n",
    "        '0.5° to 3°': get_cd_on_range(1, 3),\n",
    "        '3° to 6°': get_cd_on_range(3, 6),\n",
    "        '6° to 10°': get_cd_on_range(6, 10),\n",
    "    }, index=pids).sort_values(by='-0.5° to 0.5°')\n",
    "\n",
    "    sns.set_theme(font_scale=0.8)\n",
    "    sns.heatmap(_df.corr(method = \"spearman\"), annot=True, fmt=\".2f\", cmap='coolwarm', square=True, center=0)\n",
    "    plt.title(f'Correlation matrix of Cone Density at different eccentricities, {direction.value}-axis', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "    # sns.set_theme(font_scale=0.8)\n",
    "    # sns.heatmap(_df.corr(method = spearmanr_pval), annot=True, fmt=\".6f\", cmap='coolwarm', square=True, center=0)\n",
    "    # plt.title(f'Correlation matrix of Cone Density at different eccentricities, {direction.value}-axis', fontsize=14)\n",
    "    # plt.show()\n",
    "\n",
    "    # Call the function to plot the data\n",
    "    plot_subjects_data(_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cone_density_fd_X = preprocess_functional_data(Direction.X, standardization='none', toLog = False)['cones']\n",
    "cone_density_fd_Y = preprocess_functional_data(Direction.Y, standardization='none', toLog = False)['cones']\n",
    "\n",
    "def get_cd_on_range(left, right, direction: Direction):\n",
    "    range_eccs = np.argwhere((left <= eccs) & (eccs <= right)).flatten()\n",
    "    return np.mean((cone_density_fd_X if direction == Direction.X else cone_density_fd_Y)[:, range_eccs], axis=1)\n",
    "\n",
    "pids = np.array([s.nb for s in subjects_data])\n",
    "_df = pd.DataFrame({\n",
    "    'X-axis,\\n-10° to -6°': get_cd_on_range(-10, -6, Direction.X),\n",
    "    'X-axis,\\n-6° to -3°': get_cd_on_range(-6, -3, Direction.X),\n",
    "    'X-axis,\\n-3° to -0.5°': get_cd_on_range(-3, -0.5, Direction.X),\n",
    "    'X-axis,\\n-0.5° to 0.5°': get_cd_on_range(-0.5, 0.5, Direction.X),\n",
    "    'X-axis,\\n0.5° to 3°': get_cd_on_range(1, 3, Direction.X),\n",
    "    'X-axis,\\n3° to 6°': get_cd_on_range(3, 6, Direction.X),\n",
    "    'X-axis,\\n6° to 10°': get_cd_on_range(6, 10, Direction.X),\n",
    "    'Y-axis,\\n-10° to -6°': get_cd_on_range(-10, -6, Direction.Y),\n",
    "    'Y-axis,\\n-6° to -3°': get_cd_on_range(-6, -3, Direction.Y),\n",
    "    'Y-axis,\\n-3° to -0.5°': get_cd_on_range(-3, -0.5, Direction.Y),\n",
    "    'Y-axis,\\n-0.5° to 0.5°': get_cd_on_range(-0.5, 0.5, Direction.Y),\n",
    "    'Y-axis,\\n0.5° to 3°': get_cd_on_range(1, 3, Direction.Y),\n",
    "    'Y-axis,\\n3° to 6°': get_cd_on_range(3, 6, Direction.Y),\n",
    "    'Y-axis,\\n6° to 10°': get_cd_on_range(6, 10, Direction.Y),\n",
    "    'X-axis, \\ncone density ratio 4um/fovea': (get_cd_on_range((0.4/(MM_PER_DEGREE)),(0.5/(MM_PER_DEGREE)), Direction.X)/get_cd_on_range((0), (0.100), Direction.X)),\n",
    "    'X-axis, \\ncone density ratio 2070um/630um microm': (get_cd_on_range((2.070/(MM_PER_DEGREE)),(2.100/(MM_PER_DEGREE)), Direction.X)/get_cd_on_range((0.630/(MM_PER_DEGREE)), (0.650/(MM_PER_DEGREE)), Direction.X)),\n",
    "    'X-axis, \\n630um': get_cd_on_range((0.630/(MM_PER_DEGREE)), (0.650/(MM_PER_DEGREE)), Direction.X),\n",
    "\n",
    "\n",
    "}, index=pids)#.sort_values(by='-0.5° to 0.5°')\n",
    "\n",
    "sns.set_theme(font_scale=0.6)\n",
    "sns.heatmap(_df.corr(method='pearson'), annot=True, fmt=\".2f\", cmap='coolwarm', square=True, center=0)\n",
    "plt.title(f'Correlation matrix of Cone Density at different eccentricities.', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "for direction in Direction:\n",
    "    cone_density_fd = preprocess_functional_data(direction, standardization='none', toLog=False)['cones']\n",
    "\n",
    "    def get_cd_on_range(left, right):\n",
    "        range_eccs = np.argwhere((left <= eccs) & (eccs <= right)).flatten()\n",
    "        return np.mean(cone_density_fd[:, range_eccs], axis=1)\n",
    "\n",
    "    pids = np.array([s.nb for s in subjects_data])\n",
    "    _df = pd.DataFrame({\n",
    "        '-10° to -6°': get_cd_on_range(-10, -6),\n",
    "        '-6° to -3°': get_cd_on_range(-6, -3),\n",
    "        '-3° to -0.5°': get_cd_on_range(-3, -0.5),\n",
    "        '-0.5° to 0.5°': get_cd_on_range(-0.5, 0.5),\n",
    "        '0.5° to 3°': get_cd_on_range(1, 3),\n",
    "        '3° to 6°': get_cd_on_range(3, 6),\n",
    "        '6° to 10°': get_cd_on_range(6, 10),\n",
    "    }, index=pids).sort_values(by='-0.5° to 0.5°')\n",
    "\n",
    "    sns.set_theme(font_scale=0.8)\n",
    "    sns.heatmap(_df.corr(), annot=True, fmt=\".2f\", cmap='coolwarm', square=True, center=0)\n",
    "    plt.title(f'Correlation matrix of Cone Density at different eccentricities, {direction.value}-axis', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "cone_density_fd_X = preprocess_functional_data(Direction.X, standardization='none', toLog = False)['cones']\n",
    "cone_density_fd_Y = preprocess_functional_data(Direction.Y, standardization='none', toLog = False)['cones']\n",
    "\n",
    "def get_cd_on_range(left, right, direction: Direction):\n",
    "    range_eccs = np.argwhere((left <= eccs) & (eccs <= right)).flatten()\n",
    "    return np.mean((cone_density_fd_X if direction == Direction.X else cone_density_fd_Y)[:, range_eccs], axis=1)\n",
    "\n",
    "pids = np.array([s.nb for s in subjects_data])\n",
    "_df = pd.DataFrame({\n",
    "    'X-axis,\\n-10° to -6°': get_cd_on_range(-10, -6, Direction.X),\n",
    "    'X-axis,\\n-6° to -3°': get_cd_on_range(-6, -3, Direction.X),\n",
    "    'X-axis,\\n-3° to -0.5°': get_cd_on_range(-3, -0.5, Direction.X),\n",
    "    'X-axis,\\n-0.5° to 0.5°': get_cd_on_range(-0.5, 0.5, Direction.X),\n",
    "    'X-axis,\\n0.5° to 3°': get_cd_on_range(1, 3, Direction.X),\n",
    "    'X-axis,\\n3° to 6°': get_cd_on_range(3, 6, Direction.X),\n",
    "    'X-axis,\\n6° to 10°': get_cd_on_range(6, 10, Direction.X),\n",
    "    'Y-axis,\\n-10° to -6°': get_cd_on_range(-10, -6, Direction.Y),\n",
    "    'Y-axis,\\n-6° to -3°': get_cd_on_range(-6, -3, Direction.Y),\n",
    "    'Y-axis,\\n-3° to -0.5°': get_cd_on_range(-3, -0.5, Direction.Y),\n",
    "    'Y-axis,\\n-0.5° to 0.5°': get_cd_on_range(-0.5, 0.5, Direction.Y),\n",
    "    'Y-axis,\\n0.5° to 3°': get_cd_on_range(1, 3, Direction.Y),\n",
    "    'Y-axis,\\n3° to 6°': get_cd_on_range(3, 6, Direction.Y),\n",
    "    'Y-axis,\\n6° to 10°': get_cd_on_range(6, 10, Direction.Y),\n",
    "    'X-axis, \\ncone density ratio 4um/fovea': (get_cd_on_range((0.4/(MM_PER_DEGREE)),(0.5/(MM_PER_DEGREE)), Direction.X)/get_cd_on_range((0), (0.100), Direction.X)),\n",
    "    'X-axis, \\ncone density ratio 2070um/630um microm': (get_cd_on_range((2.070/(MM_PER_DEGREE)),(2.100/(MM_PER_DEGREE)), Direction.X)/get_cd_on_range((0.630/(MM_PER_DEGREE)), (0.650/(MM_PER_DEGREE)), Direction.X)),\n",
    "    'X-axis, \\n630um': get_cd_on_range((0.630/(MM_PER_DEGREE)), (0.650/(MM_PER_DEGREE)), Direction.X),\n",
    "\n",
    "\n",
    "}, index=pids)#.sort_values(by='-0.5° to 0.5°')\n",
    "\n",
    "sns.set_theme(font_scale=0.6)\n",
    "sns.heatmap(_df.corr(method='pearson'), annot=True, fmt=\".2f\", cmap='coolwarm', square=True, center=0)\n",
    "plt.title(f'Correlation matrix of Cone Density at different eccentricities.', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the last two entries\n",
    "y_data = _df['X-axis,\\n-0.5° to 0.5°']\n",
    "x_data = _df['X-axis,\\n0.5° to 3°']\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_data, y_data, color='blue', label='Data Points')\n",
    "\n",
    "# Add labels and title\n",
    "plt.ylabel('Cone Density Ratio (2070/630 um)')\n",
    "plt.xlabel('Cone Density at 2.0° to 2.5°')\n",
    "plt.title('Scatter Plot: Cone Density Ratio vs Cone Density at 630 um')\n",
    "\n",
    "# Show grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density fit analysis  - Ratios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cone_density_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "eccs_in_MM = eccs*MM_PER_DEGREE\n",
    "# print(eccs_in_MM)\n",
    "\n",
    "# Normalize by the max value for each of the 201 parameters (column-wise normalization)\n",
    "normalized_data_x = cone_density_fd_X/ np.max(cone_density_fd_X, axis=1, keepdims=True)\n",
    "normalized_data_y = cone_density_fd_Y/ np.max(cone_density_fd_Y, axis=1, keepdims=True)\n",
    "\n",
    "# Compute the mean across the 33 patients\n",
    "mean_data_x= np.mean(normalized_data_x, axis=0)\n",
    "mean_data_y= np.mean(normalized_data_y, axis=0)\n",
    "\n",
    "mean_data_x_nonnorm = np.mean(cone_density_fd_X, axis=0)\n",
    "mean_data_y_nonnorm = np.mean(cone_density_fd_Y, axis=0)\n",
    "\n",
    "# Plot all 33 lines with lower opacity\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(33):\n",
    "    plt.plot(eccs_in_MM, cone_density_fd_X[i, :],  color='blue', alpha=0.2)  # Low opacity\n",
    "    plt.plot(eccs_in_MM, cone_density_fd_Y[i, :],  color='blue', alpha=0.2)  # Low opacity\n",
    "\n",
    "\n",
    "plt.xlabel(\"Eccentricity (mm)\")\n",
    "plt.ylabel(\"Normalized Value\")\n",
    "plt.title(\"Normalized Data Across Patients with Mean Line\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot all 33 lines with lower opacity\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(33):\n",
    "    plt.plot(eccs_in_MM, normalized_data_x[i, :],  color='blue', alpha=0.2)  # Low opacity\n",
    "    plt.plot(eccs_in_MM, normalized_data_y[i, :],  color='blue', alpha=0.2)  # Low opacity\n",
    "# Plot the mean line with higher opacity\n",
    "plt.plot( eccs_in_MM, mean_data_x, color='red',  linewidth=2, label=\"Mean\", alpha=0.9)\n",
    "plt.plot( eccs_in_MM, mean_data_y, color='green',  linewidth=2, label=\"Mean\", alpha=0.9)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Eccentricity (mm)\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Data Across Patients \")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derivatives\n",
    "derivative_x = np.gradient(cone_density_fd_X, eccs_in_MM, axis=1)\n",
    "derivative_y = np.gradient(cone_density_fd_Y, eccs_in_MM, axis=1)\n",
    "\n",
    "# Compute g'(x) using g'(x) = f'(g(x)) / f(g(x))\n",
    "gx_derivative_x = derivative_x / cone_density_fd_X\n",
    "gx_derivative_y = derivative_y / cone_density_fd_Y\n",
    "\n",
    "# Avoid division by zero and NaNs\n",
    "gx_derivative_x[np.isinf(gx_derivative_x) | np.isnan(gx_derivative_x)] = 0\n",
    "gx_derivative_y[np.isinf(gx_derivative_y) | np.isnan(gx_derivative_y)] = 0\n",
    "\n",
    "print(gx_derivative_x.shape)\n",
    "# # Compute the mean derivative\n",
    "# mean_derivative_x = np.gradient(mean_data_x, eccs_in_MM)\n",
    "# mean_derivative_y = np.gradient(mean_data_y, eccs_in_MM)\n",
    "\n",
    "# Plot all 33 derivative curves with lower opacity\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(33):\n",
    "    plt.plot(eccs_in_MM, gx_derivative_x[i, :], color='blue', alpha=0.2)  # Low opacity\n",
    "    plt.plot(eccs_in_MM, gx_derivative_y[i, :], color='blue', alpha=0.2)  # Low opacity\n",
    "\n",
    "# Plot the mean derivative with higher opacity\n",
    "# plt.plot(eccs_in_MM, mean_derivative_x, color='red', linewidth=2, label=\"Mean Derivative X\", alpha=0.9)\n",
    "# plt.plot(eccs_in_MM, mean_derivative_y, color='green', linewidth=2, label=\"Mean Derivative Y\", alpha=0.9)\n",
    "\n",
    "plt.xlabel(\"Eccentricity (mm)\")\n",
    "plt.ylabel(\"Derivative of Normalized Value\")\n",
    "plt.title(\"Derivative of Normalized Data Across Patients with Mean Line\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Plot all 33 derivative curves with lower opacity\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(33):\n",
    "    plt.plot(eccs_in_MM, derivative_x[i, :], color='blue', alpha=0.2)  # Low opacity\n",
    "    plt.plot(eccs_in_MM, derivative_y[i, :], color='blue', alpha=0.2)  # Low opacity\n",
    "\n",
    "# Plot the mean derivative with higher opacity\n",
    "# plt.plot(eccs_in_MM, mean_derivative_x, color='red', linewidth=2, label=\"Mean Derivative X\", alpha=0.9)\n",
    "# plt.plot(eccs_in_MM, mean_derivative_y, color='green', linewidth=2, label=\"Mean Derivative Y\", alpha=0.9)\n",
    "\n",
    "plt.xlabel(\"Eccentricity (mm)\")\n",
    "plt.ylabel(\"Derivative of Normalized Value\")\n",
    "plt.title(\"Derivative of Normalized Data Across Patients with Mean Line\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cone_density_fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radii = [0.150, 0.300, 1.2]\n",
    "indices = []\n",
    "ratio = []\n",
    "\n",
    "# mean_data_x = cone_density_fd_X[0]\n",
    "# mean_data_y = cone_density_fd_Y[0]\n",
    "for i, radius in enumerate (radii):\n",
    "    index_pos = np.searchsorted(eccs_in_MM, radius)\n",
    "    index_neg = np.searchsorted(eccs_in_MM, -radius)\n",
    "    ratio_values =[mean_data_x[index_pos], mean_data_y[index_pos],\n",
    "                    mean_data_x[index_neg], mean_data_y[index_neg]]\n",
    "    print(ratio_values)\n",
    "    ratio.append( np.mean(ratio_values))\n",
    "    print (f\"Ratio at {radii[i]} mm, for eccentricity of radius {eccs_in_MM[index_pos]} mm is {ratio[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zhang 2015 - assessing wheter the total number of cones results are reproducible by our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrating the density function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eccs_in_MM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cone_density_fd_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import color\n",
    "from cv2 import mean\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "import numpy as np\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "def integrate_cone_density(eccs_in_MM : np.array, cone_density_fd_X : np.array,\n",
    "                            cone_density_fd_Y : np.array, radius: float = 0.1, exclude_center: bool = False):\n",
    "\n",
    "\n",
    "\n",
    "    if exclude_center:\n",
    "        #makes a mask to esclude the center of the fovea on both positive and negative direction\n",
    "        mask_pos = (eccs_in_MM > 0.3) & (eccs_in_MM < radius)\n",
    "        mask_neg = (eccs_in_MM < -0.3) & (eccs_in_MM > -radius)\n",
    "        mask = mask_pos | mask_neg\n",
    "        x_integr = eccs_in_MM[mask]\n",
    "\n",
    "    else :\n",
    "        mask = (eccs_in_MM > -radius) & (eccs_in_MM < radius)\n",
    "\n",
    "    x_integr = eccs_in_MM[mask]\n",
    "\n",
    "    int_results = []\n",
    "    #concatenate the two arrays\n",
    "    cone_density_fd_XY = np.concatenate((cone_density_fd_X, cone_density_fd_Y), axis=0)\n",
    "    print(cone_density_fd_XY.shape)\n",
    "\n",
    "    for i in range(cone_density_fd_XY.shape[0]):  # Loop over each dataset\n",
    "        y_integr = cone_density_fd_XY[i, mask]\n",
    "        int_val = integrate.simpson(y=y_integr, x=x_integr)\n",
    "        int_results.append(int_val)\n",
    "        # plt.plot(x_integr, y_integr, color='blue', alpha=0.2)  # Low opacity\n",
    "\n",
    "    # Convert to NumPy array for statistics\n",
    "    int_results = np.array(int_results)\n",
    "\n",
    "    # Compute statistics\n",
    "    mean_int = np.mean(int_results)\n",
    "    std_int = np.std(int_results)\n",
    "    min_int = np.min(int_results)\n",
    "    max_int = np.max(int_results)\n",
    "\n",
    "    # Compute COV (Coefficient of Variation)\n",
    "    cov_int = (std_int / mean_int) * 100  # Convert to percentage\n",
    "\n",
    "    #plot the y_integr\n",
    "\n",
    "    # plt.show() \n",
    "\n",
    "    return mean_int, std_int, min_int, max_int, cov_int, int_results\n",
    "\n",
    "\n",
    "\n",
    "mean_int, std_int, min_int, max_int, cov_int, int_results = integrate_cone_density(eccs_in_MM, cone_density_fd_X,\n",
    "                                                                                    cone_density_fd_Y, radius = 1, exclude_center=False)\n",
    "# Print results\n",
    "print(f\"Mean integrated cone density: {mean_int}\")\n",
    "print(f\"Standard deviation: {std_int}\")\n",
    "print(f\"Minimum: {min_int}\")\n",
    "print(f\"Maximum: {max_int}\")\n",
    "\n",
    "# Print the COV\n",
    "print(f\"Coefficient of Variation (COV): {cov_int:.2f}%\")\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(int_results, bins=100, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Integrated Cone Density\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Integrated Cone Density Values\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COV_results = []\n",
    "\n",
    "# Compute the COV for different radii\n",
    "radii = np.linspace(0.1, 1.5, 100)\n",
    "\n",
    "for radius in radii:\n",
    "    mean_int, std_int, min_int, max_int, cov_int, int_results = integrate_cone_density(eccs_in_MM, cone_density_fd_X,\n",
    "                                                                                    cone_density_fd_Y, radius = radius)\n",
    "    COV_results.append(cov_int)\n",
    "\n",
    "# Convert to NumPy array\n",
    "COV_results = np.array(COV_results)\n",
    "\n",
    "# Plot the COV as a function of radius\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(radii, COV_results, color='blue', linewidth=2)\n",
    "plt.xlabel(\"Radius (mm)\")\n",
    "plt.ylabel(\"COV\")\n",
    "plt.title(\"COV as a Function of Radius\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between peak and [?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for intra-/inter-individual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import warnings\n",
    "from scipy.stats import spearmanr, pearsonr, binomtest\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def mixedlm(cd: np.ndarray, lt: np.ndarray, pids: np.ndarray, eccs: np.ndarray, standardization: str = 'inter') -> Tuple[float, float]:\n",
    "    data = pd.DataFrame({'Subject': pids, 'Eccentricity': eccs, 'LayerThickness': lt, 'ConeDensity': cd})\n",
    "    if standardization == 'intra':\n",
    "        model = smf.mixedlm(\"LayerThickness ~ ConeDensity\", data, groups=\"Subject\", re_formula=\"~Eccentricity\")\n",
    "    else:\n",
    "        model = smf.mixedlm(\"LayerThickness ~ ConeDensity\", data, groups=\"Subject\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        result = model.fit()\n",
    "    return result.fe_params['ConeDensity'], result.pvalues['ConeDensity']\n",
    "\n",
    "def plot_slice_correlation(layer_name: str, degree: float | Iterable | None, direction: Direction, eccs: np.ndarray, comp_to = 'cones', restrict_to_os = False, standardization: str = 'inter'):\n",
    "    \n",
    "    fd = preprocess_functional_data(direction, standardization)\n",
    "\n",
    "    n_subject = fd['cones'].shape[0]\n",
    "\n",
    "    if degree is None:\n",
    "        degree = eccs\n",
    "        ecc_str = 'across all eccs'\n",
    "    elif isinstance(degree, Iterable):\n",
    "        degree = np.round(np.array(degree), 1)\n",
    "        ecc_str = f'on {np.min(degree)}° to {np.max(degree)}°'\n",
    "    else:\n",
    "        degree = np.round(float(degree), 1)\n",
    "        ecc_str = f'at {degree}°'\n",
    "\n",
    "    indices = np.searchsorted(eccs, degree)\n",
    "\n",
    "    cd = fd[comp_to][:, indices].flatten()\n",
    "    lt = fd[layer_name][:, indices].flatten()\n",
    "    pids = np.repeat(np.arange(n_subject), len(indices))\n",
    "    eccentricities = np.tile(eccs[indices], n_subject)\n",
    "    if restrict_to_os:\n",
    "        os_mask = ~np.isnan(fd['os'][:, indices].flatten())\n",
    "        cd = cd[os_mask]\n",
    "        lt = lt[os_mask]\n",
    "        pids = pids[os_mask]\n",
    "        eccentricities = eccentricities[os_mask]\n",
    "    valid = ~np.isnan(cd) & ~np.isnan(lt)\n",
    "    if not valid.any():\n",
    "        print(f'No valid data for {layer_name} {ecc_str}.')\n",
    "        return\n",
    "    cd = cd[valid]\n",
    "    lt = lt[valid]\n",
    "    pids = pids[valid]\n",
    "    eccentricities = eccentricities[valid]\n",
    "\n",
    "    LL_UR = (lt > 0) != (cd > 0)\n",
    "    UL_LR = (lt > 0) == (cd > 0)\n",
    "\n",
    "    spearman_corr = spearmanr(cd, lt)\n",
    "    pearson_corr = pearsonr(cd, lt)\n",
    "    if (perform_mlm := len(indices) > 1):\n",
    "        mixedlm_corr = mixedlm(cd, lt, pids, eccentricities)\n",
    "    binom_corr = binomtest(\n",
    "        LL_UR.sum() if pearson_corr.correlation < 0 else UL_LR.sum(),\n",
    "        LL_UR.sum() + UL_LR.sum(), \n",
    "        p=0.5, alternative='greater'\n",
    "    )\n",
    "\n",
    "    plot_limit = max(3, 0.1 + np.ceil(np.max(np.abs([lt, cd])) * 10) / 10)\n",
    "    # plot_limit=3.2\n",
    "    # colors = iter(plt.get_cmap('Accent', 33)(np.arange(33)).tolist())\n",
    "    # for _cd, _lt in zip(cone_density_fd.data_matrix[:,:,0], layer_fds[layer_name].data_matrix[:, :, 0]):\n",
    "    #     plt.scatter(_cd, _lt, 2, color=next(colors), alpha=0.6)\n",
    "    plt.scatter(cd[LL_UR], lt[LL_UR], 5, color='blue', alpha=0.6, label=f'n = {LL_UR.sum()}')\n",
    "    plt.scatter(cd[UL_LR], lt[UL_LR], 5, color='red', alpha=0.6, label=f'n = {UL_LR.sum()}')\n",
    "\n",
    "    plt.axhline(0, color='black', linewidth=0.5)\n",
    "    plt.axvline(0, color='black', linewidth=0.5)\n",
    "    plt.fill_between([-plot_limit, 0], -plot_limit, 0, color='red', alpha=0.05)\n",
    "    plt.fill_between([0, plot_limit], 0, plot_limit, color='red', alpha=0.05)\n",
    "    plt.fill_between([-plot_limit, 0], 0, plot_limit, color='blue', alpha=0.05)\n",
    "    plt.fill_between([0, plot_limit], -plot_limit, 0, color='blue', alpha=0.05)\n",
    "\n",
    "    indices = np.argsort(cd)\n",
    "    x_p = np.linspace(-plot_limit, plot_limit, 100)\n",
    "    if perform_mlm:\n",
    "        p = np.polyfit(cd[indices], lt[indices], 1)\n",
    "        plt.plot(x_p, np.polyval(p, x_p), color='olive', label=f'fit: y = {p[0]:.4g}x')\n",
    "    else:\n",
    "        slope_mlm = mixedlm_corr[0]\n",
    "        plt.plot(x_p, slope_mlm * x_p, color='olive', label=f'MLM: y = {slope_mlm:.4g}x')\n",
    "    # plt.plot(x_p, np.sign(p[0]) * x_p, '--', color='olive', label=f'identity', alpha=0.6)\n",
    "\n",
    "    plt.ylim(-plot_limit, plot_limit)\n",
    "    plt.xlim(-plot_limit, plot_limit)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    comp_to_str = 'Cone density' if comp_to=='cones' else f'{names_r[comp_to]} thickness'\n",
    "    plt.xlabel(f'{comp_to_str} (Z-Score)')\n",
    "    plt.ylabel(f'{names_r[layer_name]} thickness, (Z-Score)')\n",
    "    # plot legend on right side, out of the plot\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    # below the legend, add box of text displaying the statistics results\n",
    "    plt.text(1.02, 0.7, \n",
    "             f'Spearman: {spearman_corr[0]:.3f}, p={spearman_corr[1]:.2g}'\n",
    "             f'\\nPearson: {pearson_corr[0]:.3f}, p={pearson_corr[1]:.2g}'\n",
    "             f'\\nBinomial test: p={binom_corr.pvalue:.2g}'\n",
    "             f'\\nMixedLM: {mixedlm_corr[0]:.3f}, p={mixedlm_corr[1]:.2g}' if perform_mlm else '', \n",
    "             fontsize=12, ha='left', transform=plt.gca().transAxes)\n",
    "\n",
    "    plt.title(f'{names_r[layer_name]} thickness vs {comp_to_str} across subjects & {ecc_str}, {direction.value}-axis\\nStandardized {standardization}-individually')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intra-individual analysis\n",
    "\n",
    "Have a look at the violin plots as well: `P:\\AOSLO\\_automation\\_PROCESSED\\Photoreceptors\\Healthy\\_Results\\all_stats_new\\spearman_correlation_for_*.png`.\n",
    "The following plots are just an other way to visualize the same thing since both are intra-individual analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['rnfl', 'pr_rpe', 'onl', 'gcl_ipl']#, 'cones']\n",
    "for i in range(len(features)):\n",
    "    # for j in range(i+1, len(features)):\n",
    "    layer, comp_to = features[i], 'cones'#features[j]\n",
    "    # for direction in Direction:\n",
    "    plot_slice_correlation(layer, None, Direction.X, eccs, comp_to=comp_to, standardization='intra')\n",
    "    \n",
    "# plot_slice_correlation('onl', None, Direction.X, eccs, comp_to='cones', restrict_to_os=False, standardization='intra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inter-individual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['rnfl', 'pr_rpe', 'os', 'onl', 'cones']\n",
    "\n",
    "step = 0.1\n",
    "deg = np.arange(-1, 1+step, step)\n",
    "\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        layer, comp_to = features[i], features[j]\n",
    "        plot_slice_correlation(layer, deg, Direction.X, eccs, comp_to=comp_to, standardization='inter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def compute_correlations_eccwise(direction: Direction, eccs: np.ndarray, correlation_fun: Callable[[np.ndarray, np.ndarray], Tuple[float, float]] = lambda cd, lt: spearmanr(cd, lt, nan_policy='omit')\n",
    ") -> Dict[str, Tuple[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Compute inter-individual Spearman correlations between standardized cone density and each standardized layer, for each eccentricity.\n",
    "    In a nutshell, for each eccentricity, we correlate the deviations of the cone density and the layer thickness from their respective means,\n",
    "    to assess inter-individual relationships between cone density and retinal layer thicknesses.\n",
    "    \"\"\"\n",
    "    fd = preprocess_functional_data(direction, standardization='inter')\n",
    "    results = {}\n",
    "    for layer, layer_fd in fd.items():\n",
    "        if layer == 'cones':\n",
    "            continue\n",
    "        pointwise_corr = np.zeros(len(eccs))\n",
    "        pointwise_pvalues = np.zeros(len(eccs))\n",
    "        \n",
    "        # Iterate over each eccentricity to compute Spearman correlation and p-values\n",
    "        for i in range(len(eccs)):\n",
    "            cone_density_values = fd['cones'][:, i]\n",
    "            layer_values = layer_fd[:, i]\n",
    "            corr, pv = correlation_fun(cone_density_values, layer_values)\n",
    "            # corr, pv = spearmanr(cone_density_values, layer_values, nan_policy='omit')\n",
    "            pointwise_corr[i] = corr\n",
    "            pointwise_pvalues[i] = pv\n",
    "        results[layer] = (pointwise_corr, pointwise_pvalues)\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_correlations_eccwise(results: Dict[str, Tuple[np.ndarray]], eccs: np.ndarray, direction: Direction, layers_to_plot: List[str] | None = None, abs_: bool = False, pv_threshold: float | None = None, corr_name: str = 'Spearman'):\n",
    "    \"\"\"\n",
    "    Plot the pointwise correlations of cone density with each layer, for each eccentricity.\n",
    "    \"\"\"\n",
    "    f = lambda x: np.abs(x) if abs_ else x\n",
    "    colors = plt.get_cmap('Accent_r', len(results))\n",
    "    plt.figure(figsize=(10, 6), dpi=300)\n",
    "    \n",
    "    def __plot_smooth_alpha(xs, ys, alphas, color, linewidth_fun):\n",
    "        assert len(xs) == len(ys) == len(alphas) == 2\n",
    "        n_steps = 2 + int(np.abs(np.diff(alphas))[0] / 0.01)\n",
    "        x = np.linspace(xs[0], xs[1], n_steps)\n",
    "        y = np.interp(x, xs, ys)\n",
    "        alpha = np.linspace(alphas[0], alphas[1], n_steps)\n",
    "        \n",
    "        for j in range(n_steps - 1):\n",
    "            plt.plot(\n",
    "                x[j:j+2], \n",
    "                y[j:j+2],\n",
    "                color=color,\n",
    "                alpha=alpha[j],\n",
    "                linewidth=linewidth_fun(alpha[j]),\n",
    "                label=None\n",
    "            )\n",
    "\n",
    "    min_alpha = 0.15\n",
    "    plots = []\n",
    "    for i, layer in enumerate(layers_to_plot or results.keys()):\n",
    "        correlations, pvalues = results[layer]\n",
    "        alphas = np.minimum(\n",
    "            1, \n",
    "            np.where(np.isnan(pvalues), 0, min_alpha + (1 - min_alpha) * (1 - pvalues) ** 4)\n",
    "        )  # Higher alpha for smaller p-values, lower alpha for larger p-values\n",
    "        if pv_threshold is not None:\n",
    "            alphas = np.where(pvalues <= pv_threshold, alphas, 0)\n",
    "\n",
    "        for j in range(len(eccs) - 1):\n",
    "            __plot_smooth_alpha(\n",
    "                eccs[j:j+2], \n",
    "                f(correlations[j:j+2]),\n",
    "                color=colors(i),\n",
    "                alphas=alphas[j:j+2],\n",
    "                linewidth_fun=lambda a: 0.3 + a * 0.7,\n",
    "            )\n",
    "        \n",
    "        label = names_r[layer] if layer != 'nonfit' else \"nonfit\"\n",
    "        plots.append(plt.scatter(\n",
    "            eccs, f(correlations), \n",
    "            label=label,\n",
    "            color=colors(i),\n",
    "            alpha=alphas,\n",
    "            edgecolors='none',\n",
    "            s=20 + alphas * 30\n",
    "        ))\n",
    "\n",
    "\n",
    "    legend = plt.legend(loc='best')\n",
    "\n",
    "    for lh in legend.legend_handles:\n",
    "        lh.set_alpha(np.ones_like(alphas))  # Set alpha of legend markers to 1\n",
    "\n",
    "    lim = 0.8 #np.ceil(max([abs(y) for y in plt.ylim()]) * 10) / 10\n",
    "    plt.ylim(0 if abs_ else -lim, lim)\n",
    "    plt.xlim(-10, 10)\n",
    "\n",
    "    plt.title(f'Pointwise {corr_name} Correlation of Cone Density with Layers, {direction.value}-Axis')\n",
    "    plt.xlabel('Eccentricity [°]')\n",
    "    ylabel = f'{corr_name} correlation coefficient'\n",
    "    plt.ylabel(f'|{ylabel}|' if abs_ else ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "# print(eccs)\n",
    "direction = Direction.X\n",
    "results = compute_correlations_eccwise(direction, eccs)\n",
    "plot_correlations_eccwise(results, eccs, direction, abs_=False, pv_threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "direction = Direction.Y\n",
    "results = compute_correlations_eccwise(direction, eccs)\n",
    "plot_correlations_eccwise(results, eccs, direction, abs_=False, pv_threshold=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cone spacing analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reproducing Figure 3 B. from [Foote et al. 2019](https://doi.org/10.1167/iovs.18-25688), which compares cone spacing and OS thickness in healthy subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_to_spacing(cd: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert cone density [cells/mm²] to cone spacing [arcmin].\n",
    "    \"\"\"\n",
    "    return np.sqrt(2 / cd / np.sqrt(3)) / MM_PER_DEGREE * 60 # in arcmin\n",
    "\n",
    "def get_data_from_range(layer_name, left, right):\n",
    "    range_eccs = np.argwhere((left <= eccs) & (eccs < right)).flatten()\n",
    "    cd = np.array([getattr(s, f'density_fit_X') for s in subjects_data])[:, range_eccs].flatten()\n",
    "    os = np.array([getattr(s, f'{layer_name}_X') for s in subjects_data])[:, range_eccs].flatten()\n",
    "    # print(os)\n",
    "    valid = ~np.isnan(cd) & ~np.isnan(os)\n",
    "    cd = cd[valid]\n",
    "    os = os[valid]\n",
    "    return cd, os\n",
    "\n",
    "start = -0.5\n",
    "end = 0.5\n",
    "step = 0.1\n",
    "layer_name = 'os'\n",
    "cd, lt = get_data_from_range(layer_name, start, end+step) \n",
    "icd = density_to_spacing(cd)\n",
    "p = np.polyfit(cd, lt * 1000, 1)\n",
    "# Compute fitted values\n",
    "lt_fitted = np.polyval(p, cd)\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(np.mean((lt * 1000 - lt_fitted) ** 2))\n",
    "\n",
    "plt.scatter(cd, lt * 1000, 5, color='gray', alpha=0.6, label=f'{start:.1f}° to {end:.1f}°, β={p[0]:.2f}, RMSE={rmse:.2f}')\n",
    "plt.plot(np.sort(cd), np.polyval(p, np.sort(cd)), color='black', alpha=1, label=f'All data, {start:.1f}° to {end:.1f}°, β={p[0]:.2f}, RMSE={rmse:.2f}')\n",
    "\n",
    "plt.xlabel('Cone density (cones/mm^2)', fontsize=14)\n",
    "plt.ylabel(f'{names_r[layer_name]} thickness (µm)', fontsize=14)\n",
    "plt.title(f'Cone density vs {names_r[layer_name]} thickness across all patients and {start:.1f}° to {end:.1f}° eccs', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of above figure: the lower the cone spacing (i.e. the higher the cone density), the thicker the OS layer. Note that it ***does not*** contradict the previous analysis, where we found that subjects with higher cone density have thinner foveal OS layer. Indeed, what is shown here (which is a mere aggregation of intra-individual analysis) only confirms that the OS layer is thicker in the fovea, where cone density peaks; there is no surprise here. The previous analysis, on the other hand, really was inter-individual, and showed that subjects with higher foveal cone density tend to have thinner foveal OS layer, which in itself is a more interesting finding.\n",
    "\n",
    "Note that both observations are not mutually exclusive: it is in fact an instance of [Simpsom's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox#/media/File:Simpsons_paradox_-_animation.gif). Indeed, when stratifying the data by eccentricity (see next figure, for PR+RPE from -0.5° to 0.5° to make it more obvious), we recover what we found in the previous inter-individual analysis: around the fovea, cone spacing is positively correlated with OS thickness (i.e. the higher the cone density, the thinner the OS layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "start = -0.5\n",
    "end = 0.5\n",
    "step = 0.1\n",
    "layer_name = 'os'\n",
    "spacing = True\n",
    "\n",
    "# Create a figure and axis so we can place the legend properly\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "degrees_range = np.round(np.arange(start, end, step), 1)\n",
    "colors = plt.get_cmap('Accent', len(degrees_range))\n",
    "\n",
    "for i, deg in enumerate(degrees_range):\n",
    "    cd, lt = get_data_from_range(layer_name, deg, deg + step)\n",
    "    \n",
    "    # Example: convert cd to spacing if requested\n",
    "    icd = density_to_spacing(cd)\n",
    "    if spacing:\n",
    "        cd = icd\n",
    "    \n",
    "    # Optional cleaning step (remove NaN / inf) if you suspect such data\n",
    "    mask = (~np.isnan(cd)) & (~np.isnan(lt)) & np.isfinite(cd) & np.isfinite(lt)\n",
    "    cd = cd[mask]\n",
    "    lt = lt[mask]\n",
    "    \n",
    "    # Skip if not enough points\n",
    "    if len(cd) < 2:\n",
    "        continue\n",
    "    \n",
    "    # Linear regression using linregress\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(cd, lt * 1000)\n",
    "    \n",
    "    # Compute predicted values\n",
    "    lt_fitted = slope * cd + intercept\n",
    "    \n",
    "    # Compute RMSE\n",
    "    rmse = np.sqrt(np.mean((lt * 1000 - lt_fitted) ** 2))\n",
    "    \n",
    "    # Scatter plot of points\n",
    "    ax.scatter(cd, lt * 1000, s=5, color=colors(i), alpha=0.6,\n",
    "               label=(f'{deg:.1f}° to {deg + step:.1f}°, '\n",
    "                      f'β={slope:.2f}, p={p_value:.3g}'))\n",
    "    \n",
    "    # Plot regression line (sorted by x for a clean line)\n",
    "    idx_sorted = np.argsort(cd)\n",
    "    ax.plot(cd[idx_sorted], lt_fitted[idx_sorted], color=colors(i), alpha=0.6)\n",
    "\n",
    "# Now do the regression for all data combined\n",
    "cd_all, lt_all = get_data_from_range(layer_name, start, end + step)\n",
    "if spacing:\n",
    "    cd_all = density_to_spacing(cd_all)\n",
    "\n",
    "# Optional cleaning for \"all\" data\n",
    "mask_all = (~np.isnan(cd_all)) & (~np.isnan(lt_all)) & np.isfinite(cd_all) & np.isfinite(lt_all)\n",
    "cd_all = cd_all[mask_all]\n",
    "lt_all = lt_all[mask_all]\n",
    "\n",
    "if len(cd_all) >= 2:\n",
    "    slope_all, intercept_all, r_value_all, p_value_all, std_err_all = linregress(cd_all, lt_all * 1000)\n",
    "    lt_fitted_all = slope_all * cd_all + intercept_all\n",
    "    rmse_all = np.sqrt(np.mean((lt_all * 1000 - lt_fitted_all) ** 2))\n",
    "    \n",
    "    idx_all_sorted = np.argsort(cd_all)\n",
    "    ax.plot(cd_all[idx_all_sorted],\n",
    "            lt_fitted_all[idx_all_sorted],\n",
    "            color='black', alpha=1,\n",
    "            label=(f'All data, {start:.1f}° to {end:.1f}°, '\n",
    "                   f'β={slope_all:.2f}, p={p_value_all:.3g}'))\n",
    "\n",
    "ax.set_xlabel('Cone density (cones/mm^2)', fontsize=14)\n",
    "ax.set_ylabel(f'{names_r[layer_name]} thickness (µm)', fontsize=14)\n",
    "ax.set_title(f'Cone density vs {names_r[layer_name]} thickness '\n",
    "             f'across all patients\\n{start:.1f}° to {end:.1f}° eccentricities',\n",
    "             fontsize=14)\n",
    "\n",
    "# Place legend outside\n",
    "if len(degrees_range) > 10:\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "else:\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "start = -2.5\n",
    "end = 2.5\n",
    "step = 0.1\n",
    "layer_name = 'os'\n",
    "spacing = True\n",
    "\n",
    "degrees_range = np.round(np.arange(start, end, step), 1)\n",
    "colors = plt.get_cmap('Accent', len(degrees_range))\n",
    "\n",
    "for i, deg in enumerate(degrees_range):\n",
    "    cd, lt = get_data_from_range(layer_name, deg, deg + step) \n",
    "    icd = density_to_spacing(cd)\n",
    "\n",
    "    if spacing:\n",
    "        cd = icd\n",
    "    \n",
    "    # --- Data cleaning step ---\n",
    "    # Remove NaNs and optionally infinities\n",
    "    mask = (~np.isnan(cd)) & (~np.isnan(lt)) & np.isfinite(cd) & np.isfinite(lt)\n",
    "    cd = cd[mask]\n",
    "    lt = lt[mask]\n",
    "    \n",
    "    # If after cleaning we have too few points, skip\n",
    "    if len(cd) < 2:\n",
    "        continue\n",
    "\n",
    "    # Use linregress\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(cd, lt * 1000)\n",
    "    \n",
    "    lt_fitted = slope * cd + intercept\n",
    "\n",
    "    plt.scatter(\n",
    "        cd, lt * 1000, s=5, color=colors(i), alpha=0.6,\n",
    "        label=(\n",
    "            f'{deg:.1f}° to {deg + step:.1f}°, '\n",
    "            f'β={slope:.2f}, p={p_value:.3g}'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    sorted_idx = np.argsort(cd)\n",
    "    cd_sorted = cd[sorted_idx]\n",
    "    lt_fit_sorted = lt_fitted[sorted_idx]\n",
    "\n",
    "    plt.plot(cd_sorted, lt_fit_sorted, color=colors(i), alpha=0.6)\n",
    "\n",
    "# Fit and plot for all data\n",
    "cd_all, lt_all = get_data_from_range(layer_name, start, end + step)\n",
    "icd_all = density_to_spacing(cd_all)\n",
    "\n",
    "if spacing:\n",
    "    cd_all = icd_all\n",
    "\n",
    "# --- Data cleaning for \"all data\" ---\n",
    "mask_all = (~np.isnan(cd_all)) & (~np.isnan(lt_all)) & np.isfinite(cd_all) & np.isfinite(lt_all)\n",
    "cd_all = cd_all[mask_all]\n",
    "lt_all = lt_all[mask_all]\n",
    "\n",
    "if len(cd_all) >= 2:\n",
    "    slope_all, intercept_all, r_value_all, p_value_all, std_err_all = linregress(cd_all, lt_all * 1000)\n",
    "    lt_fitted_all = slope_all * cd_all + intercept_all\n",
    "\n",
    "\n",
    "    sorted_idx_all = np.argsort(cd_all)\n",
    "    cd_sorted_all = cd_all[sorted_idx_all]\n",
    "    lt_fit_sorted_all = lt_fitted_all[sorted_idx_all]\n",
    "\n",
    "    plt.plot(\n",
    "        cd_sorted_all, lt_fit_sorted_all,\n",
    "        color='black', alpha=1,\n",
    "        label=(\n",
    "            f'All data, {start:.1f}° to {end:.1f}°, '\n",
    "            f'β={slope_all:.2f}, p={p_value_all:.3g}'\n",
    "        )\n",
    "    )\n",
    "plt.ylabel(f'{names_r[layer_name]} thickness (µm)', fontsize=14)\n",
    "if spacing:\n",
    "    plt.xlabel('Cone spacing (arcmin)', fontsize=14)\n",
    "    plt.title(\n",
    "        f'Cone spacing vs {names_r[layer_name]} thickness across all patients '\n",
    "        f'and {start:.1f}° to {end:.1f}° eccs',\n",
    "        fontsize=14\n",
    "    )\n",
    "else:\n",
    "    plt.xlabel('Cone density (cones/mm^2)', fontsize=14)\n",
    "\n",
    "    plt.title(\n",
    "        f'Cone density vs {names_r[layer_name]} thickness across all patients '\n",
    "        f'and {start:.1f}° to {end:.1f}° eccs',\n",
    "        fontsize=14\n",
    "    )\n",
    "if len(degrees_range) > 10:\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "else:\n",
    "    plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = -0.5\n",
    "# end = 0.5\n",
    "# step = 0.1\n",
    "# layer_name = 'os'\n",
    "\n",
    "\n",
    "def plot_scatter_fit(start: float, end: float, step: float, layer_name: str, spacing: bool = False):\n",
    "    # Create figure and axis for custom legend placement\n",
    "    # fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    degrees_range = np.round(np.arange(start, end, step), 1)\n",
    "    colors = plt.get_cmap('Accent', len(degrees_range))\n",
    "    \n",
    "    # Lists to collect the already z-scored cone density (cd_z) and corresponding thickness values\n",
    "    all_cd_z = []\n",
    "    all_lt_vals = []\n",
    "    \n",
    "    for i, deg in enumerate(degrees_range):\n",
    "        # print(layer_name)\n",
    "        cd, lt = get_data_from_range(layer_name, deg, deg + step)\n",
    "        icd = density_to_spacing(cd)\n",
    "        if spacing:\n",
    "            cd = icd\n",
    "        \n",
    "        # Clean the data (remove NaNs/infs)\n",
    "        mask = (~np.isnan(cd)) & (~np.isnan(lt)) & np.isfinite(cd) & np.isfinite(lt)\n",
    "        cd = cd[mask]\n",
    "        lt = lt[mask]\n",
    "        \n",
    "        if len(cd) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Compute z-score for cone density using the subrange's mean and std\n",
    "        mean_cd = np.mean(cd)\n",
    "        std_cd = np.std(cd)\n",
    "        if std_cd == 0:\n",
    "            continue\n",
    "        cd_z = (cd - mean_cd) / std_cd\n",
    "        \n",
    "        # Save these z-scored values and their corresponding thickness values\n",
    "        all_cd_z.append(cd_z)\n",
    "        all_lt_vals.append(lt * 1000)  # Convert thickness to µm\n",
    "        \n",
    "        # Perform regression on the subrange's z-scored data\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(cd_z, lt * 1000)\n",
    "        lt_fitted = slope * cd_z + intercept\n",
    "        \n",
    "        # Compute RMSE\n",
    "        rmse = np.sqrt(np.mean((lt * 1000 - lt_fitted) ** 2))\n",
    "        \n",
    "        # Scatter plot for subrange data\n",
    "        plt.scatter(cd_z, lt * 1000, s=5, color=colors(i), alpha=0.6,\n",
    "                   label=f'{deg:.1f}° to {deg+step:.1f}°, β={slope:.2f}, p={p_value:.3g}')\n",
    "        \n",
    "        # Plot the regression line for this subrange (sorting for a smooth line)\n",
    "        idx_sorted = np.argsort(cd_z)\n",
    "        plt.plot(cd_z[idx_sorted], lt_fitted[idx_sorted], color=colors(i), alpha=0.6)\n",
    "    \n",
    "    # Combine all the already z-scored data from each subrange\n",
    "    if all_cd_z and all_lt_vals:\n",
    "        all_cd_z = np.concatenate(all_cd_z)\n",
    "        all_lt_vals = np.concatenate(all_lt_vals)\n",
    "        \n",
    "        # Compute regression on the combined, already z-scored data\n",
    "        slope_all, intercept_all, r_value_all, p_value_all, std_err_all = linregress(all_cd_z, all_lt_vals)\n",
    "        lt_fitted_all = slope_all * all_cd_z + intercept_all\n",
    "        rmse_all = np.sqrt(np.mean((all_lt_vals - lt_fitted_all) ** 2))\n",
    "        \n",
    "        # Plot the regression line for all combined data\n",
    "        idx_sorted_all = np.argsort(all_cd_z)\n",
    "        plt.plot(all_cd_z[idx_sorted_all], lt_fitted_all[idx_sorted_all],\n",
    "                color='black', alpha=1,\n",
    "                label=f'All data, β={slope_all:.2f}, p={p_value_all:.3g}')\n",
    "    \n",
    "    # Plot cosmetics\n",
    "    \n",
    "    plt.ylabel(f'{layer_name} thickness (µm)', fontsize=14)\n",
    "\n",
    "    if spacing:\n",
    "       plt.title(f'Z-scored Cone Spacing vs {layer_name} Thickness', fontsize=14)\n",
    "       plt.xlabel('Z-scored Cone Spacing', fontsize=14)\n",
    "    else: \n",
    "        plt.title(f'Z-scored Cone Density vs {layer_name} Thickness', fontsize=14)\n",
    "        plt.xlabel('Z-scored Cone Density', fontsize=14)\n",
    "        \n",
    "    if len(degrees_range) > 10:\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    else:\n",
    "        plt.legend()\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "layer_names_alt = [\n",
    "    # \"cvi\",\n",
    "    # \"gcl_ipl\",\n",
    "    # \"onl\",\n",
    "    # \"inl_opl\",\n",
    "    # \"rnfl\",\n",
    "    # \"chrd\",\n",
    "    # \"pr_rpe\",\n",
    "    \"os\"\n",
    "]\n",
    "\n",
    "if len(layer_names_alt) == 1:\n",
    "    plot_scatter_fit(start = -2.5, end = 2.5, step = 0.1, layer_name= layer_names_alt[0], spacing = True)\n",
    "else:\n",
    "    for layer_name in layer_names_alt:\n",
    "        print(layer_name)\n",
    "        plot_scatter_fit(start = -2.5, end = 2.5, step = 0.1, layer_name= layer_names_alt, spacing = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "results = compute_correlations_eccwise(Direction.X, eccs)\n",
    "# print(results)\n",
    "threshold = 0.05\n",
    "layers_to_plot = ['rnfl', 'pr_rpe', 'os']\n",
    "\n",
    "#Range of eccentricity to plot\n",
    "range_start = -2.5\n",
    "range_end = 0.5\n",
    "\n",
    "# correlations_to_plot = \"negative\"\n",
    "\n",
    "\n",
    "\n",
    "for layer_name, (correlations, p_values) in results.items():\n",
    "# checks if we want to plot the layer\n",
    "    if layer_name not in layers_to_plot:\n",
    "        continue\n",
    "\n",
    "    for ecc, corr, p_val in zip(eccs, correlations, p_values):\n",
    "        #checks if the ecc is in the range\n",
    "        if ecc < range_start or ecc > range_end:\n",
    "            continue\n",
    "\n",
    "        if p_val < threshold:\n",
    "            # 1) Retrieve the actual data for this layer and eccentricity\n",
    "            x_data, y_data = get_data_from_range(layer_name, (ecc-0.05), (ecc+0.05))\n",
    "            \n",
    "\n",
    "            \n",
    "            # 3) Fit a line via linregress\n",
    "            slope, intercept, r_value, p_value_fit, std_err = linregress(x_data, y_data)\n",
    "            \n",
    "            # 4) Compute points on the line for plotting\n",
    "            x_line = np.linspace(x_data.min(), x_data.max(), 100)\n",
    "            y_line = slope * x_line + intercept\n",
    "\n",
    "            if layer_name != 'cvi':\n",
    "                # 2) Scatter plot\n",
    "                plt.figure(figsize=(5, 4))\n",
    "                plt.scatter(x_data, y_data, label='Data points')\n",
    "\n",
    "                plt.plot(x_line, y_line, color='red',\n",
    "                        label=(f'Fit: slope={slope:.3f}, '\n",
    "                                f'intercept={intercept:.3f}, '\n",
    "                                f'fit p={p_value_fit:.3g}'))\n",
    "                \n",
    "                # 5) Add some labels/legend\n",
    "                plt.title(f'{names_r[layer_name]}, Ecc={ecc}, Corr p<{threshold}, Spearman corr : {corr:.3f}')\n",
    "                plt.ylabel('Layer thickness (mm)')\n",
    "                plt.xlabel('Photoreceptor density (cones/mm²)')\n",
    "                plt.legend()\n",
    "                \n",
    "                # 6) Show or save the figure\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr, theilslopes\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Compute max of cone density\n",
    "max_cd = np.max(cone_density_fd_X, axis=1)\n",
    "\n",
    "# Get max of outer segment thickness (OS), handling NaNs\n",
    "os = np.vstack([getattr(s, f'os_X') for s in subjects_data])\n",
    "os_cleaned = np.where(np.isnan(os), -np.inf, os)  # Replace NaNs with -inf\n",
    "max_os = np.max(os_cleaned, axis=1)\n",
    "\n",
    "# Winsorizing (reducing effect of outliers)\n",
    "max_cd_winsor = winsorize(max_cd, limits=[0.05, 0.05])  # Trim top/bottom 5%\n",
    "max_os_winsor = winsorize(max_os, limits=[0.05, 0.05])\n",
    "\n",
    "# Spearman correlation (non-parametric, robust to non-normality)\n",
    "spearman_corr, spearman_p = spearmanr(max_cd_winsor, max_os_winsor)\n",
    "print(f\"Spearman correlation: {spearman_corr:.3f}, p-value: {spearman_p:.3f}\")\n",
    "\n",
    "# Theil-Sen robust regression (handles outliers better than OLS)\n",
    "slope, intercept, lower, upper = theilslopes(max_os_winsor * 1000, max_cd_winsor, 0.95)\n",
    "print(f\"Robust fit: y = {slope:.2f}x + {intercept:.2f}\")\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(max_cd_winsor, max_os_winsor * 1000, color='blue', alpha=0.7, label='Data Points')\n",
    "plt.plot(np.sort(max_cd_winsor), slope * np.sort(max_cd_winsor) + intercept, color='red', linewidth=2, label=f'Robust Fit: y = {slope:.2f}x')\n",
    "\n",
    "# Labels and title\n",
    "plt.ylabel('Max Outer Segment Thickness (µm)')\n",
    "plt.xlabel('Max Cone Density (cones/mm^2)')\n",
    "plt.title('Scatter Plot: Max Cone Density vs Max Outer Segment Thickness')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlating Axial length with layer thicknesses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd.axial_length = axial_dict[sd.name]\n",
    "# sd.spherical_equiv = spherical_dict[sd.name]\n",
    "\n",
    "for s in subjects_data:\n",
    "        print(\"axial_lenght\", s.axial_length)\n",
    "        print(\"spherical_equiv\", s.spherical_equiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_axial_length_correlation(direction : Direction, subjects_data : list = subjects_data ):\n",
    "\n",
    "    #initializes an array for the axial lengths of length number of subjects\n",
    "    axial_lengths = np.zeros(len(subjects_data))\n",
    "    for i, s in enumerate(subjects_data):\n",
    "        axial_lengths[i] = s.axial_length\n",
    "\n",
    "    #initializes a dict to save the correlation results\n",
    "    results = {}\n",
    "\n",
    "\n",
    "    for layer in layer_names:\n",
    "        print(f\"Processing {layer} layer\")\n",
    "        #initializes a 2d array for the layer thicknesses of length number of subjects\n",
    "        layer_fds = np.zeros((len(subjects_data), len(eccs)))\n",
    "        for i, s in enumerate(subjects_data):\n",
    "            layer_fds[i] = np.array(getattr(s, f'{layer}_{direction.value}'))\n",
    "\n",
    "        bin_centers, correlations, pvalues = compute_axial_length_correlations_interval(layer_fds, axial_lengths, eccs)\n",
    "        results[layer] = (correlations, pvalues)\n",
    "\n",
    "    return bin_centers, results, axial_lengths\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def compute_axial_length_correlations_interval(thicknesses: np.ndarray, \n",
    "                                               axial_lengths: np.ndarray,\n",
    "                                               eccs: np.ndarray,\n",
    "                                               interval: float = 0.5,\n",
    "                                               correlation_fun: Callable[[np.ndarray, np.ndarray], Tuple[float, float]] = lambda x, y: spearmanr(x, y, nan_policy='omit')\n",
    "                                              ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute Spearman correlations between axial length and the layer thickness averaged over specified eccentricity intervals.\n",
    "    \n",
    "    Parameters:\n",
    "      thicknesses: 2D numpy array of shape (n_subjects, n_eccentricities) containing layer thicknesses.\n",
    "      axial_lengths: 1D numpy array of shape (n_subjects,) with each subject's axial length.\n",
    "      eccs: 1D numpy array of eccentricity values corresponding to the columns of `thicknesses`.\n",
    "      interval: float, the width of each eccentricity bin (default 0.5).\n",
    "      correlation_fun: Function to compute correlation (default uses spearmanr with nan_policy='omit').\n",
    "      \n",
    "    Returns:\n",
    "      bin_centers: 1D numpy array of the center eccentricity for each bin.\n",
    "      correlations: 1D numpy array of correlation coefficients for each bin.\n",
    "      p_values: 1D numpy array of corresponding p-values.\n",
    "    \"\"\"\n",
    "    # Create bins based on the eccentricity range\n",
    "    min_e = np.min(eccs)\n",
    "    max_e = np.max(eccs)\n",
    "    bins = np.arange(min_e, max_e + interval, interval)\n",
    "    # Digitize eccentricity values into bins (bins numbered starting at 1)\n",
    "    bin_indices = np.digitize(eccs, bins)\n",
    "    \n",
    "    unique_bins = np.unique(bin_indices)\n",
    "    correlations = []\n",
    "    p_values = []\n",
    "    bin_centers = []\n",
    "    \n",
    "    for b in unique_bins:\n",
    "        # Get indices of eccs in the current bin\n",
    "        idx = np.where(bin_indices == b)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Average the thickness values across the selected eccentricities for each subject\n",
    "        avg_thickness = np.nanmean(thicknesses[:, idx], axis=1)\n",
    "        \n",
    "        # Z-score the average thickness across subjects\n",
    "        mean_val = np.nanmean(avg_thickness)\n",
    "        std_val = np.nanstd(avg_thickness)\n",
    "        if std_val != 0:\n",
    "            avg_thickness_z = (avg_thickness - mean_val) / std_val\n",
    "        else:\n",
    "            avg_thickness_z = avg_thickness\n",
    "        \n",
    "        # Compute the correlation between axial_lengths and the z-scored average thickness\n",
    "        corr, p = correlation_fun(axial_lengths, avg_thickness_z)\n",
    "        correlations.append(corr)\n",
    "        p_values.append(p)\n",
    "        \n",
    "        # Use the mean of the eccs in the bin as the bin center\n",
    "        center = np.mean(eccs[idx])\n",
    "        bin_centers.append(center)\n",
    "        \n",
    "    return np.array(bin_centers), np.array(correlations), np.array(p_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_centers, axcorr_results, axial_lengths = process_axial_length_correlation(Direction.X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_threshold = 0.2\n",
    "\n",
    "# Plot the significant points\n",
    "ecc = eccs\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for layer, (corr, pval) in axcorr_results.items():\n",
    "    sig_idx = pval < sig_threshold\n",
    "    # If there are significant points, scatter them with the layer label.\n",
    "    if np.any(sig_idx):\n",
    "        plt.scatter(bin_centers[sig_idx], corr[sig_idx], s=50, label=layer, edgecolor='black')\n",
    "    else:\n",
    "        # No significant points: add a dummy scatter for the legend.\n",
    "        plt.scatter([], [], s=50, label=layer)\n",
    "\n",
    "plt.xlabel('Eccentricity')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title('Axial Length Correlations (Significant Points)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def plot_axial_length_layer_scatter_grouped(axial_lengths: np.ndarray, \n",
    "                                              layer_fds: np.ndarray, \n",
    "                                              eccs: np.ndarray, \n",
    "                                              interval: float = 0.5,\n",
    "                                              significance_threshold: float = 0.05):\n",
    "    \"\"\"\n",
    "    Plot axial length vs. layer thickness for each data point, color-coded by the eccentricity bin\n",
    "    (without averaging over the bin). For each bin, a linear regression is performed and its line is \n",
    "    plotted if significant.\n",
    "\n",
    "    Parameters:\n",
    "      axial_lengths: 1D array of shape (n_subjects,) containing each subject's axial length.\n",
    "      layer_fds: 2D array of shape (n_subjects, n_eccentricities) for the layer thickness.\n",
    "      eccs: 1D array of eccentricity values corresponding to the columns of layer_fds.\n",
    "      interval: The width of each eccentricity bin (default 0.5).\n",
    "      significance_threshold: p-value threshold for plotting the regression line.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Define bins over the range of eccentricities.\n",
    "    min_e, max_e = np.min(eccs), np.max(eccs)\n",
    "    bins = np.arange(min_e, max_e + interval, interval)\n",
    "    # For each eccentricity, determine its bin (np.digitize returns indices starting at 1)\n",
    "    bin_indices = np.digitize(eccs, bins)\n",
    "    unique_bins = np.unique(bin_indices)\n",
    "    \n",
    "    # Use the 'Accent' colormap with one color per unique bin.\n",
    "    cmap = plt.get_cmap('Accent', len(unique_bins))\n",
    "    norm = plt.Normalize(vmin=bins[0], vmax=bins[-1])\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    for j, b in enumerate(unique_bins):\n",
    "        # b is the bin number (starting at 1). Determine the corresponding eccentricity indices.\n",
    "        idx = np.where(bin_indices == b)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        \n",
    "        # For each subject and for each eccentricity in this bin, we have a data point.\n",
    "        # Expand axial_lengths so that each subject's axial length is paired with each ecc in idx.\n",
    "        n_subjects = axial_lengths.shape[0]\n",
    "        X = np.repeat(axial_lengths, len(idx))\n",
    "        Y = layer_fds[:, idx].flatten()\n",
    "        # Get the corresponding eccentricity for each data point (for later use if needed)\n",
    "        ecc_values = np.tile(eccs[idx], n_subjects)\n",
    "        \n",
    "        # Remove any non-finite values\n",
    "        mask = np.isfinite(X) & np.isfinite(Y)\n",
    "        X_clean = X[mask]\n",
    "        Y_clean = Y[mask]\n",
    "        if len(X_clean) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Scatter plot: all points in this bin get the same color.\n",
    "        # For labeling, we show the eccentricity interval corresponding to this bin.\n",
    "        bin_start = bins[b - 1]\n",
    "        bin_end = bin_start + interval\n",
    "        ax.scatter(X_clean, Y_clean, s=20, color=cmap(j), alpha=0.5, \n",
    "                   label=f'{bin_start:.2f}°–{bin_end:.2f}°')\n",
    "        \n",
    "        # Perform linear regression on the points in this bin.\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(X_clean, Y_clean)\n",
    "        if p_value < significance_threshold:\n",
    "            x_fit = np.linspace(np.min(X_clean), np.max(X_clean), 100)\n",
    "            y_fit = slope * x_fit + intercept\n",
    "            ax.plot(x_fit, y_fit, color=cmap(j), linewidth=2,\n",
    "                    label=f'Fit {bin_start:.2f}–{bin_end:.2f}°: β={slope:.2f}, p={p_value:.3g}')\n",
    "    \n",
    "    ax.set_xlabel('Axial Length')\n",
    "    ax.set_ylabel('Layer Thickness')\n",
    "    ax.set_title('Axial Length vs. Layer Thickness\\nColor-coded by Eccentricity Bin')\n",
    "    \n",
    "    # If there are not too many bins, show the legend; otherwise, add a colorbar.\n",
    "    if len(unique_bins) <= 10:\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    else:\n",
    "        cbar = fig.colorbar(sm, ax=ax)\n",
    "        cbar.set_label('Eccentricity')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "layer_names_alt = [\"onl\"]\n",
    "\n",
    "for layer in layer_names_alt:\n",
    "    print(f\"Processing {layer} layer\")\n",
    "    #initializes a 2d array for the layer thicknesses of length number of subjects\n",
    "    layer_fds = np.zeros((len(subjects_data), len(eccs)))\n",
    "    for i, s in enumerate(subjects_data):\n",
    "        layer_fds[i] = np.array(getattr(s, f'{layer}_{direction.value}'))\n",
    "\n",
    "    # Plot the axial length vs. layer thickness scatter plot\n",
    "    plot_axial_length_layer_scatter_grouped(axial_lengths, layer_fds, eccs, significance_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_functional_feature(data: np.ndarray, \n",
    "                                  axial_lengths: np.ndarray = None, \n",
    "                                  standardization: str = 'inter', \n",
    "                                  axialLength_standard: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess a functional feature (e.g. cone density or layer thickness, which is a function of eccentricity)\n",
    "    by (Z-)standardizing it.\n",
    "    `data` should have shape (n_subjects, n_eccentricities).\n",
    "    \n",
    "    - For an intra-individual analysis, use standardization='intra' to standardize within subjects (i.e. within each row).\n",
    "    - For an inter-individual analysis, use standardization='inter' to standardize across subjects, eccentricity-wise (i.e. within each column).\n",
    "    \"\"\"\n",
    "    # If requested, remove axial length effects\n",
    "    if axialLength_standard and axial_lengths is not None:\n",
    "        residuals = np.empty_like(data)\n",
    "        # For each eccentricity, regress the feature on axial length and keep the residuals\n",
    "        for i in range(data.shape[1]):\n",
    "            # Create a mask for valid (finite) values in both arrays\n",
    "            valid_mask = np.isfinite(data[:, i]) & np.isfinite(axial_lengths)\n",
    "            if np.sum(valid_mask) < 2:\n",
    "                # Not enough data to perform regression, set results to NaN\n",
    "                residuals[:, i] = np.nan\n",
    "            else:\n",
    "                coef = np.polyfit(axial_lengths[valid_mask], data[valid_mask, i], 1)  # simple linear regression\n",
    "                pred = np.polyval(coef, axial_lengths[valid_mask])\n",
    "                # Compute residuals only for valid entries; for others, assign NaN\n",
    "                res = np.full(data.shape[0], np.nan)\n",
    "                res[valid_mask] = data[valid_mask, i] - pred\n",
    "                residuals[:, i] = res\n",
    "        data = residuals\n",
    "\n",
    "    # Standardize the data as before, using only valid values\n",
    "    if standardization == 'inter':\n",
    "        # Compute mean and std ignoring NaNs\n",
    "        mean = np.nanmean(data, axis=0, keepdims=True)\n",
    "        std = np.nanstd(data, axis=0, keepdims=True)\n",
    "        return (data - mean) / std\n",
    "    elif standardization == 'intra':\n",
    "        mean = np.nanmean(data, axis=1, keepdims=True)\n",
    "        std = np.nanstd(data, axis=1, keepdims=True)\n",
    "        return (data - mean) / std\n",
    "\n",
    "    return data\n",
    "\n",
    "def preprocess_functional_data(direction: Direction, \n",
    "                               standardization: str = 'inter', \n",
    "                               toLog: bool = True, \n",
    "                               axial_lengths: np.ndarray = None, \n",
    "                               axialLength_standard: bool = False) -> Dict[str, np.ndarray]:\n",
    "    '''\n",
    "    Preprocess functional data (e.g. cone density, layer thicknesses) for a given direction (X or Y)\n",
    "    by (Z-)standardizing it.\n",
    "    '''\n",
    "    layer_fds = {\n",
    "        layer: preprocess_functional_feature(\n",
    "            data=np.array([getattr(s, f'{layer}_{direction.value}') for s in subjects_data]),\n",
    "            axial_lengths=axial_lengths,\n",
    "            standardization=standardization,\n",
    "            axialLength_standard=axialLength_standard\n",
    "        )\n",
    "        for layer in layer_names\n",
    "    }\n",
    "    \n",
    "    if toLog:\n",
    "        cone_density_fd = preprocess_functional_feature(\n",
    "            data=np.array([np.log(getattr(s, f'density_fit_{direction.value}')) for s in subjects_data]),\n",
    "            axial_lengths=axial_lengths,\n",
    "            standardization=standardization,\n",
    "            axialLength_standard=axialLength_standard\n",
    "        )\n",
    "    else:\n",
    "        cone_density_fd = preprocess_functional_feature(\n",
    "            data=np.array([getattr(s, f'density_fit_{direction.value}') for s in subjects_data]),\n",
    "            axial_lengths=axial_lengths,\n",
    "            standardization=standardization,\n",
    "            axialLength_standard=axialLength_standard\n",
    "        )\n",
    "\n",
    "    cone_density_nonfit = preprocess_functional_feature(\n",
    "        data=np.array([getattr(s, f'density_{direction.value}') for s in subjects_data]),\n",
    "        axial_lengths=axial_lengths,\n",
    "        standardization=standardization,\n",
    "        axialLength_standard=axialLength_standard\n",
    "    )\n",
    "    \n",
    "    # Return a dictionary with the cone density and the layers.\n",
    "    return {'cones': cone_density_fd, **layer_fds}\n",
    "\n",
    "def compute_correlations_eccwise(direction: Direction, \n",
    "                                 eccs: np.ndarray, \n",
    "                                 correlation_fun: Callable[[np.ndarray, np.ndarray], Tuple[float, float]] = lambda cd, lt: spearmanr(cd, lt, nan_policy='omit'),\n",
    "                                 axial_lengths: np.ndarray = None, \n",
    "                                 axialLength_standard: bool = False) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Compute inter-individual Spearman correlations between standardized cone density and each standardized layer, for each eccentricity.\n",
    "    For each eccentricity, we correlate the deviations of the cone density and the layer thickness from their respective means,\n",
    "    to assess inter-individual relationships between cone density and retinal layer thicknesses.\n",
    "    \"\"\"\n",
    "    \n",
    "    fd = preprocess_functional_data(direction=direction, \n",
    "                                    standardization='inter', \n",
    "                                    axial_lengths=axial_lengths, \n",
    "                                    axialLength_standard=axialLength_standard)\n",
    "    results = {}\n",
    "    for layer, layer_fd in fd.items():\n",
    "        if layer == 'cones':\n",
    "            continue\n",
    "        pointwise_corr = np.zeros(len(eccs))\n",
    "        pointwise_pvalues = np.zeros(len(eccs))\n",
    "        \n",
    "        # Iterate over each eccentricity to compute Spearman correlation and p-values\n",
    "        for i in range(len(eccs)):\n",
    "            cone_density_values = fd['cones'][:, i]\n",
    "            layer_values = layer_fd[:, i]\n",
    "            corr, pv = correlation_fun(cone_density_values, layer_values)\n",
    "            pointwise_corr[i] = corr\n",
    "            pointwise_pvalues[i] = pv\n",
    "        results[layer] = (pointwise_corr, pointwise_pvalues)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = Direction.X\n",
    "results = compute_correlations_eccwise(direction, eccs, axial_lengths=axial_lengths, axialLength_standard=False)\n",
    "plot_correlations_eccwise(results, eccs, direction, abs_=False, pv_threshold=0.1)\n",
    "\n",
    "results = compute_correlations_eccwise(direction, eccs, axial_lengths=axial_lengths, axialLength_standard=True)\n",
    "plot_correlations_eccwise(results, eccs, direction, abs_=False, pv_threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = Direction.Y\n",
    "results = compute_correlations_eccwise(direction, eccs, axial_lengths=axial_lengths, axialLength_standard=False)\n",
    "plot_correlations_eccwise(results, eccs, direction, abs_=False, pv_threshold=0.1)\n",
    "\n",
    "results = compute_correlations_eccwise(direction, eccs, axial_lengths=axial_lengths, axialLength_standard=True)\n",
    "plot_correlations_eccwise(results, eccs, direction, abs_=False, pv_threshold=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aoslo12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

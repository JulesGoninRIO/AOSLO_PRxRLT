{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3a471e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a40ef",
   "metadata": {},
   "source": [
    "### Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ce513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure the parent directory is in the system path for module imports\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "from pytest import param\n",
    "from zmq import has\n",
    "\n",
    "import scipy\n",
    "from scipy import integrate, interpolate\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotx\n",
    "from matplotlib.colors import to_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b7054f",
   "metadata": {},
   "source": [
    "### Plot Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotx\n",
    "\n",
    "# plt.style.use('science')  # Use ggplot style for all plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)  # Default figure size\n",
    "plt.rcParams['figure.dpi'] = 300  # Default figure dpi\n",
    "plt.rcParams['font.size'] = 12  # Default font size\n",
    "plt.rcParams['lines.linewidth'] = 2  # Default line width\n",
    "plt.rcParams['axes.labelsize'] = 14  # Default label size\n",
    "plt.rcParams['axes.titlesize'] = 16  # Default title size\n",
    "plt.rcParams['xtick.labelsize'] = 12  # Default x-tick label size\n",
    "plt.rcParams['ytick.labelsize'] = 12  # Default y-tick label size\n",
    "plt.rcParams['legend.fontsize'] = 12  # Default legend font size\n",
    "plt.rcParams['figure.titlesize'] = 18  # Default figure title size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee85ec",
   "metadata": {},
   "source": [
    "## Gathering subjects' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad41af",
   "metadata": {},
   "source": [
    "### Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c67b8",
   "metadata": {},
   "source": [
    "data structure to store subject data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class SubjectData:\n",
    "    name: str = None\n",
    "    pid: str = None\n",
    "    nb: int = None\n",
    "    session: str = None\n",
    "\n",
    "    width_nas: float = None\n",
    "    width_tem: float = None\n",
    "    width_inf: float = None\n",
    "    width_sup: float = None\n",
    "    max_slope_nas: float = None\n",
    "    max_slope_tem: float = None\n",
    "    max_slope_inf: float = None\n",
    "    max_slope_sup: float = None\n",
    "\n",
    "    oct_bump_X: float = None\n",
    "    oct_bump_Y: float = None\n",
    "    oct_width_X: float = None\n",
    "    oct_width_Y: float = None\n",
    "    oct_max_slope: float = None\n",
    "    oct_depth: float = None\n",
    "    oct_flatness: float = None\n",
    "\n",
    "    age: float = None\n",
    "    axial_length: float = None\n",
    "    spherical_equiv: float = None\n",
    "    sex: int = None\n",
    "\n",
    "    eccs: np.ndarray = None\n",
    "    density_X: pd.Series = None\n",
    "    density_Y: pd.Series = None\n",
    "    density_fit_X: pd.Series = None\n",
    "    density_fit_Y: pd.Series = None\n",
    "\n",
    "    cvi_X: pd.Series = None\n",
    "    cvi_Y: pd.Series = None\n",
    "    gcl_ipl_X: pd.Series = None\n",
    "    gcl_ipl_Y: pd.Series = None\n",
    "    onl_X: pd.Series = None\n",
    "    onl_Y: pd.Series = None\n",
    "    inl_opl_X: pd.Series = None\n",
    "    inl_opl_Y: pd.Series = None\n",
    "    rnfl_X: pd.Series = None\n",
    "    rnfl_Y: pd.Series = None\n",
    "    chrd_X: pd.Series = None\n",
    "    chrd_Y: pd.Series = None\n",
    "    pr_rpe_X: pd.Series = None\n",
    "    pr_rpe_Y: pd.Series = None\n",
    "    os_X: pd.Series = None\n",
    "    os_Y: pd.Series = None\n",
    "\n",
    "    nb_cones: float = None\n",
    "    nb_cones_fit: float = None\n",
    "\n",
    "    width_gcl_X: float = None\n",
    "    width_gcl_Y: float = None\n",
    "    min_thick_gcl: float = None\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoveaParams:\n",
    "    \"\"\"Class for storing fovea 3D fitted parameters.\"\"\"\n",
    "    # Patient information\n",
    "    subject: str\n",
    "    patient_id: str\n",
    "    subject_folder: str\n",
    "    trial_name: str\n",
    "    age: Optional[int] = None\n",
    "\n",
    "    # Fitted parameters\n",
    "    A00: Optional[float] = None\n",
    "    A10: Optional[float] = None\n",
    "    A01: Optional[float] = None\n",
    "    A20: Optional[float] = None\n",
    "    A02: Optional[float] = None\n",
    "    A11: Optional[float] = None\n",
    "    foveal_depth: Optional[float] = None\n",
    "    foveal_center_X: Optional[float] = None\n",
    "    foveal_width_X: Optional[float] = None\n",
    "    foveal_center_Y: Optional[float] = None\n",
    "    foveal_width_Y: Optional[float] = None\n",
    "    foveal_max_slope: Optional[float] = None\n",
    "    foveal_flatness: Optional[float] = None\n",
    "    foveal_volume: Optional[float] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54482948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# here to avoid having to rerun the pipeline for -\n",
    "# all subjects everytime i want to test something on the model.\n",
    " \n",
    "# Since the list of subjects is ordered by strings , it goes from 10 to 100 to 103 etc...\n",
    "# which requires a bit of work to get the first 5 subjects\n",
    "\n",
    "#It will later be used to extract the first 5 subjects from the list of subject_data\n",
    "\n",
    "take_first_five = False\n",
    "first_five_subjects = [\"Subject10\",\"Subject100\",\"Subject101\",\"Subject104\",\"Subject105\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9893f9",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1716aec0",
   "metadata": {},
   "source": [
    "#### Foveal Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1979fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fovea_data(base_path: str) -> List[FoveaParams]:\n",
    "    \"\"\"\n",
    "    Extract fovea parameters from CSV files with known structure.\n",
    "    \n",
    "    Args:\n",
    "        base_path: Path to the base directory containing subject folders\n",
    "        (subjfolder/trialfolder/layer_new/fovea_3d_fitted_params.csv)\n",
    "    \n",
    "    Returns:\n",
    "        List of FoveaParams objects, one for each found CSV file\n",
    "    \"\"\"\n",
    "    fovea_data = []\n",
    "    \n",
    "    # Get only the subject directories (directories starting with \"Subject\")\n",
    "    try:\n",
    "        subject_dirs = [d for d in os.listdir(base_path) \n",
    "                      if os.path.isdir(os.path.join(base_path, d)) and d.startswith(\"Subject\")]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Base path not found: {base_path}\")\n",
    "        return []\n",
    "    \n",
    "    # For each subject directory, get the session directories\n",
    "    for subject_dir in subject_dirs:\n",
    "        subject_path = os.path.join(base_path, subject_dir)\n",
    "        \n",
    "        # Extract subject number\n",
    "        import re\n",
    "        subject_match = re.search(r'Subject(\\d+)', subject_dir)\n",
    "        if not subject_match:\n",
    "            continue\n",
    "            \n",
    "        subject_num = subject_match.group(1)\n",
    "        patient_id = f\"{subject_num}\"\n",
    "        \n",
    "        try:\n",
    "            # Get session directories (directories starting with \"Session\")\n",
    "            session_dirs = [d for d in os.listdir(subject_path) \n",
    "                          if os.path.isdir(os.path.join(subject_path, d)) and d.startswith(\"Session\")]\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        \n",
    "        # For each session directory, check if the CSV file exists\n",
    "        for session_dir in session_dirs:\n",
    "            session_path = os.path.join(subject_path, session_dir)\n",
    "            csv_path = os.path.join(session_path, \"layer_new\", \"fovea_3d_fitted_params.csv\")\n",
    "            \n",
    "            # Check if the CSV file exists\n",
    "            if os.path.isfile(csv_path):\n",
    "                # try:\n",
    "                # Read CSV file\n",
    "                df = pd.read_csv(csv_path, sep=';', header=None, names=['param', 'value'])\n",
    "                \n",
    "                # Create basic FoveaParams object with patient info\n",
    "                fovea_obj = FoveaParams(\n",
    "                    patient_id=str(patient_id),\n",
    "                    subject=f\"Subject{patient_id}\",\n",
    "                    subject_folder=subject_dir,\n",
    "                    trial_name=session_dir\n",
    "                )\n",
    "                print(f\"Processing file: {csv_path} for patient {patient_id}\")\n",
    "                \n",
    "                # Fill in parameter values\n",
    "                for _, row in df.iterrows():\n",
    "                    param_name = row['param']\n",
    "                    param_value = row['value']\n",
    "                    print(f\"Processing {param_name} with value {param_value} for patient {patient_id}\")\n",
    "                    \n",
    "                    if param_value == \"params\":\n",
    "                        print(f\"Skipping parameter {param_name} for patient {patient_id} as it is 'params'\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if this parameter exists in our class\n",
    "                    if hasattr(fovea_obj, param_name) or hasattr(fovea_obj, f\"foveal_{param_name}\"):\n",
    "                        print(f\"Setting {param_name} for patient {patient_id}\")\n",
    "                        try:\n",
    "                            # Convert to float and set attribute\n",
    "                            print(f\"Trying to set {param_name} for patient {patient_id}\")\n",
    "                            setattr(fovea_obj, param_name, float(param_value))\n",
    "                            print(f\"Successfully set {param_name} for patient {patient_id}\")\n",
    "                        except:\n",
    "                            \n",
    "                            print(f\"Error setting {param_name} for patient {patient_id}: {param_value}\")\n",
    "\n",
    "                    if hasattr(fovea_obj, f\"foveal_{param_name}\"):\n",
    "                        try:\n",
    "                            print(f\"Trying to set foveal_{param_name} for patient {patient_id}\")\n",
    "                            setattr(fovea_obj, f\"foveal_{param_name}\", float(param_value))\n",
    "\n",
    "                        except:\n",
    "                            print(f\"Skipping parameter {param_name} for patient {patient_id} \")\n",
    "\n",
    "                            pass\n",
    "                    \n",
    "                fovea_data.append(fovea_obj)\n",
    "                # except Exception as e:\n",
    "                    # print(f\"Error processing file {csv_path}: {str(e)}\")\n",
    "    \n",
    "    return fovea_data\n",
    "\n",
    "def save_to_dataframe(fovea_data: List[FoveaParams], output_file: str = \"fovea_parameters.csv\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert list of FoveaParams objects to a pandas DataFrame and save to CSV.\n",
    "    \n",
    "    Args:\n",
    "        fovea_data: List of FoveaParams objects\n",
    "        output_file: Path to save the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing all fovea data\n",
    "    \"\"\"\n",
    "    # Convert to list of dictionaries\n",
    "    data_dicts = [vars(f) for f in fovea_data]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_dicts)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586969b",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e436c87",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from src.cell.analysis.constants import MM_PER_DEGREE\n",
    "from src.cell.layer.helpers import gaussian_filter_nan\n",
    "from src.configs.parser import Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cd5bfd",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parser.initialize()\n",
    "\n",
    "subjects_sessions = [[int(n) for n in s.strip().split()] for s in open('../src/processed.txt').readlines()] \n",
    "\n",
    "\n",
    "try:\n",
    "    sheet = pd.ExcelFile(r'V:\\Studies\\AOSLO\\data\\cohorts\\AOSLO healthy\\DATA_HC+DM.xlsx').parse('Healthy', header=0, nrows=45, index_col=0)\n",
    "    sheet.index = sheet.index.map(lambda x: f'Subject{x}')\n",
    "    age_dict = ((sheet['Date of visit'] - sheet['DDN']).dt.days / 365).to_dict()\n",
    "    axial_dict = sheet['AL D (mm)'].where(sheet['Laterality'] == 'OD', sheet['AL G (mm)']).to_dict()\n",
    "    spherical_dict = sheet['Equi Sph D'].where(sheet['Laterality'] == 'OD', sheet['Equi Sph G']).to_dict()\n",
    "    sex_dict = sheet['Sexe'].map(lambda x: 1 if x == 'F' else 0).to_dict()\n",
    "except:\n",
    "    # if the excel file is not found, use a hardcoded dictionary\n",
    "    age_dict = {}\n",
    "base_path = Path(r'P:\\AOSLO\\_automation\\_PROCESSED\\Photoreceptors\\Healthy\\_Results')\n",
    "\n",
    "# look-up table for subject and session numbers\n",
    "\n",
    "\n",
    "# subject for which OCTs are tilted (white dot is not well aligned with PR+RPE peak)\n",
    "# see explanation in `PRxRLT_expmanual.ipynb`\n",
    "oct_to_exclude = {\n",
    "    13, 18, 20, 25, 26, 30, 35, 42, 46, 66, 100, 105,\n",
    "} \n",
    "\n",
    "\n",
    "subjects_data: List[SubjectData] = []\n",
    "for subject_n, session_n in subjects_sessions:\n",
    "    if subject_n in oct_to_exclude:\n",
    "        continue\n",
    "\n",
    "    sd = SubjectData()\n",
    "    sd.name = f'Subject{subject_n}'\n",
    "    sd.pid = f'AOHC_{subject_n}'\n",
    "    sd.nb = subject_n\n",
    "    sd.session = f'Session{session_n}'\n",
    "\n",
    "    #\n",
    "    path = base_path / sd.name / sd.session\n",
    "    print(f'Loading {sd.name} {sd.session}...')\n",
    "\n",
    "    # record subject's metadata from the excel sheet\n",
    "    sd.age = age_dict[sd.name]\n",
    "    sd.axial_length = axial_dict[sd.name]\n",
    "    sd.spherical_equiv = spherical_dict[sd.name]\n",
    "    sd.sex = sex_dict[sd.name]\n",
    "\n",
    "    # record foveal shape parameters (populated by `src/save_layer_features.ipynb`)\n",
    "    df_oct = pd.read_csv(path / Parser.get_layer_thickness_dir() / 'fovea_3d_fitted_params.csv', sep=';', index_col=0)\n",
    "    sd.oct_bump_X = df_oct.loc['A20', 'params']\n",
    "    sd.oct_bump_Y = df_oct.loc['A02', 'params']\n",
    "    sd.oct_width_X = df_oct.loc['width_X', 'params'] * np.sqrt(2 * 2.8) / MM_PER_DEGREE # in °\n",
    "    sd.oct_width_Y = df_oct.loc['width_Y', 'params'] * np.sqrt(2 * 2.8) / MM_PER_DEGREE # in °\n",
    "    sd.oct_max_slope = df_oct.loc['max_slope', 'params']\n",
    "    sd.oct_depth = df_oct.loc['depth', 'params'] # in mm\n",
    "    sd.oct_flatness = df_oct.loc['flatness', 'params']\n",
    "    # sd.oct_volume = df_oct.loc['volume', 'params']\n",
    "\n",
    "    # record cone density and fitted parameters (populated by `src/cell/analysis/density_analysis_pipeline_manager.py`)\n",
    "    df_density = pd.read_csv(path / Parser.get_density_analysis_dir() / 'densities.csv', sep=';', index_col=0)\n",
    "    df_raw_density_x = pd.read_csv(path / Parser.get_density_analysis_dir() / 'densities_raw_x.csv', sep=';', index_col=0)\n",
    "    df_raw_density_y = pd.read_csv(path / Parser.get_density_analysis_dir() / 'densities_raw_y.csv', sep=';', index_col=0)\n",
    "    \n",
    "    sd.width_nas = df_density['width_nasal'].iloc[0]\n",
    "    sd.width_tem = df_density['width_temporal'].iloc[0]\n",
    "    sd.width_inf = df_density['width_inferior'].iloc[0]\n",
    "    sd.width_sup = df_density['width_superior'].iloc[0]\n",
    "    sd.max_slope_nas = df_density['max_slope_nasal'].iloc[0]\n",
    "    sd.max_slope_tem = df_density['max_slope_temporal'].iloc[0]\n",
    "    sd.max_slope_inf = df_density['max_slope_inferior'].iloc[0]\n",
    "    sd.max_slope_sup = df_density['max_slope_superior'].iloc[0]\n",
    "    sd.density_X = df_density['dens_smthd_X']\n",
    "    sd.density_Y = df_density['dens_smthd_Y']\n",
    "    sd.density_fit_X = df_density['dens_fit_X']\n",
    "    sd.density_fit_Y = df_density['dens_fit_Y']\n",
    "    \n",
    "    sd.eccs = df_density.index.to_numpy()\n",
    "\n",
    "    # record layer thicknesses (populated by `src/save_layer_features.ipynb`)\n",
    "    df_thick = pd.read_csv(path / Parser.get_density_analysis_dir() / 'results.csv', sep=',', index_col=0, skiprows=1).query('-10 <= index <= 10')\n",
    "    sd.cvi_X = df_thick['CVI_X']\n",
    "    sd.cvi_Y = df_thick['CVI_Y']\n",
    "    sd.gcl_ipl_X = df_thick['GCL+IPL_X']\n",
    "    sd.gcl_ipl_Y = df_thick['GCL+IPL_Y']\n",
    "    sd.onl_X = df_thick['ONL_X']\n",
    "    sd.onl_Y = df_thick['ONL_Y']\n",
    "    sd.inl_opl_X = df_thick['INL+OPL_X']\n",
    "    sd.inl_opl_Y = df_thick['INL+OPL_Y']\n",
    "    sd.rnfl_X = df_thick['RNFL_X']\n",
    "    sd.rnfl_Y = df_thick['RNFL_Y']\n",
    "    sd.chrd_X = df_thick['Choroid_X']\n",
    "    sd.chrd_Y = df_thick['Choroid_Y']\n",
    "    sd.pr_rpe_X = df_thick['PhotoR+RPE_X']\n",
    "    sd.pr_rpe_Y = df_thick['PhotoR+RPE_Y']\n",
    "    sd.os_X = df_thick['OS_X']\n",
    "    sd.os_Y = df_thick['OS_Y']\n",
    "\n",
    "    subjects_data.append(sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b37ce",
   "metadata": {},
   "source": [
    "#### Populating Additional fields based on the previously gathered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_cones(ecc: np.ndarray, dens_X: pd.Series, dens_Y: pd.Series, radius: float, smoothen: bool = True) -> float:\n",
    "    \n",
    "    '''\n",
    "    Given the cone density profiles along the X and Y axes, compute the total number of cones within a disk of radius `radius` (in degree) centered at the fovea by linearly interpolating (radially) the density profiles and integrating over the disk.\n",
    "    '''\n",
    "    smthd_x = gaussian_filter_nan(dens_X, sigma=4) if smoothen else dens_X.to_numpy()\n",
    "    smthd_y = gaussian_filter_nan(dens_Y, sigma=4) if smoothen else dens_Y.to_numpy()\n",
    "   \n",
    "    x_amax = np.nanargmax(smthd_x)\n",
    "    p = np.polyfit(ecc[x_amax-2:x_amax+3], smthd_x[x_amax-2:x_amax+3], 2)\n",
    "    x_amax = -p[1] / (2 * p[0])\n",
    "\n",
    "    y_amax = np.nanargmax(smthd_y)\n",
    "    p = np.polyfit(ecc[y_amax-2:y_amax+3], smthd_y[y_amax-2:y_amax+3], 2)\n",
    "    y_amax = -p[1] / (2 * p[0])\n",
    "\n",
    "    R = np.linspace(0.0001, radius, 500) # radius in degrees\n",
    "    disk = np.r_[\n",
    "        np.interp(x_amax + R, ecc, smthd_x),\n",
    "        np.interp(x_amax - R, ecc, smthd_x),\n",
    "        np.interp(y_amax + R, ecc, smthd_y),\n",
    "        np.interp(y_amax - R, ecc, smthd_y)\n",
    "    ]\n",
    "    \n",
    "    norm_coef = MM_PER_DEGREE**2 * 2 * np.pi\n",
    "    # integrate cone density over disk to get total nb of cones\n",
    "    return norm_coef * np.trapz(np.nanmean(disk, axis=0) * R, R)\n",
    "\n",
    "RADIUS = 3.33 # degree\n",
    "for sd in subjects_data:\n",
    "    sd.nb_cones = get_nb_cones(sd.eccs, sd.density_X, sd.density_Y, radius = RADIUS)\n",
    "    sd.nb_cones_fit = get_nb_cones(sd.eccs, sd.density_fit_X, sd.density_fit_Y, radius = RADIUS, smoothen=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff70b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "def adjust_flat(gcl_data: np.ndarray, peak_left: int, peak_right: int) -> np.ndarray:\n",
    "    slope = (gcl_data[peak_right] - gcl_data[peak_left]) / (peak_right - peak_left)\n",
    "    transformed_gcl = gcl_data - slope * (np.arange(len(gcl_data)) - peak_left)\n",
    "    return transformed_gcl\n",
    "\n",
    "def get_gcl_width(gcl: pd.Series) -> Tuple[float, float]:\n",
    "    '''\n",
    "    Given the GCL+IPL thickness profile, compute the width of the pit as well as the minimum thickness of the layer. Here, the width of the pit is defined as the distance between the two points where the thickness is 20% of the depth of the pit. The depth of the pit is defined as the difference between the thickness surrounding the pit and the thickness at the pit's bottom.\n",
    "    '''\n",
    "    # name = gcl.name\n",
    "    gcl_to_plot = gcl.copy()\n",
    "    eccs = gcl[np.abs(gcl.index) <= 6].index.to_numpy()\n",
    "    gcl = gcl.interpolate(method='polynomial', order=1)[eccs].to_numpy()\n",
    "    # plt.plot(eccs, gcl, label=name)\n",
    "    smooth_param = 3\n",
    "    peak_left = peak_right = []\n",
    "    while not (len(peak_left) >= 1 and len(peak_right) >= 1) and smooth_param < 10:\n",
    "        smoothed_gcl = gaussian_filter_nan(gcl, smooth_param)\n",
    "        peaks = find_peaks(smoothed_gcl)[0]\n",
    "        peak_left  = [peak for peak in peaks if peak < len(smoothed_gcl) / 3]\n",
    "        peak_right = [peak for peak in peaks if peak > 2 * len(smoothed_gcl) / 3]\n",
    "        smooth_param += 1\n",
    "    assert len(peak_left) >= 1 and len(peak_right) >= 1, f'No peaks found for {gcl.name}'\n",
    "    peak_left = round(np.mean(peak_left))   \n",
    "    peak_right = round(np.mean(peak_right))\n",
    "    adjusted_gcl = adjust_flat(gcl, peak_left, peak_right)\n",
    "    smoothed_aj_gcl = gaussian_filter_nan(adjusted_gcl, 2)\n",
    "\n",
    "    y_min = np.nanmin(smoothed_aj_gcl[peak_left:peak_right])\n",
    "    y_target = y_min + (smoothed_aj_gcl[peak_left] - y_min) / 5\n",
    "    intercepts = np.where(np.diff(np.sign(smoothed_aj_gcl - y_target)))[0]\n",
    "    leftmost = eccs[intercepts[0]]\n",
    "    rightmost = eccs[intercepts[-1]+1]\n",
    "    width_pit_gcl = rightmost - leftmost\n",
    "\n",
    "    indicies = np.argpartition(gcl, 10)[:10]\n",
    "    p = np.polyfit(eccs[indicies], gcl[indicies], 2)\n",
    "    if p[0] == 0:\n",
    "    #     # gcl_to_plot.plot()\n",
    "    #     plt.plot(eccs, gcl, label='gcl')\n",
    "        plt.plot(np.sort(eccs[indicies]), np.polyval(p, np.sort(eccs[indicies])), '--')\n",
    "    min_thickness_gcl = np.polyval(p, -p[1] / (2 * p[0]))\n",
    "    return width_pit_gcl, min_thickness_gcl\n",
    "\n",
    "for sd in subjects_data:\n",
    "    width_gcl_x, min_thick_x = get_gcl_width(sd.gcl_ipl_X)\n",
    "    width_gcl_y, min_thick_y = get_gcl_width(sd.gcl_ipl_Y)\n",
    "    sd.width_gcl_X = width_gcl_x\n",
    "    sd.width_gcl_Y = width_gcl_y\n",
    "    sd.min_thick_gcl = min(min_thick_x, min_thick_y)\n",
    "    # print(f'{sd.name:>10}: {width_gcl_x:.2f}°, {depth_gcl_x:.4f}, {width_gcl_y:.2f}°, {depth_gcl_y:.4f}')\n",
    "    # plt.xlim(-6, 6)\n",
    "    # plt.legend()\n",
    "    # plt.title(sd.name)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "eccs = subjects_data[0].eccs\n",
    "layer_names = ['rnfl', 'gcl_ipl', 'inl_opl', 'onl', 'pr_rpe', 'os', 'chrd']\n",
    "names_r = {'rnfl': 'RNFL', 'gcl_ipl': 'GCL+IPL', 'inl_opl': 'INL+OPL', 'onl': 'ONL', 'pr_rpe': 'PhotoR+RPE', 'os': 'OS', 'chrd': 'Choroid', 'cones': 'Cone Density'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a23925",
   "metadata": {},
   "source": [
    "## General Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.shared.helpers.direction import Direction\n",
    "\n",
    "\n",
    "def preprocess_functional_feature(data: np.ndarray, standardization: str = 'inter') -> np.ndarray:\n",
    "    '''\n",
    "    Preprocess a functional feature (functional feature such as cone density or layer thickness, for which the feature is a function of eccentricity) by (Z-)standardizing it.\n",
    "    Given `data` matrix should have shape (n_subjects, n_eccentricities).\n",
    "    \n",
    "    - For an intra-indivual analysis, use `standardization='intra'` to standardize within subjects (i.e. within each row). This removes inter-subject variability.\n",
    "    - For an inter-individual analysis, use `standardization='inter'` to standardize across subjects, eccentricity-wise (i.e. within each column). Removes eccentricity-level variability, focuses on between-patient trends\n",
    "    '''\n",
    "    if standardization == 'inter':\n",
    "        mean = np.mean(data, axis=0, keepdims=True)\n",
    "        std = np.std(data, axis=0, keepdims=True)\n",
    "        return (data - mean) / std\n",
    "    if standardization == 'intra':\n",
    "        mean = np.nanmean(data, axis=1, keepdims=True)\n",
    "        std = np.nanstd(data, axis=1, keepdims=True)\n",
    "        return (data - mean) / std\n",
    "    return data\n",
    "\n",
    "def preprocess_functional_data(direction: Direction, standardization: str = 'inter', toLog : bool = True) -> Dict[str, np.ndarray]:\n",
    "    '''\n",
    "    Preprocess functional data (e.g. cone density, layer thicknesses) for a given direction (X or Y) by (Z-)standardizing it.\n",
    "    '''\n",
    "\n",
    "    layer_fds = {\n",
    "        layer: preprocess_functional_feature(\n",
    "            np.array([getattr(s, f'{layer}_{direction.value}') for s in subjects_data]), standardization\n",
    "        )\n",
    "        for layer in layer_names\n",
    "    }\n",
    "    if toLog:\n",
    "        cone_density_fd = preprocess_functional_feature(\n",
    "            np.array([np.log(getattr(s, f'density_fit_{direction.value}')) for s in subjects_data]), standardization\n",
    "        )\n",
    "    else:\n",
    "        cone_density_fd = preprocess_functional_feature(\n",
    "            np.array([(getattr(s, f'density_fit_{direction.value}')) for s in subjects_data]), standardization\n",
    "        )\n",
    "\n",
    "    cone_density_nonfit = preprocess_functional_feature(\n",
    "            np.array([(getattr(s, f'density_{direction.value}')) for s in subjects_data]), standardization\n",
    "        )\n",
    "    # return {'cones': cone_density_fd, \"nonfit\": cone_density_nonfit, **layer_fds}\n",
    "    return {'cones': cone_density_fd, \"nonfit\": cone_density_nonfit, **layer_fds}\n",
    "\n",
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def kendall_pval(x,y):\n",
    "    return kendalltau(x,y)[1]\n",
    "\n",
    "def pearsonr_pval(x,y):\n",
    "    return pearsonr(x,y)[1]\n",
    "\n",
    "def spearmanr_pval(x,y):\n",
    "    return spearmanr(x,y, nan_policy = \"omit\")[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e2dde",
   "metadata": {},
   "source": [
    "## Density Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cone_density_fd_X = preprocess_functional_data(Direction.X, standardization='none', toLog = False)['cones']\n",
    "cone_density_fd_Y = preprocess_functional_data(Direction.Y, standardization='none', toLog = False)['cones']\n",
    "\n",
    "eccs_in_MM = eccs * MM_PER_DEGREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e56d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(eccs_in_MM)\n",
    "\n",
    "# Normalize by the max value for each of the 201 parameters (column-wise normalization)\n",
    "normalized_data_x = cone_density_fd_X/ np.max(cone_density_fd_X, axis=1, keepdims=True)\n",
    "normalized_data_y = cone_density_fd_Y/ np.max(cone_density_fd_Y, axis=1, keepdims=True)\n",
    "\n",
    "# Compute the mean across the 33 patients\n",
    "mean_data_x= np.mean(normalized_data_x, axis=0)\n",
    "mean_data_y= np.mean(normalized_data_y, axis=0)\n",
    "\n",
    "mean_data_x_nonnorm = np.mean(cone_density_fd_X, axis=0)\n",
    "mean_data_y_nonnorm = np.mean(cone_density_fd_Y, axis=0)\n",
    "\n",
    "# After your existing code for knee point detection\n",
    "# plot_flipped_comparison(eccs_in_MM, cone_density_fd_X, cone_density_fd_Y)\n",
    "\n",
    "# You might also want to see the same plot for normalized data\n",
    "# plot_flipped_comparison(eccs_in_MM, normalized_data_x, normalized_data_y)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotx\n",
    "from matplotlib.colors import to_rgb\n",
    "\n",
    "# Get the background color of the current style\n",
    "with plt.style.context(matplotx.styles.pacoty):\n",
    "    fig, ax = plt.subplots()\n",
    "    bg_color = ax.get_facecolor()  # Get the background color\n",
    "    plt.close(fig)  # Close the figure we just created\n",
    "\n",
    "# Convert to RGB if it's not already\n",
    "bg_rgb = to_rgb(bg_color) if isinstance(bg_color, str) else bg_color[:3]\n",
    "\n",
    "# Define breakpoints (positive and negative)\n",
    "breakpoints = [0.175, 0.25, 0.75, 1.25, 2.75]\n",
    "# all_breakpoints = sorted([-x for x in breakpoints] + breakpoints)\n",
    "\n",
    "# Create a gradient from white to the background color\n",
    "num_steps = len(breakpoints)\n",
    "colors = [\n",
    "    (1, 1, 1),  # white\n",
    "    tuple(0.9 + 0.1 * x for x in bg_rgb),  # 80% white + 20% bg\n",
    "    tuple(0.7 + 0.3 * x for x in bg_rgb),  # 60% white + 40% bg\n",
    "    tuple(0.5 + 0.5 * x for x in bg_rgb),  # 40% white + 60% bg\n",
    "    tuple(0.3 + 0.7 * x for x in bg_rgb),  # 20% white + 80% bg\n",
    "    tuple(x for x in bg_rgb)  # full background color\n",
    "]\n",
    "\n",
    "# Define retinal regions and their boundaries\n",
    "retinal_regions = [\n",
    "    {'name': 'Perifovea', 'start': 1.25, 'end': np.inf, 'color': colors[5]},\n",
    "    {'name': 'Parafovea', 'start': 0.75, 'end': 1.25, 'color': colors[3]},\n",
    "    {'name': 'Fovea', 'start': 0.175, 'end': 0.75, 'color': colors[2]},\n",
    "    # {'name': 'Faz', 'start': 0.175, 'end': 0.25, 'color': colors[1]},\n",
    "    {'name': 'Foveola', 'start': -0.175, 'end': 0.175, 'color': colors[0]},\n",
    "    # {'name': 'Faz', 'start': -0.25, 'end': -0.175, 'color': colors[1]},\n",
    "    {'name': 'Fovea', 'start': -0.75, 'end': -0.175, 'color': colors[2]},\n",
    "    {'name': 'Parafovea', 'start': -1.25, 'end': -0.75, 'color': colors[3]},\n",
    "    {'name': 'Perifovea', 'start': -np.inf, 'end': -1.25, 'color': colors[5]}\n",
    "]\n",
    "\n",
    "# Create regions with corresponding colors\n",
    "regions_with_colors = [(r['start'], r['end'], r['color']) for r in retinal_regions]\n",
    "\n",
    "def plot_with_regions(data_x, data_y):\n",
    "    # with plt.style.context(matplotx.styles.pacoty):\n",
    "    # First plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Add gradient background\n",
    "    for left, right, color in regions_with_colors:\n",
    "        plt.axvspan(left, right, facecolor=color, alpha=1, zorder=0)\n",
    "    \n",
    "    # Add vertical lines and labels for retinal regions\n",
    "    for region in retinal_regions:\n",
    "        if not np.isinf(region['start']):\n",
    "            plt.axvline(region['start'], color='lightgrey', linestyle='--', alpha=1, linewidth=0.5, zorder = 1)\n",
    "        if not np.isinf(region['end']):\n",
    "            plt.axvline(region['end'], color='lightgrey', linestyle='--', alpha=1, linewidth=0.5, zorder = 1)\n",
    "        \n",
    "        # Calculate position for label (middle of region)\n",
    "        if np.isinf(region['start']):\n",
    "            x_pos = region['end'] - 0.3\n",
    "        elif np.isinf(region['end']):\n",
    "            x_pos = region['start'] + 0.3\n",
    "        else:\n",
    "            x_pos = (region['start'] + region['end']) / 2\n",
    "        # if region['name'] == 'Foveola':\n",
    "        #     plt.text(x_pos, 1.05*plt.ylim()[1], region['name'], \n",
    "        #             rotation=0, verticalalignment='top', \n",
    "        #             horizontalalignment='center', fontsize=10,\n",
    "        #             bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=0))\n",
    "        # else:\n",
    "        #     plt.text(x_pos, 1.05*plt.ylim()[1], region['name'], \n",
    "        #         rotation=90, verticalalignment='top', \n",
    "        #         horizontalalignment='center', fontsize=10,\n",
    "        #         bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=0))\n",
    "        \n",
    "    # Plot data\n",
    "    for i in range(33):\n",
    "        plt.plot(eccs_in_MM, data_x[i, :], color='blue', alpha=0.2, zorder = 2)\n",
    "        plt.plot(eccs_in_MM, data_y[i, :], color='blue', alpha=0.2, zorder = 2)\n",
    "\n",
    "    mean_data_x = np.mean(data_x, axis=0)\n",
    "    mean_data_y = np.mean(data_y, axis=0)\n",
    "\n",
    "\n",
    "    # computes complete mean of all the data\n",
    "    mean_data = np.mean(np.concatenate([data_x, data_y], axis=0), axis=0)\n",
    "\n",
    "\n",
    "    plt.plot(eccs_in_MM, mean_data, color='red')\n",
    "    # plt.plot(eccs_in_MM, mean_data_y, color='green')\n",
    "    \n",
    "    plt.xlabel(\"Eccentricity (mm)\")\n",
    "    plt.ylabel(\"Normalized Value\")\n",
    "    plt.title(\"Normalized Data Across Patients with Mean Line\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "plot_with_regions(cone_density_fd_X, cone_density_fd_Y)\n",
    "# Plotting the cone density data for each subject\n",
    "plot_with_regions(normalized_data_x, normalized_data_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283dfd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a scatter plot between the maximum of the cone density and the cone nb\n",
    "# Function to create scatter plots comparing max/min cone density with total number of cones\n",
    "\n",
    "\n",
    "\n",
    "def plot_max_vs_total_cones(subjects_data, title_prefix=\"Maximum\"):\n",
    "    \"\"\"\n",
    "    Creates a scatter plot comparing maximum (or minimum) cone density with total number of cones\n",
    "    \n",
    "    Parameters:\n",
    "    subjects_data (list): List of SubjectData objects\n",
    "    title_prefix (str): Prefix for the plot title, either \"Maximum\" or \"Minimum\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract data based on whether we're looking at maximum or minimum\n",
    "    if title_prefix == \"Maximum\":\n",
    "        density_values = [np.nanmax(sd.density_fit_X) for sd in subjects_data]\n",
    "    else:  # Minimum\n",
    "        density_values = [np.nanmin(sd.density_fit_X) for sd in subjects_data]\n",
    "    \n",
    "    nb_cones = [sd.nb_cones_fit for sd in subjects_data]\n",
    "    \n",
    "    # Create scatter plot\n",
    "    with plt.style.context(matplotx.styles.pacoty):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Plot scatter points\n",
    "        plt.scatter(density_values, nb_cones, s=80, alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        # Add linear regression line\n",
    "        slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(density_values, nb_cones)\n",
    "        x_line = np.linspace(min(density_values), max(density_values), 100)\n",
    "        y_line = slope * x_line + intercept\n",
    "        plt.plot(x_line, y_line, 'r-', alpha=0.8, \n",
    "                 label=f'y = {slope:.2f}x + {intercept:.2f}\\nR² = {r_value**2:.3f}, p = {p_value:.4f}')\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.xlabel(f\"{title_prefix} Cone Density (cones/mm²)\", fontsize=12)\n",
    "        plt.ylabel(\"Total Number of Cones\", fontsize=12)\n",
    "        plt.title(f\"{title_prefix} Cone Density vs Total Number of Cones\", fontsize=14)\n",
    "        plt.grid(False)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Plot maximum cone density vs total cones\n",
    "plot_max_vs_total_cones(subjects_data, \"Maximum\")\n",
    "\n",
    "# Plot minimum cone density vs total cones\n",
    "plot_max_vs_total_cones(subjects_data, \"Minimum\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9c9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "radii = [0.150, 0.300, 1.2]\n",
    "indices = []\n",
    "ratio = []\n",
    "\n",
    "# mean_data_x = cone_density_fd_X[0]\n",
    "# mean_data_y = cone_density_fd_Y[0]\n",
    "for i, radius in enumerate (radii):\n",
    "    index_pos = np.searchsorted(eccs_in_MM, radius)\n",
    "    index_neg = np.searchsorted(eccs_in_MM, -radius)\n",
    "    ratio_values =[mean_data_x[index_pos], mean_data_y[index_pos],\n",
    "                    mean_data_x[index_neg], mean_data_y[index_neg]]\n",
    "    print(ratio_values)\n",
    "    ratio.append( np.mean(ratio_values))\n",
    "    print (f\"Ratio at {radii[i]} mm, for eccentricity of radius {eccs_in_MM[index_pos]} mm is {ratio[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e6448d",
   "metadata": {},
   "source": [
    "### Comparison with Zhang's 2015 Integration Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def integrate_cone_density_circle(eccs_in_MM: np.array,\n",
    "                                  cone_density_fd_X: np.array,\n",
    "                                  cone_density_fd_Y: np.array,\n",
    "                                  radius: float = 1.0,\n",
    "                                  num_r: int = 400,\n",
    "                                  num_theta: int = 400,\n",
    "                                  exclude_center: bool = False):\n",
    "    \"\"\"\n",
    "    Integrates cone density over a circular region by interpolating between the horizontal (X)\n",
    "    and vertical (Y) density measurements for each subject.\n",
    "    \n",
    "    Also computes:\n",
    "      - The total integrated density over the circle (a single value per subject).\n",
    "      - The maximum density along the horizontal and vertical meridians.\n",
    "      - The cumulative integrated density as a function of radius (in mm).\n",
    "    \n",
    "    Parameters:\n",
    "        eccs_in_MM (np.array): 1D array of eccentricities (radial positions in MM).\n",
    "        cone_density_fd_X (np.array): 2D array of cone densities along the horizontal meridian.\n",
    "                                      Shape: (n_subjects, len(eccs_in_MM)).\n",
    "        cone_density_fd_Y (np.array): 2D array of cone densities along the vertical meridian.\n",
    "                                      Shape: (n_subjects, len(eccs_in_MM)).\n",
    "        radius (float): Maximum radius (in MM) for the circular integration.\n",
    "        num_r (int): Number of radial grid points.\n",
    "        num_theta (int): Number of angular grid points.\n",
    "        exclude_center (bool): If True, excludes the central region (sets r_min to 0.3 MM).\n",
    "        \n",
    "    Returns:\n",
    "        mean_int (float): Mean integrated density across subjects.\n",
    "        std_int (float): Standard deviation of the integrated densities.\n",
    "        min_int (float): Minimum integrated density.\n",
    "        max_int (float): Maximum integrated density.\n",
    "        cov_int (float): Coefficient of Variation (std/mean * 100).\n",
    "        int_results (np.array): Array of integrated density values (one per subject).\n",
    "        max_x_results (np.array): Array of maximum densities along the X meridian per subject.\n",
    "        max_y_results (np.array): Array of maximum densities along the Y meridian per subject.\n",
    "        r_grid (np.array): The radial grid used for integration.\n",
    "        cumulative_integrations (np.array): 2D array (n_subjects x num_r) of cumulative integrated\n",
    "                                             density as a function of radius.\n",
    "    \"\"\"\n",
    "    # Set the lower bound for integration in r\n",
    "    r_min = 0.3 if exclude_center else 0.0\n",
    "    \n",
    "    # Create the polar grid for integration:\n",
    "    r = np.linspace(r_min, radius, num_r)\n",
    "    theta = np.linspace(0, 2 * np.pi, num_theta)\n",
    "    \n",
    "    int_results = []\n",
    "    max_x_results = []\n",
    "    max_y_results = []\n",
    "    cumulative_integrations_list = []\n",
    "    n_subjects = cone_density_fd_X.shape[0]\n",
    "    \n",
    "    # Loop over each subject (each row in the data)\n",
    "    for i in range(n_subjects):\n",
    "        # Create interpolation functions for the horizontal and vertical densities\n",
    "        f_x = interpolate.interp1d(eccs_in_MM, cone_density_fd_X[i, :],\n",
    "                                   bounds_error=False, fill_value=\"extrapolate\")\n",
    "        f_y = interpolate.interp1d(eccs_in_MM, cone_density_fd_Y[i, :],\n",
    "                                   bounds_error=False, fill_value=\"extrapolate\")\n",
    "        \n",
    "        # Evaluate the interpolated densities on the radial grid\n",
    "        density_x = f_x(r)  # shape: (num_r,)\n",
    "        density_y = f_y(r)  # shape: (num_r,)\n",
    "        \n",
    "        # Compute maximum density along each meridian within the integration region\n",
    "        max_x = np.max(density_x)\n",
    "        max_y = np.max(density_y)\n",
    "        max_x_results.append(max_x)\n",
    "        max_y_results.append(max_y)\n",
    "        \n",
    "        # Compute the density field on the polar grid.\n",
    "        # For each (r, theta) point, combine the two measurements as:\n",
    "        # density(r, theta) = density_x(r)*cos²(theta) + density_y(r)*sin²(theta)\n",
    "        density_field = (density_x[:, None] * np.cos(theta)**2 +\n",
    "                         density_y[:, None] * np.sin(theta)**2)\n",
    "        \n",
    "        # Multiply by the Jacobian (r) to account for the area element in polar coordinates\n",
    "        density_field_weighted = density_field * r[:, None]\n",
    "        \n",
    "        # Integrate first over theta (axis=1) then over r using Simpson’s rule.\n",
    "        integral_theta = integrate.simpson(density_field_weighted, x=theta, axis=1)\n",
    "        integrated_density = integrate.simpson(integral_theta, x=r)\n",
    "        int_results.append(integrated_density)\n",
    "        \n",
    "        # Compute cumulative integration as a function of r using cumulative trapezoidal rule.\n",
    "        # This gives the integrated cone density from r_min up to each r value.\n",
    "        cumulative_integration = integrate.cumulative_trapezoid(integral_theta, r, initial=0)\n",
    "        cumulative_integrations_list.append(cumulative_integration)\n",
    "    \n",
    "    # Convert lists to NumPy arrays for further statistics\n",
    "    int_results = np.array(int_results)\n",
    "    max_x_results = np.array(max_x_results)\n",
    "    max_y_results = np.array(max_y_results)\n",
    "    cumulative_integrations = np.array(cumulative_integrations_list)\n",
    "    \n",
    "    # Compute the integration metrics for the total integrated density\n",
    "    mean_int = np.mean(int_results)\n",
    "    std_int = np.std(int_results)\n",
    "    min_int = np.min(int_results)\n",
    "    max_int = np.max(int_results)\n",
    "    cov_int = (std_int / mean_int * 100) if mean_int != 0 else np.nan\n",
    "    \n",
    "    return (mean_int, std_int, min_int, max_int, cov_int, int_results,\n",
    "            max_x_results, max_y_results, r, cumulative_integrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "(mean_int, std_int, min_int, max_int, cov_int, int_results, max_x_results, \n",
    " max_y_results, r_grid, cumulative_integrations) = integrate_cone_density_circle(\n",
    "    eccs_in_MM, cone_density_fd_X, cone_density_fd_Y, radius=1.0, exclude_center=False)\n",
    "\n",
    "print(f\"Mean integrated cone density: {mean_int}\")\n",
    "print(f\"Standard deviation: {std_int}\")\n",
    "print(f\"Minimum integrated density: {min_int}\")\n",
    "print(f\"Maximum integrated density: {max_int}\")\n",
    "print(f\"Coefficient of Variation (COV): {cov_int:.2f}%\\n\")\n",
    "\n",
    "# Show results for each subject:\n",
    "print(\"Subject\\tIntegrated Density\\tMax (X direction)\\tMax (Y direction)\")\n",
    "for i in range(len(int_results)):\n",
    "    print(f\"{i+1}\\t{int_results[i]:.3f}\\t\\t\\t{max_x_results[i]:.3f}\\t\\t\\t{max_y_results[i]:.3f}\")\n",
    "\n",
    "# Plot histogram of integrated densities for all subjects\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(int_results, bins=100, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel(\"Integrated Cone Density\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.grid(False)\n",
    "plt.title(\"Histogram of Integrated Cone Density Values\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Compute mean and standard deviation across subjects for each radius value\n",
    "mean_cum = np.mean(cumulative_integrations, axis=0)\n",
    "std_cum = np.std(cumulative_integrations, axis=0)\n",
    "\n",
    "# Define breakpoints for the gradient\n",
    "breakpoints = [0.175, 0.25, 0.75, 1.25, 2.75]\n",
    "all_breakpoints = sorted([-x for x in breakpoints] + breakpoints)\n",
    "\n",
    "# with plt.style.context(matplotx.styles.pacoty):\n",
    "fig, ax = plt.subplots()\n",
    "bg_color = ax.get_facecolor()  # Get the background color\n",
    "plt.close(fig)  # Close the figure we just created\n",
    "\n",
    "# Convert to RGB if it's not already\n",
    "bg_rgb = to_rgb(bg_color) if isinstance(bg_color, str) else bg_color[:3]\n",
    "\n",
    "# Create a smooth gradient from white to the background color\n",
    "colors = [\n",
    "    (1, 1, 1),  # white\n",
    "    tuple(0.9 + 0.1 * x for x in bg_rgb),  # 80% white + 20% bg\n",
    "    tuple(0.7 + 0.3 * x for x in bg_rgb),  # 60% white + 40% bg\n",
    "    tuple(0.5 + 0.5 * x for x in bg_rgb),  # 40% white + 60% bg\n",
    "    tuple(0.3 + 0.7 * x for x in bg_rgb),  # 20% white + 80% bg\n",
    "    tuple(x for x in bg_rgb)  # full background color\n",
    "]\n",
    "\n",
    "# Define retinal regions (for background gradient)\n",
    "retinal_regions = [\n",
    "\n",
    "    {'name': 'Parafovea', 'start': 0.75, 'end': 1, 'color': colors[5]},\n",
    "    {'name': 'Fovea', 'start': 0.175, 'end': 0.75, 'color': colors[4]},\n",
    "    {'name': 'Faz', 'start': 0.175, 'end': 0.25, 'color': colors[3]},\n",
    "    {'name': 'Foveola', 'start': 0, 'end': 0.175, 'color': colors[0]},\n",
    "    \n",
    "]\n",
    "\n",
    "# Create regions with corresponding colors\n",
    "regions_with_colors = [(r['start'], r['end'], r['color']) for r in retinal_regions]\n",
    "\n",
    "# Create the plot with the gradient background\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Add gradient background (use axvspan for each region)\n",
    "for left, right, color in regions_with_colors:\n",
    "    plt.axvspan(left, right, facecolor=color, alpha=1.0, zorder=0)\n",
    "\n",
    "# Plot the mean and shaded area for standard deviation\n",
    "plt.plot(r_grid, mean_cum, label=\"Mean Cumulative Cone Density\", color='blue')\n",
    "plt.fill_between(r_grid, mean_cum - std_cum, mean_cum + std_cum, alpha=0.1, label=\"±1 Std Dev\", color='blue')\n",
    "\n",
    "# Add vertical lines and labels for retinal regions\n",
    "for region in retinal_regions:\n",
    "    if not np.isinf(region['start']):\n",
    "        plt.axvline(region['start'], color='black', linestyle='--', alpha=0.3, linewidth=0.5)\n",
    "    if not np.isinf(region['end']):\n",
    "        plt.axvline(region['end'], color='black', linestyle='--', alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "\n",
    "\n",
    "# Final labels and title\n",
    "plt.xlabel(\"Eccentricity (mm)\")\n",
    "plt.ylabel(\"Cumulative Integrated Cone Density\")\n",
    "plt.title(\"Cumulative Cone Density vs. Eccentricity\")\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1dbc5e",
   "metadata": {},
   "source": [
    "## Intra/Inter- Individual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8987a458",
   "metadata": {},
   "source": [
    "### Functions for intra-/inter-individual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4580bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import warnings\n",
    "from scipy.stats import spearmanr, pearsonr, binomtest\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def mixedlm(cd: np.ndarray, lt: np.ndarray, pids: np.ndarray, eccs: np.ndarray, standardization: str = 'inter') -> Tuple[float, float]:\n",
    "    data = pd.DataFrame({'Subject': pids, 'Eccentricity': eccs, 'LayerThickness': lt, 'ConeDensity': cd})\n",
    "    if standardization == 'intra':\n",
    "        model = smf.mixedlm(\"LayerThickness ~ ConeDensity\", data, groups=\"Subject\", re_formula=\"~Eccentricity\")\n",
    "    else:\n",
    "        model = smf.mixedlm(\"LayerThickness ~ ConeDensity\", data, groups=\"Subject\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        result = model.fit()\n",
    "    return result.fe_params['ConeDensity'], result.pvalues['ConeDensity']\n",
    "\n",
    "def plot_slice_correlation(layer_name: str, degree: float | Iterable | None, direction: Direction, eccs: np.ndarray, comp_to = 'cones', restrict_to_os = False, standardization: str = 'inter'):\n",
    "    \n",
    "    fd = preprocess_functional_data(direction, standardization)\n",
    "\n",
    "    n_subject = fd['cones'].shape[0]\n",
    "\n",
    "    if degree is None:\n",
    "        degree = eccs\n",
    "        ecc_str = 'across all eccs'\n",
    "    elif isinstance(degree, Iterable):\n",
    "        degree = np.round(np.array(degree), 1)\n",
    "        ecc_str = f'on {np.min(degree)}° to {np.max(degree)}°'\n",
    "    else:\n",
    "        degree = np.round(float(degree), 1)\n",
    "        ecc_str = f'at {degree}°'\n",
    "\n",
    "    indices = np.searchsorted(eccs, degree)\n",
    "\n",
    "    cd = fd[comp_to][:, indices].flatten()\n",
    "    lt = fd[layer_name][:, indices].flatten()\n",
    "    pids = np.repeat(np.arange(n_subject), len(indices))\n",
    "    eccentricities = np.tile(eccs[indices], n_subject)\n",
    "    if restrict_to_os:\n",
    "        os_mask = ~np.isnan(fd['os'][:, indices].flatten())\n",
    "        cd = cd[os_mask]\n",
    "        lt = lt[os_mask]\n",
    "        pids = pids[os_mask]\n",
    "        eccentricities = eccentricities[os_mask]\n",
    "    valid = ~np.isnan(cd) & ~np.isnan(lt)\n",
    "    if not valid.any():\n",
    "        print(f'No valid data for {layer_name} {ecc_str}.')\n",
    "        return\n",
    "    cd = cd[valid]\n",
    "    lt = lt[valid]\n",
    "    pids = pids[valid]\n",
    "    eccentricities = eccentricities[valid]\n",
    "\n",
    "    LL_UR = (lt > 0) != (cd > 0)\n",
    "    UL_LR = (lt > 0) == (cd > 0)\n",
    "\n",
    "    spearman_corr = spearmanr(cd, lt)\n",
    "    pearson_corr = pearsonr(cd, lt)\n",
    "    if (perform_mlm := len(indices) > 1):\n",
    "        mixedlm_corr = mixedlm(cd, lt, pids, eccentricities)\n",
    "    binom_corr = binomtest(\n",
    "        LL_UR.sum() if pearson_corr.correlation < 0 else UL_LR.sum(),\n",
    "        LL_UR.sum() + UL_LR.sum(), \n",
    "        p=0.5, alternative='greater'\n",
    "    )\n",
    "    with(plt.style.context(matplotx.styles.pacoty)):\n",
    "        \n",
    "        plot_limit = max(3, 0.1 + np.ceil(np.max(np.abs([lt, cd])) * 10) / 10)\n",
    "        # plot_limit=3.2\n",
    "        # colors = iter(plt.get_cmap('Accent', 33)(np.arange(33)).tolist())\n",
    "        # for _cd, _lt in zip(cone_density_fd.data_matrix[:,:,0], layer_fds[layer_name].data_matrix[:, :, 0]):\n",
    "        #     plt.scatter(_cd, _lt, 2, color=next(colors), alpha=0.6)\n",
    "        plt.scatter(cd[LL_UR], lt[LL_UR], 5, color='blue', alpha=0.6, label=f'n = {LL_UR.sum()}')\n",
    "        plt.scatter(cd[UL_LR], lt[UL_LR], 5, color='red', alpha=0.6, label=f'n = {UL_LR.sum()}')\n",
    "\n",
    "        plt.axhline(0, color='black', linewidth=0.5)\n",
    "        plt.axvline(0, color='black', linewidth=0.5)\n",
    "        plt.fill_between([-plot_limit, 0], -plot_limit, 0, color='red', alpha=0.05)\n",
    "        plt.fill_between([0, plot_limit], 0, plot_limit, color='red', alpha=0.05)\n",
    "        plt.fill_between([-plot_limit, 0], 0, plot_limit, color='blue', alpha=0.05)\n",
    "        plt.fill_between([0, plot_limit], -plot_limit, 0, color='blue', alpha=0.05)\n",
    "\n",
    "        indices = np.argsort(cd)\n",
    "        x_p = np.linspace(-plot_limit, plot_limit, 100)\n",
    "        if perform_mlm:\n",
    "            p = np.polyfit(cd[indices], lt[indices], 1)\n",
    "            plt.plot(x_p, np.polyval(p, x_p), color='olive', label=f'fit: y = {p[0]:.4g}x')\n",
    "        else:\n",
    "            slope_mlm = mixedlm_corr[0]\n",
    "            plt.plot(x_p, slope_mlm * x_p, color='olive', label=f'MLM: y = {slope_mlm:.4g}x')\n",
    "        # plt.plot(x_p, np.sign(p[0]) * x_p, '--', color='olive', label=f'identity', alpha=0.6)\n",
    "\n",
    "        plt.ylim(-plot_limit, plot_limit)\n",
    "        plt.xlim(-plot_limit, plot_limit)\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        comp_to_str = 'Cone density' if comp_to=='cones' else f'{names_r[comp_to]} thickness'\n",
    "        plt.xlabel(f'{comp_to_str} (Z-Score)')\n",
    "        plt.ylabel(f'{names_r[layer_name]} thickness, (Z-Score)')\n",
    "        # plot legend on right side, out of the plot\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        # below the legend, add box of text displaying the statistics results\n",
    "        plt.text(1.02, 0.7, \n",
    "                f'Spearman: {spearman_corr[0]:.3f}, p={spearman_corr[1]:.2g}'\n",
    "                f'\\nPearson: {pearson_corr[0]:.3f}, p={pearson_corr[1]:.2g}'\n",
    "                f'\\nBinomial test: p={binom_corr.pvalue:.2g}'\n",
    "                f'\\nMixedLM: {mixedlm_corr[0]:.3f}, p={mixedlm_corr[1]:.2g}' if perform_mlm else '', \n",
    "                fontsize=12, ha='left', transform=plt.gca().transAxes)\n",
    "\n",
    "        plt.title(f'{names_r[layer_name]} thickness vs {comp_to_str} across subjects & {ecc_str}, {direction.value}-axis\\nStandardized {standardization}-individually')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601d008d",
   "metadata": {},
   "source": [
    "### Intra-Individual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc9835b",
   "metadata": {},
   "source": [
    "\n",
    "Have a look at the violin plots as well: `P:\\AOSLO\\_automation\\_PROCESSED\\Photoreceptors\\Healthy\\_Results\\all_stats_new\\spearman_correlation_for_*.png`.\n",
    "The following plots are just an other way to visualize the same thing since both are intra-individual analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['rnfl', 'pr_rpe', 'onl', 'gcl_ipl']#, 'cones']\n",
    "for i in range(len(features)):\n",
    "    # for j in range(i+1, len(features)):\n",
    "    layer, comp_to = features[i], 'cones'#features[j]\n",
    "    # for direction in Direction:\n",
    "    plot_slice_correlation(layer, None, Direction.X, eccs, comp_to=comp_to, standardization='intra')\n",
    "    \n",
    "# plot_slice_correlation('onl', None, Direction.X, eccs, comp_to='cones', restrict_to_os=False, standardization='intra')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c0d203",
   "metadata": {},
   "source": [
    "### Inter-individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672bea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['rnfl', 'pr_rpe', 'os', 'onl', 'cones']\n",
    "\n",
    "step = 0.1\n",
    "deg = np.arange(-1, 1+step, step)\n",
    "\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        layer, comp_to = features[i], features[j]\n",
    "        plot_slice_correlation(layer, deg, Direction.X, eccs, comp_to=comp_to, standardization='inter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613293e9",
   "metadata": {},
   "source": [
    "## ECC-wise analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97b6d1",
   "metadata": {},
   "source": [
    "### General function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293cb0d",
   "metadata": {},
   "source": [
    "#### Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78749ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def compute_correlations_eccwise(direction: Direction, eccs: np.ndarray, correlation_fun: Callable[[np.ndarray, np.ndarray], Tuple[float, float]] = lambda cd, lt: spearmanr(cd, lt, nan_policy='omit')\n",
    ") -> Dict[str, Tuple[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Compute inter-individual Spearman correlations between standardized cone density and each standardized layer, for each eccentricity.\n",
    "    In a nutshell, for each eccentricity, we correlate the deviations of the cone density and the layer thickness from their respective means,\n",
    "    to assess inter-individual relationships between cone density and retinal layer thicknesses.\n",
    "    \"\"\"\n",
    "    fd = preprocess_functional_data(direction, standardization='inter')\n",
    "    results = {}\n",
    "    for layer, layer_fd in fd.items():\n",
    "        if layer == 'cones':\n",
    "            continue\n",
    "        pointwise_corr = np.zeros(len(eccs))\n",
    "        pointwise_pvalues = np.zeros(len(eccs))\n",
    "        \n",
    "        # Iterate over each eccentricity to compute Spearman correlation and p-values\n",
    "        for i in range(len(eccs)):\n",
    "            cone_density_values = fd['cones'][:, i]\n",
    "            layer_values = layer_fd[:, i]\n",
    "            corr, pv = correlation_fun(cone_density_values, layer_values)\n",
    "            # corr, pv = spearmanr(cone_density_values, layer_values, nan_policy='omit')\n",
    "            pointwise_corr[i] = corr\n",
    "            pointwise_pvalues[i] = pv\n",
    "        results[layer] = (pointwise_corr, pointwise_pvalues)\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b0c294",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e68042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple, List\n",
    "from matplotlib.colors import to_rgb\n",
    "\n",
    "def plot_correlations_eccwise(results: Dict[str, Tuple[np.ndarray]], eccs: np.ndarray, direction: str, layers_to_plot: List[str] | None = None, abs_: bool = False, pv_threshold: float | None = None, corr_name: str = 'Spearman'):\n",
    "    \"\"\"\n",
    "    Plot the pointwise correlations of cone density with each layer, for each eccentricity.\n",
    "    \"\"\"\n",
    "    with plt.style.context(matplotx.styles.pacoty):\n",
    "        \n",
    "        f = lambda x: np.abs(x) if abs_ else x\n",
    "        colors = plt.get_cmap('Accent_r', len(results))  # Using the Accent_r colormap\n",
    "        plt.figure(figsize=(10, 6), dpi=300)\n",
    "        \n",
    "        # Create a gradient background based on the previous approach\n",
    "        breakpoints = [0.175, 0.25, 0.75, 1.25, 2.75]\n",
    "        all_breakpoints = sorted([-x for x in breakpoints] + breakpoints)\n",
    "\n",
    "        # Create a smooth gradient from white to the background color\n",
    "        with plt.style.context(matplotx.styles.pacoty):\n",
    "            fig, ax = plt.subplots()\n",
    "            bg_color = ax.get_facecolor()  # Get the background color\n",
    "            plt.close(fig)  # Close the figure we just created\n",
    "        bg_rgb = to_rgb(bg_color) if isinstance(bg_color, str) else bg_color[:3]\n",
    "        \n",
    "        # Create a gradient from white to the background color\n",
    "        gradient_colors = [\n",
    "            (1, 1, 1),  # white\n",
    "            tuple(0.8 + 0.2 * x for x in bg_rgb),  # 80% white + 20% bg\n",
    "            tuple(0.6 + 0.4 * x for x in bg_rgb),  # 60% white + 40% bg\n",
    "            tuple(0.4 + 0.6 * x for x in bg_rgb),  # 40% white + 60% bg\n",
    "            tuple(0.2 + 0.8 * x for x in bg_rgb),  # 20% white + 80% bg\n",
    "            bg_rgb  # full background color\n",
    "        ]\n",
    "        \n",
    "        # Define retinal regions (for background gradient)\n",
    "        retinal_regions = [\n",
    "            {'name': 'Perifovea', 'start': 1.25, 'end': np.inf, 'color': gradient_colors[5]},\n",
    "            {'name': 'Parafovea', 'start': 0.75, 'end': 1.25, 'color': gradient_colors[3]},\n",
    "            {'name': 'Fovea', 'start': 0.25, 'end': 0.75, 'color': gradient_colors[2]},\n",
    "            {'name': 'Faz', 'start': 0.175, 'end': 0.25, 'color': gradient_colors[1]},\n",
    "            {'name': 'Foveola', 'start': -0.175, 'end': 0.175, 'color': gradient_colors[0]},\n",
    "            {'name': 'Faz', 'start': -0.25, 'end': -0.175, 'color': gradient_colors[1]},\n",
    "            {'name': 'Fovea', 'start': -0.75, 'end': -0.25, 'color': gradient_colors[2]},\n",
    "            {'name': 'Parafovea', 'start': -1.25, 'end': -0.75, 'color': gradient_colors[3]},\n",
    "            {'name': 'Perifovea', 'start': -np.inf, 'end': -1.25, 'color': gradient_colors[5]}\n",
    "        ]\n",
    "        \n",
    "        regions_with_colors = [(r['start']/MM_PER_DEGREE, r['end']/MM_PER_DEGREE, r['color']) for r in retinal_regions]\n",
    "\n",
    "        # Plot the gradient background first\n",
    "        for left, right, color in regions_with_colors:\n",
    "            plt.axvspan(left, right, facecolor=color, alpha=1.0, zorder=0)\n",
    "        \n",
    "        def __plot_smooth_alpha(xs, ys, alphas, color, linewidth_fun):\n",
    "            assert len(xs) == len(ys) == len(alphas) == 2\n",
    "            n_steps = 2 + int(np.abs(np.diff(alphas))[0] / 0.01)\n",
    "            x = np.linspace(xs[0], xs[1], n_steps)\n",
    "            y = np.interp(x, xs, ys)\n",
    "            alpha = np.linspace(alphas[0], alphas[1], n_steps)\n",
    "            \n",
    "            for j in range(n_steps - 1):\n",
    "                plt.plot(\n",
    "                    x[j:j+2], \n",
    "                    y[j:j+2],\n",
    "                    color=color,\n",
    "                    alpha=alpha[j],\n",
    "                    linewidth=linewidth_fun(alpha[j]),\n",
    "                    label=None\n",
    "                )\n",
    "\n",
    "        min_alpha = 0.15\n",
    "        plots = []\n",
    "        for i, layer in enumerate(layers_to_plot or results.keys()):\n",
    "            correlations, pvalues = results[layer]\n",
    "            alphas = np.minimum(\n",
    "                1, \n",
    "                np.where(np.isnan(pvalues), 0, min_alpha + (1 - min_alpha) * (1 - pvalues) ** 4)\n",
    "            )  # Higher alpha for smaller p-values, lower alpha for larger p-values\n",
    "            if pv_threshold is not None:\n",
    "                alphas = np.where(pvalues <= pv_threshold, alphas, 0)\n",
    "\n",
    "            for j in range(len(eccs) - 1):\n",
    "                __plot_smooth_alpha(\n",
    "                    eccs[j:j+2], \n",
    "                    f(correlations[j:j+2]),\n",
    "                    color=colors(i),\n",
    "                    alphas=alphas[j:j+2],\n",
    "                    linewidth_fun=lambda a: 0.3 + a * 0.7,\n",
    "                )\n",
    "            \n",
    "            label = names_r[layer] if layer != 'nonfit' else \"nonfit\"\n",
    "            plots.append(plt.scatter(\n",
    "                eccs, f(correlations), \n",
    "                label=label,\n",
    "                color=colors(i),\n",
    "                alpha=alphas,\n",
    "                edgecolors='none',\n",
    "                s=20 + alphas * 30\n",
    "            ))\n",
    "\n",
    "        legend = plt.legend(loc='best')\n",
    "\n",
    "        for lh in legend.legend_handles:\n",
    "            lh.set_alpha(np.ones_like(alphas))  # Set alpha of legend markers to 1\n",
    "\n",
    "        lim = 0.8 #np.ceil(max([abs(y) for y in plt.ylim()]) * 10) / 10\n",
    "        plt.ylim(0 if abs_ else -lim, lim)\n",
    "        plt.xlim(-10, 10)\n",
    "\n",
    "        plt.title(f'Pointwise {corr_name} Correlation of Cone Density with Layers, {direction}-Axis')\n",
    "        plt.xlabel('Eccentricity [°]')\n",
    "        ylabel = f'{corr_name} correlation coefficient'\n",
    "        plt.ylabel(f'|{ylabel}|' if abs_ else ylabel)\n",
    "\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99efe3ac",
   "metadata": {},
   "source": [
    "#### Covariate Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9509fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Callable, Tuple, Optional\n",
    "from scipy import stats  # used for potential further diagnostics (if needed)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def density_to_spacing(cd: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert cone density [cells/mm²] to cone spacing [arcmin].\n",
    "    \"\"\"\n",
    "    return np.sqrt(2 / cd / np.sqrt(3)) / MM_PER_DEGREE * 60 # in arcmin\n",
    "\n",
    "def get_data_from_range(layer_name, left, right, only_valid: bool = True, flatten:bool = True):\n",
    "    range_eccs = np.argwhere((left <= eccs) & (eccs < right)).flatten()\n",
    "\n",
    "    cd = np.array([getattr(s, f'density_fit_X') for s in subjects_data])[:, range_eccs]\n",
    "    os = np.array([getattr(s, f'{layer_name}_X') for s in subjects_data])[:, range_eccs]\n",
    "\n",
    "    if flatten:\n",
    "        cd = cd.flatten()\n",
    "        os = os.flatten()\n",
    "    # print(os)\n",
    "    if only_valid:\n",
    "        valid = ~np.isnan(cd) & ~np.isnan(os)\n",
    "        cd = cd[valid]\n",
    "        os = os[valid]\n",
    "    return cd, os\n",
    "\n",
    "def preprocess_functional_feature(data: np.ndarray, \n",
    "                                  covariates: Optional[np.ndarray] = None, \n",
    "                                  standardization: str = 'inter', \n",
    "                                  perform_regression: bool = False,\n",
    "                                  verbose: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess a functional feature (e.g. cone density or layer thickness as a function of eccentricity)\n",
    "    by optionally removing the effects of one or more covariates via linear regression and then (Z-)standardizing it.\n",
    "    `data` should have shape (n_subjects, n_features).\n",
    "\n",
    "    Parameters:\n",
    "      data: Array with dimensions (n_subjects, n_features)\n",
    "      covariates: Either a 1D array of shape (n_subjects,) or a 2D array of shape (n_subjects, n_covariates)\n",
    "      standardization: 'inter' to standardize across subjects (per column) or 'intra' to standardize within subjects (per row)\n",
    "      perform_regression: If True and covariates is not None, the function will remove covariate effects via linear regression.\n",
    "      verbose: If True, prints diagnostic output (e.g. regression coefficients, R², and diagnostic plots)\n",
    "\n",
    "    Returns:\n",
    "      The preprocessed (residualized and standardized) data.\n",
    "    \"\"\"\n",
    "    if perform_regression and covariates is not None:\n",
    "        # Ensure covariates is 2D (n_subjects, n_covariates)\n",
    "        if covariates.ndim == 1:\n",
    "            covariates = covariates.reshape(-1, 1)\n",
    "\n",
    "        # Create an array for the residuals\n",
    "        residuals = np.empty_like(data)\n",
    "        n_cov = covariates.shape[1]\n",
    "\n",
    "        # For each feature column (e.g. each eccentricity), perform a multiple linear regression with an intercept.\n",
    "        for i in range(data.shape[1]):\n",
    "            y = data[:, i]\n",
    "            # Only consider valid entries (finite y and finite covariates)\n",
    "            valid_mask = np.isfinite(y) & np.all(np.isfinite(covariates), axis=1)\n",
    "            if np.sum(valid_mask) < n_cov + 1:\n",
    "                residuals[:, i] = np.nan\n",
    "                if verbose:\n",
    "                    print(f\"Feature {i}: Not enough valid data points for regression.\")\n",
    "            else:\n",
    "                # Build design matrix with intercept\n",
    "                X = covariates[valid_mask, :]\n",
    "                X = np.column_stack((np.ones(X.shape[0]), X))\n",
    "                y_valid = y[valid_mask]\n",
    "                # Solve the least squares regression: y_valid = X * beta\n",
    "                beta, residuals_sum, rank, s = np.linalg.lstsq(X, y_valid, rcond=None)\n",
    "                # Compute predicted values and residuals\n",
    "                y_hat = X.dot(beta)\n",
    "                res = y_valid - y_hat\n",
    "                # Prepare a full array of residuals (with NaN where data is invalid)\n",
    "                full_res = np.full(y.shape, np.nan)\n",
    "                full_res[valid_mask] = res\n",
    "                residuals[:, i] = full_res\n",
    "\n",
    "                if verbose:\n",
    "                    # Calculate R-squared for this regression (if possible)\n",
    "                    ss_res = np.sum(res**2)\n",
    "                    ss_tot = np.sum((y_valid - np.mean(y_valid))**2)\n",
    "                    r_squared = 1 - ss_res / ss_tot if ss_tot != 0 else np.nan\n",
    "                    print(f\"Feature {i} regression coefficients: {beta}\")\n",
    "                    print(f\"Feature {i} R-squared: {r_squared}\")\n",
    "\n",
    "                    # Diagnostic plot: Predicted vs. Actual\n",
    "                    plt.figure()\n",
    "                    plt.scatter(y_valid, y_hat, c='blue', label='Fitted values')\n",
    "                    plt.plot(y_valid, y_valid, 'r--', label='Ideal')\n",
    "                    plt.xlabel('Actual values')\n",
    "                    plt.ylabel('Predicted values')\n",
    "                    plt.title(f'Feature {i}: Predicted vs. Actual')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "\n",
    "                    # Diagnostic plot: Residuals vs. Predicted\n",
    "                    plt.figure()\n",
    "                    plt.scatter(y_hat, res, c='green', label='Residuals')\n",
    "                    plt.axhline(0, color='red', linestyle='--')\n",
    "                    plt.xlabel('Predicted values')\n",
    "                    plt.ylabel('Residuals')\n",
    "                    plt.title(f'Feature {i}: Residuals vs. Predicted')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "        data = residuals\n",
    "\n",
    "        # Additional diagnostic: Plot the correlation matrix of the residualized features (across eccentricities)\n",
    "        if verbose:\n",
    "            try:\n",
    "                corr_matrix = np.corrcoef(data, rowvar=False)\n",
    "                plt.figure()\n",
    "                plt.imshow(corr_matrix, interpolation='nearest', cmap='viridis')\n",
    "                plt.title(\"Correlation Matrix among Residualized Features\")\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(\"Could not compute correlation matrix for diagnostics:\", e)\n",
    "\n",
    "    # Standardize the data using only valid (non-NaN) values\n",
    "    if standardization == 'inter':\n",
    "        mean = np.nanmean(data, axis=0, keepdims=True)\n",
    "        std = np.nanstd(data, axis=0, keepdims=True)\n",
    "        data = (data - mean) / std\n",
    "    elif standardization == 'intra':\n",
    "        mean = np.nanmean(data, axis=1, keepdims=True)\n",
    "        std = np.nanstd(data, axis=1, keepdims=True)\n",
    "        data = (data - mean) / std\n",
    "\n",
    "    return data\n",
    "\n",
    "def preprocess_functional_data(direction, \n",
    "                               standardization: str = 'inter', \n",
    "                               toLog: bool = True, \n",
    "                               covariates: Optional[np.ndarray] = None, \n",
    "                               perform_regression: bool = False,\n",
    "                               verbose: bool = False) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Preprocess functional data (e.g. cone density, layer thicknesses) for a given direction (X or Y)\n",
    "    by (Z-)standardizing it and optionally removing covariate effects.\n",
    "\n",
    "    It processes each layer as well as cone density (optionally logging cone density values).\n",
    "\n",
    "    Parameters:\n",
    "      direction: An object with a .value attribute indicating the direction (e.g., 'X' or 'Y')\n",
    "      standardization: 'inter' or 'intra' standardization mode\n",
    "      toLog: If True, apply logarithm to the cone density values before processing\n",
    "      covariates: A 1D or 2D array of covariate values to regress out\n",
    "      perform_regression: If True, perform regression to remove the effects of covariates\n",
    "      verbose: If True, prints regression diagnostics and produces diagnostic plots\n",
    "          \n",
    "    Returns:\n",
    "      A dictionary with keys for the cone density ('cones') and each layer.\n",
    "    \"\"\"\n",
    "    # Assuming subjects_data and layer_names are defined in your context\n",
    "    layer_fds = {\n",
    "        layer: preprocess_functional_feature(\n",
    "            data=np.array([getattr(s, f'{layer}_{direction.value}') for s in subjects_data]),\n",
    "            covariates=covariates,\n",
    "            standardization=standardization,\n",
    "            perform_regression=perform_regression,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        for layer in layer_names\n",
    "    }\n",
    "    \n",
    "    if toLog:\n",
    "        cone_density_fd = preprocess_functional_feature(\n",
    "            data=np.array([np.log(getattr(s, f'density_fit_{direction.value}')) for s in subjects_data]),\n",
    "            covariates=covariates,\n",
    "            standardization=standardization,\n",
    "            perform_regression=perform_regression,\n",
    "            verbose=verbose\n",
    "        )\n",
    "    else:\n",
    "        cone_density_fd = preprocess_functional_feature(\n",
    "            data=np.array([getattr(s, f'density_fit_{direction.value}') for s in subjects_data]),\n",
    "            covariates=covariates,\n",
    "            standardization=standardization,\n",
    "            perform_regression=perform_regression,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "    cone_density_nonfit = preprocess_functional_feature(\n",
    "        data=np.array([getattr(s, f'density_{direction.value}') for s in subjects_data]),\n",
    "        covariates=covariates,\n",
    "        standardization=standardization,\n",
    "        perform_regression=perform_regression,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Return a dictionary with the cone density and the layers.\n",
    "    return {'cones': cone_density_fd, **layer_fds}\n",
    "\n",
    "def compute_correlations_eccwise(direction, \n",
    "                                 eccs: np.ndarray, \n",
    "                                 correlation_fun: Callable[[np.ndarray, np.ndarray], Tuple[float, float]] = lambda cd, lt: stats.spearmanr(cd, lt, nan_policy='omit'),\n",
    "                                 covariates: Optional[np.ndarray] = None, \n",
    "                                 perform_regression: bool = False,\n",
    "                                 verbose: bool = False) -> Dict[str, Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Compute inter-individual Spearman correlations between standardized cone density and each standardized layer,\n",
    "    for each eccentricity. The functional data are first preprocessed by optionally removing covariate effects.\n",
    "\n",
    "    Parameters:\n",
    "      direction: An object with a .value attribute (e.g., 'X' or 'Y')\n",
    "      eccs: Array of eccentricity values\n",
    "      correlation_fun: A function that computes correlation and p-value between two 1D arrays\n",
    "      covariates: A 1D or 2D array of covariate values to regress out\n",
    "      perform_regression: If True, perform regression to remove covariate effects before computing correlations\n",
    "      verbose: If True, prints diagnostic information during preprocessing\n",
    "          \n",
    "    Returns:\n",
    "      A dictionary mapping each layer (other than 'cones') to a tuple of arrays: (correlation values, p-values)\n",
    "    \"\"\"\n",
    "    \n",
    "    fd = preprocess_functional_data(\n",
    "        direction=direction, \n",
    "        standardization='inter', \n",
    "        covariates=covariates, \n",
    "        perform_regression=perform_regression,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    for layer, layer_fd in fd.items():\n",
    "        if layer == 'cones':\n",
    "            continue\n",
    "        pointwise_corr = np.zeros(len(eccs))\n",
    "        pointwise_pvalues = np.zeros(len(eccs))\n",
    "        \n",
    "        # For each eccentricity compute the Spearman correlation and p-value.\n",
    "        for i in range(len(eccs)):\n",
    "            cone_density_values = fd['cones'][:, i]\n",
    "            layer_values = layer_fd[:, i]\n",
    "            corr, pv = correlation_fun(cone_density_values, layer_values)\n",
    "            pointwise_corr[i] = corr\n",
    "            pointwise_pvalues[i] = pv\n",
    "        results[layer] = (pointwise_corr, pointwise_pvalues)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9fcac2",
   "metadata": {},
   "source": [
    "### Retinal Scheme Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_circular_heatmap_multi_direction(results_dict, layer_name, \n",
    "                                           directions=None, \n",
    "                                           angular_sectors=None,\n",
    "                                           show_significance=True):\n",
    "    \"\"\"\n",
    "    Create a circular heatmap for a specific layer showing mean slopes\n",
    "    in different regions for multiple directions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_dict : dict\n",
    "        Dictionary with keys as direction names and values as layer_results\n",
    "        e.g., {\"Superior\": layer_results_S, \"Nasal\": layer_results_N, ...}\n",
    "    layer_name : str\n",
    "        Name of the layer to plot\n",
    "    directions : list of str, optional\n",
    "        Order of directions to plot. If None, uses keys from results_dict\n",
    "    angular_sectors : list of dict, optional\n",
    "        Custom angular sector definitions\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Define default angular sectors if not provided\n",
    "    if angular_sectors is None:\n",
    "        angular_sectors = [\n",
    "            {\"name\": \"Superior\", \"start\": 45, \"end\": 135},\n",
    "            {\"name\": \"Nasal\", \"start\": 315, \"end\": 45},\n",
    "            {\"name\": \"Inferior\", \"start\": 225, \"end\": 315},\n",
    "            {\"name\": \"Temporal\", \"start\": 135, \"end\": 225}\n",
    "        ]\n",
    "    \n",
    "    # Use provided direction order or get from results_dict\n",
    "    if directions is None:\n",
    "        directions = list(results_dict.keys())\n",
    "    \n",
    "    # Define the radial boundaries\n",
    "    radial_bounds = {\n",
    "        \"central\": (0, 0.5),\n",
    "        \"inner\": (0.5, 1.5),\n",
    "        \"outer\": (1.5, 3.0)\n",
    "    }\n",
    "    \n",
    "    # For visualization, normalize these to 0-1 range\n",
    "    max_radius = 3.0\n",
    "    norm_bounds = {\n",
    "        \"central\": (0, 0.5/max_radius),\n",
    "        \"inner\": (0.5/max_radius, 1.5/max_radius),\n",
    "        \"outer\": (1.5/max_radius, 3.0/max_radius)\n",
    "    }\n",
    "    \n",
    "    # Collect all slope values for normalization\n",
    "    all_slopes = []\n",
    "    for direction, layer_results in results_dict.items():\n",
    "        if layer_name in layer_results:\n",
    "            for slope_dict in layer_results[layer_name]:\n",
    "                if slope_dict[\"mean\"] != -1:\n",
    "                    all_slopes.append(slope_dict[\"mean\"])\n",
    "    \n",
    "    # Set color scale bounds\n",
    "    if all_slopes:\n",
    "        vmin = -0.3 if min(all_slopes) > -0.5 else min(all_slopes) \n",
    "        vmax = 0.3 if max(all_slopes) < 0.5 else max(all_slopes)\n",
    "    else:\n",
    "        vmin, vmax = -1, 1\n",
    "    \n",
    "    # Create colormap\n",
    "    cmap = plt.cm.plasma # Red-Blue reversed (red for positive, blue for negative)\n",
    "    \n",
    "    # Draw the heatmap\n",
    "    for sector in angular_sectors:\n",
    "        # Find the corresponding direction data\n",
    "        direction_name = sector[\"name\"]\n",
    "        if direction_name not in results_dict:\n",
    "            print(f\"Warning: Direction '{direction_name}' not found in results_dict\")\n",
    "            continue\n",
    "        \n",
    "        layer_results = results_dict[direction_name]\n",
    "        if layer_name not in layer_results:\n",
    "            print(f\"Warning: Layer '{layer_name}' not found in {direction_name} results\")\n",
    "            continue\n",
    "        \n",
    "        slopes_data = layer_results[layer_name]\n",
    "        \n",
    "        # Draw wedges for each radial region\n",
    "        for region_name, (r_inner, r_outer) in norm_bounds.items():\n",
    "            # Find the corresponding slope value\n",
    "            slope_value = -1  # default\n",
    "            for slope_dict in slopes_data:\n",
    "                if slope_dict[\"name\"] == region_name:\n",
    "                    slope_value = slope_dict[\"mean\"]\n",
    "                    break\n",
    "            \n",
    "            theta1 = sector[\"start\"]\n",
    "            theta2 = sector[\"end\"]\n",
    "            \n",
    "            # Handle wraparound for sectors that cross 0 degrees\n",
    "            if theta2 < theta1:\n",
    "                theta2 += 360\n",
    "            \n",
    "            # Create wedge\n",
    "            if slope_value != -1:\n",
    "                color = cmap((slope_value - vmin) / (vmax - vmin))\n",
    "            else:\n",
    "                color = 'lightgray'\n",
    "            \n",
    "            wedge = patches.Wedge(\n",
    "                center=(0, 0),\n",
    "                r=r_outer,\n",
    "                theta1=theta1,\n",
    "                theta2=theta2,\n",
    "                width=r_outer - r_inner,\n",
    "                facecolor=color,\n",
    "                edgecolor='white',\n",
    "                linewidth=2\n",
    "            )\n",
    "            ax.add_patch(wedge)\n",
    "            \n",
    "            # Add text annotation for slope value and significance\n",
    "            # Calculate center position of wedge\n",
    "            angle_mid = np.radians((theta1 + theta2) / 2)\n",
    "            r_mid = (r_inner + r_outer) / 2\n",
    "            x_text = r_mid * np.cos(angle_mid)\n",
    "            y_text = r_mid * np.sin(angle_mid)\n",
    "            \n",
    "            if slope_value != -1:\n",
    "                # Check if we have slopes list to calculate significance\n",
    "                p_value = None\n",
    "                if show_significance:\n",
    "                    for slope_dict in slopes_data:\n",
    "                        if slope_dict[\"name\"] == region_name and \"slopes\" in slope_dict and len(slope_dict[\"slopes\"]) > 1:\n",
    "                            # Calculate t-test p-value for slopes different from zero\n",
    "                            from scipy.stats import ttest_1samp\n",
    "                            slopes_array = slope_dict[\"slopes\"]\n",
    "                            t_stat, p_value = ttest_1samp(slopes_array, 0)\n",
    "                            break\n",
    "                \n",
    "                # Format text with significance indicator\n",
    "                text = f'{slope_value:.2f}'\n",
    "                if p_value is not None:\n",
    "                    if p_value < 0.001:\n",
    "                        text += '\\n ***'\n",
    "                    elif p_value < 0.01:\n",
    "                        text += '\\n **'\n",
    "                    elif p_value < 0.05:\n",
    "                        text += '\\n *'\n",
    "                    else:\n",
    "                        text += ' \\n ns'  # not significant\n",
    "                \n",
    "                ax.text(x_text, y_text, text, \n",
    "                       ha='center', va='center', \n",
    "                       fontsize=8, fontweight='bold',\n",
    "                       color='white' if abs(slope_value - np.mean([vmin, vmax])) > (vmax - vmin) * 0.3 else 'black')\n",
    "    \n",
    "    # Add circle boundaries\n",
    "    for region_name, (r_inner, r_outer) in norm_bounds.items():\n",
    "        circle = plt.Circle((0, 0), r_outer, fill=False, color='white', linewidth=2)\n",
    "        ax.add_patch(circle)\n",
    "    \n",
    "    # Add radial lines to separate directions\n",
    "    for sector in angular_sectors:\n",
    "        angle = np.radians(sector[\"start\"])\n",
    "        x_end = 1.1 * np.cos(angle)\n",
    "        y_end = 1.1 * np.sin(angle)\n",
    "        ax.plot([0, x_end], [0, y_end], 'white', linewidth=2)\n",
    "    \n",
    "    # Set axis properties\n",
    "    ax.set_xlim(-1.3, 1.3)\n",
    "    ax.set_ylim(-1.35, 1.3)  # Extended to accommodate legend\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title(f'Mean Slopes Heatmap - {layer_name}', fontsize=16, pad=20)\n",
    "    \n",
    "    # Add colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Mean Slope', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Add labels for directions\n",
    "    label_radius = 1.15\n",
    "    for sector in angular_sectors:\n",
    "        angle = np.radians((sector[\"start\"] + sector[\"end\"]) / 2)\n",
    "        if sector[\"end\"] < sector[\"start\"]:  # Handle wraparound\n",
    "            angle = np.radians((sector[\"start\"] + sector[\"end\"] + 360) / 2)\n",
    "        x = label_radius * np.cos(angle)\n",
    "        y = label_radius * np.sin(angle)\n",
    "        ax.text(x, y, sector[\"name\"], ha='center', va='center', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add labels for radial regions\n",
    "    angle_for_labels = np.radians(0)  # Put labels on the right side\n",
    "    for region_name, (r_inner, r_outer) in norm_bounds.items():\n",
    "        r_mid = (r_inner + r_outer) / 2\n",
    "        x = r_mid * np.cos(angle_for_labels)\n",
    "        y = r_mid * np.sin(angle_for_labels)\n",
    "\n",
    "    \n",
    "    # Add significance legend\n",
    "    legend_text = \"* p<0.05, ** p<0.01, *** p<0.001, ns = not significant\"\n",
    "    ax.text(0, -1.25, legend_text, ha='center', va='center', \n",
    "            fontsize=9, style='italic', color='gray')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_comparison_grid(results_dict, layers_to_plot, \n",
    "                          directions=None, angular_sectors=None):\n",
    "    \"\"\"\n",
    "    Create a grid of circular heatmaps for multiple layers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_dict : dict\n",
    "        Dictionary with direction names as keys and layer_results as values\n",
    "    layers_to_plot : list\n",
    "        List of layer names to plot\n",
    "    \"\"\"\n",
    "    n_layers = len(layers_to_plot)\n",
    "    n_cols = min(3, n_layers)  # Maximum 3 columns\n",
    "    n_rows = (n_layers + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 6*n_rows))\n",
    "    if n_layers == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if n_rows > 1 else axes\n",
    "    \n",
    "    for idx, layer_name in enumerate(layers_to_plot):\n",
    "        ax = axes[idx]\n",
    "        plt.sca(ax)\n",
    "        \n",
    "        # Create individual heatmap\n",
    "        create_circular_heatmap_single_axis(results_dict, layer_name, ax,\n",
    "                                          directions, angular_sectors)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(n_layers, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_circular_heatmap_single_axis(results_dict, layer_name, ax,\n",
    "                                       directions=None, angular_sectors=None):\n",
    "    \"\"\"\n",
    "    Helper function to create a circular heatmap on a specific axis.\n",
    "    \"\"\"\n",
    "    # Define default angular sectors if not provided\n",
    "    if angular_sectors is None:\n",
    "        angular_sectors = [\n",
    "            {\"name\": \"Superior\", \"start\": 45, \"end\": 135},\n",
    "            {\"name\": \"Nasal\", \"start\": 135, \"end\": 225},\n",
    "            {\"name\": \"Inferior\", \"start\": 225, \"end\": 315},\n",
    "            {\"name\": \"Temporal\", \"start\": 315, \"end\": 45}\n",
    "        ]\n",
    "    \n",
    "    # Define the radial boundaries\n",
    "    norm_bounds = {\n",
    "        \"central\": (0, 0.5/3.0),\n",
    "        \"inner\": (0.5/3.0, 1.5/3.0),\n",
    "        \"outer\": (1.5/3.0, 3.0/3.0)\n",
    "    }\n",
    "    \n",
    "    # Collect all slope values for normalization\n",
    "    all_slopes = []\n",
    "    for direction, layer_results in results_dict.items():\n",
    "        if layer_name in layer_results:\n",
    "            for slope_dict in layer_results[layer_name]:\n",
    "                if slope_dict[\"mean\"] != -1:\n",
    "                    all_slopes.append(slope_dict[\"mean\"])\n",
    "    \n",
    "    # Set color scale bounds\n",
    "    if all_slopes:\n",
    "        vmin = -0.3 if min(all_slopes) > -0.5 else min(all_slopes) \n",
    "        vmax = 0.3 if max(all_slopes) < 0.5 else max(all_slopes)\n",
    "    else:\n",
    "        vmin, vmax = -1, 1\n",
    "    \n",
    "    # Create colormap\n",
    "    cmap = plt.cm.RdBu_r\n",
    "    \n",
    "    # Draw the heatmap\n",
    "    for sector in angular_sectors:\n",
    "        direction_name = sector[\"name\"]\n",
    "        if direction_name not in results_dict:\n",
    "            continue\n",
    "        \n",
    "        layer_results = results_dict[direction_name]\n",
    "        if layer_name not in layer_results:\n",
    "            continue\n",
    "        \n",
    "        slopes_data = layer_results[layer_name]\n",
    "        \n",
    "        # Draw wedges for each radial region\n",
    "        for region_name, (r_inner, r_outer) in norm_bounds.items():\n",
    "            slope_value = -1\n",
    "            for slope_dict in slopes_data:\n",
    "                if slope_dict[\"name\"] == region_name:\n",
    "                    slope_value = slope_dict[\"mean\"]\n",
    "                    break\n",
    "            \n",
    "            theta1 = sector[\"start\"]\n",
    "            theta2 = sector[\"end\"]\n",
    "            if theta2 < theta1:\n",
    "                theta2 += 360\n",
    "            \n",
    "            if slope_value != -1:\n",
    "                color = cmap((slope_value - vmin) / (vmax - vmin))\n",
    "            else:\n",
    "                color = 'lightgray'\n",
    "            \n",
    "            wedge = patches.Wedge(\n",
    "                center=(0, 0),\n",
    "                r=r_outer,\n",
    "                theta1=theta1,\n",
    "                theta2=theta2,\n",
    "                width=r_outer - r_inner,\n",
    "                facecolor=color,\n",
    "                edgecolor='white',\n",
    "                linewidth=1.5\n",
    "            )\n",
    "            ax.add_patch(wedge)\n",
    "    \n",
    "    # Add circle boundaries\n",
    "    for region_name, (r_inner, r_outer) in norm_bounds.items():\n",
    "        circle = plt.Circle((0, 0), r_outer, fill=False, color='white', linewidth=1.5)\n",
    "        ax.add_patch(circle)\n",
    "    \n",
    "    # Set axis properties\n",
    "    ax.set_xlim(-1.2, 1.2)\n",
    "    ax.set_ylim(-1.2, 1.2)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(layer_name, fontsize=12, pad=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca892a",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axial Length covariate\n",
    "axial_lengths = np.zeros(len(subjects_data))\n",
    "for i, s in enumerate(subjects_data):\n",
    "    axial_lengths[i] = s.axial_length\n",
    "\n",
    "covariates = np.column_stack((axial_lengths))\n",
    "\n",
    "resultsX = compute_correlations_eccwise(Direction.X, eccs, covariates=axial_lengths, perform_regression=True, verbose=False)\n",
    "resultsY = compute_correlations_eccwise(Direction.Y, eccs, covariates=axial_lengths, perform_regression=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5927b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlations_eccwise(resultsY, eccs, Direction.Y, abs_=False, pv_threshold = 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlations_eccwise(resultsY, eccs, Direction.Y, abs_=False, pv_threshold =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlations_eccwise(resultsX, eccs, Direction.X, abs_=False, pv_threshold=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlations_eccwise(resultsX, eccs, Direction.X, abs_=False, pv_threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b589cb4d",
   "metadata": {},
   "source": [
    "### Circular Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6260517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import zscore\n",
    "\n",
    "results = resultsY\n",
    "\n",
    "threshold = 1\n",
    "layers_to_plot = results.keys()\n",
    "\n",
    "\n",
    "\n",
    "#Range of eccentricity to plot\n",
    "range_start = -10.0\n",
    "range_end = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layer_results_superior = {}\n",
    "\n",
    "\n",
    "# correlations_to_plot = \"negative\"\n",
    "with(plt.style.context(matplotx.styles.pacoty)):\n",
    "\n",
    "    for layer_name, (correlations, p_values) in results.items():\n",
    "        \n",
    "        circles = [\n",
    "\n",
    "        {\"name\": \"central\", \"start\": -0.5, \"end\": 0.0},\n",
    "        {\"name\": \"inner\", \"start\": -1.5, \"end\": -0.5},\n",
    "        {\"name\": \"outer\", \"start\": -3.0, \"end\": -1.5},\n",
    "        ]\n",
    "\n",
    "        slopes = [\n",
    "        {\"name\": \"central\", \"slopes\" : [], \"mean\": -1},\n",
    "\n",
    "        {\"name\": \"inner\", \"slopes\" : [],  \"mean\": -1},\n",
    "\n",
    "        {\"name\": \"outer\", \"slopes\" : [],  \"mean\": -1}\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "        if layer_name not in layers_to_plot:\n",
    "            continue\n",
    "\n",
    "        for ecc, corr, p_val in zip(eccs, correlations, p_values):\n",
    "            if ecc < range_start or ecc > range_end:\n",
    "                continue\n",
    "\n",
    "            if p_val < threshold:\n",
    "                # Retrieve the actual data for this layer and eccentricity\n",
    "                x_data, y_data = get_data_from_range(layer_name, (ecc - 0.05), (ecc + 0.05))\n",
    "                \n",
    "                # Z-score the data for regression\n",
    "                x_data_z = zscore(x_data)\n",
    "                y_data_z = zscore(y_data)\n",
    "                \n",
    "                # Fit a line via linregress on the z-scored data\n",
    "                slopeprint, intercept, r_value, p_value_fit, std_err = linregress(x_data_z, y_data_z)\n",
    "\n",
    "                for circle in circles:\n",
    "                    if ecc >= circle[\"start\"] and ecc < circle[\"end\"]:\n",
    "                        for slope_dict in slopes:\n",
    "                            if slope_dict[\"name\"] == circle[\"name\"]:\n",
    "                                slope_dict[\"slopes\"].append(slopeprint)\n",
    "                                break\n",
    "                        # print(f\"DEBUG: {layer_name} at ecc {ecc} in {circle['name']} circle with slope {slopeprint:.3f}\")\n",
    "                        break\n",
    "\n",
    "                slope, intercept, r_value, p_value_fit, std_err = linregress(x_data, y_data)\n",
    "\n",
    "                # r2scores[layer_name].append(r_value**2)\n",
    "                \n",
    "                # Compute points on the line for plotting using the z-scored data\n",
    "                x_line = np.linspace(x_data.min(), x_data.max(), 100)\n",
    "                y_line = slope * x_line + intercept\n",
    "\n",
    "                # print(f\"processing {layer_name} at ecc {ecc} with corr {corr:.3f}, p-value {p_val:.3f}, slope {slopeprint:.3f}\")\n",
    "\n",
    "            \n",
    "\n",
    "        for slope in slopes:\n",
    "            if len(slope[\"slopes\"]) > 0:\n",
    "                # print(f\"DEBUG: {layer_name} mean slope for {slope['name']} circle:\", np.mean(slope[\"slopes\"]))\n",
    "                slope[\"mean\"] = np.mean(slope[\"slopes\"])\n",
    "            else:\n",
    "                # print(f\"DEBUG: {layer_name} no slopes for {slope['name']} circle, setting mean to -1\")\n",
    "                slope[\"mean\"] = -1\n",
    "\n",
    "        # print(f\"DEBUG: {layer_name} slopes:\", [s[\"mean\"] for s in slopes])\n",
    "\n",
    "        layer_results_superior[layer_name] = slopes\n",
    "        # print(f\"DEBUG: {layer_name} layer results:\", layer_results_superior)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb474a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "from scipy.stats import zscore\n",
    "\n",
    "results = resultsY\n",
    "\n",
    "threshold = 1\n",
    "layers_to_plot = results.keys()\n",
    "\n",
    "\n",
    "\n",
    "#Range of eccentricity to plot\n",
    "range_start = 0.0\n",
    "range_end = 10.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layer_results_inferior  = {}\n",
    "\n",
    "\n",
    "# correlations_to_plot = \"negative\"\n",
    "with(plt.style.context(matplotx.styles.pacoty)):\n",
    "\n",
    "    for layer_name, (correlations, p_values) in results.items():\n",
    "        \n",
    "        circles = [\n",
    "\n",
    "        {\"name\": \"central\", \"start\": 0.0, \"end\": 0.5},\n",
    "        {\"name\": \"inner\", \"start\": 0.5, \"end\": 1.5},\n",
    "        {\"name\": \"outer\", \"start\": 1.5, \"end\": 3.0},\n",
    "        ]\n",
    "\n",
    "        slopes = [\n",
    "        {\"name\": \"central\", \"slopes\" : [], \"mean\": -1},\n",
    "\n",
    "        {\"name\": \"inner\", \"slopes\" : [],  \"mean\": -1},\n",
    "\n",
    "        {\"name\": \"outer\", \"slopes\" : [],  \"mean\": -1}\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "        if layer_name not in layers_to_plot:\n",
    "            continue\n",
    "\n",
    "        for ecc, corr, p_val in zip(eccs, correlations, p_values):\n",
    "            if ecc < range_start or ecc > range_end:\n",
    "                continue\n",
    "\n",
    "            if p_val < threshold:\n",
    "                # Retrieve the actual data for this layer and eccentricity\n",
    "                x_data, y_data = get_data_from_range(layer_name, (ecc - 0.05), (ecc + 0.05))\n",
    "                \n",
    "                # Z-score the data for regression\n",
    "                x_data_z = zscore(x_data)\n",
    "                y_data_z = zscore(y_data)\n",
    "                \n",
    "                # Fit a line via linregress on the z-scored data\n",
    "                slopeprint, intercept, r_value, p_value_fit, std_err = linregress(x_data_z, y_data_z)\n",
    "\n",
    "                for circle in circles:\n",
    "                    if ecc*MM_PER_DEGREE>= circle[\"start\"] and ecc*MM_PER_DEGREE < circle[\"end\"]:\n",
    "                        for slope_dict in slopes:\n",
    "                            if slope_dict[\"name\"] == circle[\"name\"]:\n",
    "                                slope_dict[\"slopes\"].append(slopeprint)\n",
    "                                break\n",
    "                        # print(f\"DEBUG: {layer_name} at ecc {ecc} in {circle['name']} circle with slope {slopeprint:.3f}\")\n",
    "                        break\n",
    "\n",
    "                slope, intercept, r_value, p_value_fit, std_err = linregress(x_data, y_data)\n",
    "\n",
    "                # r2scores[layer_name].append(r_value**2)\n",
    "                \n",
    "                # Compute points on the line for plotting using the z-scored data\n",
    "                x_line = np.linspace(x_data.min(), x_data.max(), 100)\n",
    "                y_line = slope * x_line + intercept\n",
    "\n",
    "                # print(f\"processing {layer_name} at ecc {ecc} with corr {corr:.3f}, p-value {p_val:.3f}, slope {slopeprint:.3f}\")\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        for slope in slopes:\n",
    "            if len(slope[\"slopes\"]) > 0:\n",
    "                # print(f\"DEBUG: {layer_name} mean slope for {slope['name']} circle:\", np.mean(slope[\"slopes\"]))\n",
    "                slope[\"mean\"] = np.mean(slope[\"slopes\"])\n",
    "            else:\n",
    "                # print(f\"DEBUG: {layer_name} no slopes for {slope['name']} circle, setting mean to -1\")\n",
    "                slope[\"mean\"] = -1\n",
    "\n",
    "        # print(f\"DEBUG: {layer_name} slopes:\", [s[\"mean\"] for s in slopes])\n",
    "\n",
    "        layer_results_inferior[layer_name] = slopes\n",
    "        # print(f\"DEBUG: {layer_name} layer results:\", layer_results_inferior)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "from scipy.stats import zscore\n",
    "\n",
    "results = resultsX\n",
    "\n",
    "threshold = 1\n",
    "layers_to_plot = results.keys()\n",
    "\n",
    "\n",
    "\n",
    "#Range of eccentricity to plot\n",
    "range_start = -10.0\n",
    "range_end = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layer_results_temporal = {}\n",
    "\n",
    "\n",
    "# correlations_to_plot = \"negative\"\n",
    "with(plt.style.context(matplotx.styles.pacoty)):\n",
    "\n",
    "    for layer_name, (correlations, p_values) in results.items():\n",
    "        \n",
    "        circles = [\n",
    "\n",
    "        {\"name\": \"central\", \"start\": -0.5, \"end\": 0.0},\n",
    "        {\"name\": \"inner\", \"start\": -1.5, \"end\": -0.5},\n",
    "        {\"name\": \"outer\", \"start\": -3.0, \"end\": -1.5},\n",
    "        ]\n",
    "\n",
    "        slopes = [\n",
    "        {\"name\": \"central\", \"slopes\" : [], \"mean\": -1},\n",
    "\n",
    "        {\"name\": \"inner\", \"slopes\" : [],  \"mean\": -1},\n",
    "\n",
    "        {\"name\": \"outer\", \"slopes\" : [],  \"mean\": -1}\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "        if layer_name not in layers_to_plot:\n",
    "            continue\n",
    "\n",
    "        for ecc, corr, p_val in zip(eccs, correlations, p_values):\n",
    "            if ecc < range_start or ecc > range_end:\n",
    "                continue\n",
    "\n",
    "            if p_val < threshold:\n",
    "                # Retrieve the actual data for this layer and eccentricity\n",
    "                x_data, y_data = get_data_from_range(layer_name, (ecc - 0.05), (ecc + 0.05))\n",
    "                \n",
    "                # Z-score the data for regression\n",
    "                x_data_z = zscore(x_data)\n",
    "                y_data_z = zscore(y_data)\n",
    "                \n",
    "                # Fit a line via linregress on the z-scored data\n",
    "                slopeprint, intercept, r_value, p_value_fit, std_err = linregress(x_data_z, y_data_z)\n",
    "\n",
    "                for circle in circles:\n",
    "                    if ecc*MM_PER_DEGREE >= circle[\"start\"] and ecc*MM_PER_DEGREE < circle[\"end\"]:\n",
    "                        for slope_dict in slopes:\n",
    "                            if slope_dict[\"name\"] == circle[\"name\"]:\n",
    "                                slope_dict[\"slopes\"].append(slopeprint)\n",
    "                                break\n",
    "                        # print(f\"DEBUG: {layer_name} at ecc {ecc} in {circle['name']} circle with slope {slopeprint:.3f}\")\n",
    "                        break\n",
    "\n",
    "                slope, intercept, r_value, p_value_fit, std_err = linregress(x_data, y_data)\n",
    "\n",
    "                \n",
    "                # Compute points on the line for plotting using the z-scored data\n",
    "                x_line = np.linspace(x_data.min(), x_data.max(), 100)\n",
    "                y_line = slope * x_line + intercept\n",
    "\n",
    "\n",
    "\n",
    "        for slope in slopes:\n",
    "            if len(slope[\"slopes\"]) > 0:\n",
    "                # print(f\"DEBUG: {layer_name} mean slope for {slope['name']} circle:\", np.mean(slope[\"slopes\"]))\n",
    "                slope[\"mean\"] = np.mean(slope[\"slopes\"])\n",
    "            else:\n",
    "                # print(f\"DEBUG: {layer_name} no slopes for {slope['name']} circle, setting mean to -1\")\n",
    "                slope[\"mean\"] = -1\n",
    "\n",
    "        # print(f\"DEBUG: {layer_name} slopes:\", [s[\"mean\"] for s in slopes])\n",
    "\n",
    "        layer_results_temporal[layer_name] = slopes\n",
    "        # print(f\"DEBUG: {layer_name} layer results:\", layer_results_temporal)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8488823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "from scipy.stats import zscore\n",
    "\n",
    "results = resultsY\n",
    "\n",
    "threshold = 1\n",
    "layers_to_plot = results.keys()\n",
    "\n",
    "range_start = 0\n",
    "range_end = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layer_results_nasal = {}\n",
    "\n",
    "\n",
    "\n",
    "with(plt.style.context(matplotx.styles.pacoty)):\n",
    "\n",
    "    for layer_name, (correlations, p_values) in results.items():\n",
    "        \n",
    "        circles = [\n",
    "\n",
    "        {\"name\": \"central\", \"start\": 0.0, \"end\": 0.5},\n",
    "        {\"name\": \"inner\", \"start\": 0.5, \"end\": 1.5},\n",
    "        {\"name\": \"outer\", \"start\": 1.5, \"end\": 3.0},\n",
    "        ]\n",
    "\n",
    "        slopes = [\n",
    "        {\"name\": \"central\", \"slopes\" : [], \"mean\": -1},\n",
    "\n",
    "        {\"name\": \"inner\", \"slopes\" : [],  \"mean\": -1},\n",
    "\n",
    "        {\"name\": \"outer\", \"slopes\" : [],  \"mean\": -1}\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "        if layer_name not in layers_to_plot:\n",
    "            continue\n",
    "\n",
    "        for ecc, corr, p_val in zip(eccs, correlations, p_values):\n",
    "            if ecc < range_start or ecc > range_end:\n",
    "                continue\n",
    "\n",
    "            if p_val < threshold:\n",
    "                # Retrieve the actual data for this layer and eccentricity\n",
    "                x_data, y_data = get_data_from_range(layer_name, (ecc - 0.05), (ecc + 0.05))\n",
    "                \n",
    "                # Z-score the data for regression\n",
    "                x_data_z = zscore(x_data)\n",
    "                y_data_z = zscore(y_data)\n",
    "                \n",
    "                # Fit a line via linregress on the z-scored data\n",
    "                slopeprint, intercept, r_value, p_value_fit, std_err = linregress(x_data_z, y_data_z)\n",
    "\n",
    "                for circle in circles:\n",
    "                    if ecc*MM_PER_DEGREE >= circle[\"start\"] and ecc*MM_PER_DEGREE < circle[\"end\"]:\n",
    "                        for slope_dict in slopes:\n",
    "                            if slope_dict[\"name\"] == circle[\"name\"]:\n",
    "                                slope_dict[\"slopes\"].append(slopeprint)\n",
    "                                break\n",
    "                        # print(f\"DEBUG: {layer_name} at ecc {ecc} in {circle['name']} circle with slope {slopeprint:.3f}\")\n",
    "                        break\n",
    "\n",
    "                slope, intercept, r_value, p_value_fit, std_err = linregress(x_data, y_data)\n",
    "\n",
    "                # r2scores[layer_name].append(r_value**2)\n",
    "                \n",
    "                # Compute points on the line for plotting using the z-scored data\n",
    "                x_line = np.linspace(x_data.min(), x_data.max(), 100)\n",
    "                y_line = slope * x_line + intercept\n",
    "\n",
    "                # print(f\"processing {layer_name} at ecc {ecc} with corr {corr:.3f}, p-value {p_val:.3f}, slope {slopeprint:.3f}\")\n",
    "\n",
    "                \n",
    "\n",
    "        for slope in slopes:\n",
    "            if len(slope[\"slopes\"]) > 0:\n",
    "                # print(f\"DEBUG: {layer_name} mean slope for {slope['name']} circle:\", np.mean(slope[\"slopes\"]))\n",
    "                slope[\"mean\"] = np.mean(slope[\"slopes\"])\n",
    "            else:\n",
    "                # print(f\"DEBUG: {layer_name} no slopes for {slope['name']} circle, setting mean to -1\")\n",
    "                slope[\"mean\"] = -1\n",
    "\n",
    "        # print(f\"DEBUG: {layer_name} slopes:\", [s[\"mean\"] for s in slopes])\n",
    "\n",
    "        layer_results_nasal[layer_name] = slopes\n",
    "        # print(f\"DEBUG: {layer_name} layer results:\", layer_results_nasal)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8785c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55a623b8",
   "metadata": {},
   "source": [
    "### Extra: Heatmap Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0463215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.patches import Rectangle, Polygon\n",
    "from matplotlib.collections import PolyCollection\n",
    "from typing import Dict, Tuple, List, Optional, Union\n",
    "import matplotx\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def plot_cross_heatmap(results_x: Dict[str, Tuple[np.ndarray]], \n",
    "                       results_y: Dict[str, Tuple[np.ndarray]], \n",
    "                       eccs: np.ndarray,\n",
    "                       layers_to_plot: List[str] | None = None,\n",
    "                       corr_name: str = 'Spearman',\n",
    "                       pv_threshold: float = 0.05,\n",
    "                       cmap: str = 'RdBu_r',\n",
    "                       vmin: float = -1.0,\n",
    "                       vmax: float = 1.0,\n",
    "                       figsize: Tuple[float, float] = (10, 10),\n",
    "                       bar_width: float = 1.5,\n",
    "                       save_path: str | None = None,\n",
    "                       dpi: int = 300,\n",
    "                       show_regions: bool = True,\n",
    "                       apply_style: bool = True,\n",
    "                       region_cmap: str = 'gray',\n",
    "                       region_alpha: float = 0.3,\n",
    "                       taper_style: str = 'linear',\n",
    "                       smoothing: Union[bool, str] = False,\n",
    "                       smoothing_sigma: float = 1.0,\n",
    "                       smoothing_window: int = 5):\n",
    "    \"\"\"\n",
    "    Plot correlation results as cross-shaped heatmaps with tapered center, one image for each layer.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_x : Dict with x-axis correlation results\n",
    "    results_y : Dict with y-axis correlation results\n",
    "    eccs : Array of eccentricities\n",
    "    layers_to_plot : List of layers to plot (if None, plot all)\n",
    "    corr_name : Name of correlation method\n",
    "    pv_threshold : P-value threshold for significance\n",
    "    cmap : Colormap name for correlation data (e.g., 'RdBu_r', 'coolwarm', 'seismic')\n",
    "    vmin, vmax : Color scale limits (defaults to -1.0, 1.0)\n",
    "    figsize : Figure size for each plot\n",
    "    bar_width : Width of the cross bars in eccentricity units\n",
    "    save_path : Base path to save figures (optional)\n",
    "    dpi : Resolution for saved figures\n",
    "    show_regions : Whether to show retinal region boundaries\n",
    "    apply_style : Whether to apply matplotx style\n",
    "    region_cmap : Colormap for background regions\n",
    "    region_alpha : Alpha transparency for region backgrounds\n",
    "    taper_style : Style of tapering ('linear', 'quadratic', 'exponential')\n",
    "    smoothing : False, True, 'gaussian', 'savgol', or 'moving_average'\n",
    "    smoothing_sigma : Standard deviation for Gaussian smoothing\n",
    "    smoothing_window : Window size for Savitzky-Golay or moving average smoothing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine which layers to plot\n",
    "    layers = layers_to_plot or list(results_x.keys())\n",
    "    \n",
    "    # Define retinal regions in degrees (assuming MM_PER_DEGREE = 0.3)\n",
    "    MM_PER_DEGREE = 0.3\n",
    "    \n",
    "    # Define regions with their order (for gradient coloring)\n",
    "    retinal_regions = [\n",
    "        {'name': 'Foveola', 'start': -0.175/MM_PER_DEGREE, 'end': 0.175/MM_PER_DEGREE, 'order': 0},\n",
    "        {'name': 'FAZ', 'start': -0.25/MM_PER_DEGREE, 'end': -0.175/MM_PER_DEGREE, 'order': 1},\n",
    "        {'name': 'FAZ', 'start': 0.175/MM_PER_DEGREE, 'end': 0.25/MM_PER_DEGREE, 'order': 1},\n",
    "        {'name': 'Fovea', 'start': -0.75/MM_PER_DEGREE, 'end': -0.25/MM_PER_DEGREE, 'order': 2},\n",
    "        {'name': 'Fovea', 'start': 0.25/MM_PER_DEGREE, 'end': 0.75/MM_PER_DEGREE, 'order': 2},\n",
    "        {'name': 'Parafovea', 'start': -1.25/MM_PER_DEGREE, 'end': -0.75/MM_PER_DEGREE, 'order': 3},\n",
    "        {'name': 'Parafovea', 'start': 0.75/MM_PER_DEGREE, 'end': 1.25/MM_PER_DEGREE, 'order': 3},\n",
    "        {'name': 'Perifovea', 'start': -10, 'end': -1.25/MM_PER_DEGREE, 'order': 4},\n",
    "        {'name': 'Perifovea', 'start': 1.25/MM_PER_DEGREE, 'end': 10, 'order': 4},\n",
    "    ]\n",
    "    \n",
    "    # Create a figure for each layer\n",
    "    for layer in layers:\n",
    "        if layer not in results_x or layer not in results_y:\n",
    "            print(f\"Skipping layer {layer} - not found in results\")\n",
    "            continue\n",
    "        \n",
    "        # Apply style context if requested\n",
    "        if apply_style:\n",
    "            with plt.style.context(matplotx.styles.pacoty):\n",
    "                fig, ax = plt.subplots(figsize=figsize, dpi=100)\n",
    "        else:\n",
    "            fig, ax = plt.subplots(figsize=figsize, dpi=100)\n",
    "        \n",
    "        # Extract correlation values and p-values\n",
    "        corr_x, pval_x = results_x[layer]\n",
    "        corr_y, pval_y = results_y[layer]\n",
    "        \n",
    "        # Apply smoothing if requested\n",
    "        if smoothing:\n",
    "            # Determine smoothing method\n",
    "            if smoothing is True or smoothing == 'gaussian':\n",
    "                # Gaussian smoothing\n",
    "                corr_x = gaussian_filter1d(corr_x, sigma=smoothing_sigma)\n",
    "                corr_y = gaussian_filter1d(corr_y, sigma=smoothing_sigma)\n",
    "                \n",
    "            elif smoothing == 'savgol':\n",
    "                # Savitzky-Golay filter (preserves features better)\n",
    "                # Ensure window is odd\n",
    "                window = smoothing_window if smoothing_window % 2 == 1 else smoothing_window + 1\n",
    "                # Ensure window is not larger than data\n",
    "                window = min(window, len(eccs) - 1)\n",
    "                if window >= 3:  # Minimum window size for savgol\n",
    "                    polyorder = min(3, window - 1)  # Polynomial order\n",
    "                    corr_x = savgol_filter(corr_x, window, polyorder)\n",
    "                    corr_y = savgol_filter(corr_y, window, polyorder)\n",
    "                \n",
    "            elif smoothing == 'moving_average':\n",
    "                # Simple moving average\n",
    "                kernel = np.ones(smoothing_window) / smoothing_window\n",
    "                # Pad the data to handle edges\n",
    "                pad_width = smoothing_window // 2\n",
    "                corr_x_padded = np.pad(corr_x, pad_width, mode='edge')\n",
    "                corr_y_padded = np.pad(corr_y, pad_width, mode='edge')\n",
    "                # Convolve and trim\n",
    "                corr_x = np.convolve(corr_x_padded, kernel, mode='valid')\n",
    "                corr_y = np.convolve(corr_y_padded, kernel, mode='valid')\n",
    "            \n",
    "            # Ensure correlations stay within [-1, 1] after smoothing\n",
    "            corr_x = np.clip(corr_x, -1, 1)\n",
    "            corr_y = np.clip(corr_y, -1, 1)\n",
    "        \n",
    "        # Create masked arrays based on p-value threshold\n",
    "        mask_x = pval_x > pv_threshold if pv_threshold is not None else np.zeros_like(pval_x, dtype=bool)\n",
    "        mask_y = pval_y > pv_threshold if pv_threshold is not None else np.zeros_like(pval_y, dtype=bool)\n",
    "        \n",
    "        # Set black background\n",
    "        ax.set_facecolor('black')\n",
    "        fig.patch.set_facecolor('black')\n",
    "        \n",
    "        # Get colormap and normalization\n",
    "        norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "        cmap_obj = plt.cm.get_cmap(cmap)\n",
    "        \n",
    "        # Create collections for horizontal and vertical bars\n",
    "        horizontal_patches = []\n",
    "        horizontal_colors = []\n",
    "        vertical_patches = []\n",
    "        vertical_colors = []\n",
    "        \n",
    "        # Define taper function\n",
    "        def get_taper_width(distance_from_center, max_width, taper_style='linear'):\n",
    "            \"\"\"Calculate bar width based on distance from center\"\"\"\n",
    "            if distance_from_center >= max_width/2:\n",
    "                return max_width\n",
    "            \n",
    "            # Normalize distance to 0-1 range\n",
    "            t = distance_from_center / (max_width/2)\n",
    "            \n",
    "            if taper_style == 'linear':\n",
    "                return max_width * t\n",
    "            elif taper_style == 'quadratic':\n",
    "                return max_width * (t ** 2)\n",
    "            elif taper_style == 'exponential':\n",
    "                return max_width * (1 - np.exp(-3 * t))\n",
    "            else:\n",
    "                return max_width * t\n",
    "        \n",
    "        # Create patches for each eccentricity position\n",
    "        for i, ecc in enumerate(eccs):\n",
    "            # Calculate the width at this eccentricity\n",
    "            distance_from_center = abs(ecc)\n",
    "            current_width = get_taper_width(distance_from_center, bar_width, taper_style)\n",
    "            \n",
    "            # Calculate exact boundaries to ensure no gaps\n",
    "            if i == 0:\n",
    "                left_bound = eccs[0] - (eccs[1] - eccs[0])/2\n",
    "            else:\n",
    "                left_bound = (eccs[i-1] + eccs[i])/2\n",
    "                \n",
    "            if i == len(eccs) - 1:\n",
    "                right_bound = eccs[-1] + (eccs[-1] - eccs[-2])/2\n",
    "            else:\n",
    "                right_bound = (eccs[i] + eccs[i+1])/2\n",
    "            \n",
    "            ecc_width = right_bound - left_bound\n",
    "            \n",
    "            # Create horizontal bar segment\n",
    "            if not mask_x[i]:\n",
    "                if distance_from_center >= bar_width/2:\n",
    "                    # Full width horizontal bar outside the taper region\n",
    "                    h_patch = Rectangle((left_bound, -bar_width/2), \n",
    "                                       ecc_width, bar_width)\n",
    "                else:\n",
    "                    # Tapered horizontal bar\n",
    "                    h_patch = Rectangle((left_bound, -current_width/2), \n",
    "                                       ecc_width, current_width)\n",
    "                \n",
    "                horizontal_patches.append(h_patch)\n",
    "                horizontal_colors.append(cmap_obj(norm(corr_x[i])))\n",
    "            \n",
    "            # Create vertical bar segment\n",
    "            if not mask_y[i]:\n",
    "                if distance_from_center >= bar_width/2:\n",
    "                    # Full width vertical bar outside the taper region\n",
    "                    v_patch = Rectangle((-bar_width/2, left_bound), \n",
    "                                       bar_width, ecc_width)\n",
    "                else:\n",
    "                    # Tapered vertical bar\n",
    "                    v_patch = Rectangle((-current_width/2, left_bound), \n",
    "                                       current_width, ecc_width)\n",
    "                \n",
    "                vertical_patches.append(v_patch)\n",
    "                vertical_colors.append(cmap_obj(norm(corr_y[i])))\n",
    "        \n",
    "        # Create patch collections\n",
    "        h_collection = plt.matplotlib.collections.PatchCollection(\n",
    "            horizontal_patches, facecolors=horizontal_colors, \n",
    "            edgecolors='none', zorder=2)\n",
    "        v_collection = plt.matplotlib.collections.PatchCollection(\n",
    "            vertical_patches, facecolors=vertical_colors, \n",
    "            edgecolors='none', zorder=2)\n",
    "        \n",
    "        # Add collections to axes\n",
    "        ax.add_collection(h_collection)\n",
    "        ax.add_collection(v_collection)\n",
    "        \n",
    "        # Add significance dimming overlay\n",
    "        significance_threshold = 0.05\n",
    "        sig_mask_x = pval_x <= significance_threshold\n",
    "        sig_mask_y = pval_y <= significance_threshold\n",
    "        \n",
    "        # Create dimming overlays for non-significant regions\n",
    "        for i, ecc in enumerate(eccs):\n",
    "            distance_from_center = abs(ecc)\n",
    "            current_width = get_taper_width(distance_from_center, bar_width, taper_style)\n",
    "            \n",
    "            # Calculate exact boundaries to match data patches\n",
    "            if i == 0:\n",
    "                left_bound = eccs[0] - (eccs[1] - eccs[0])/2\n",
    "            else:\n",
    "                left_bound = (eccs[i-1] + eccs[i])/2\n",
    "                \n",
    "            if i == len(eccs) - 1:\n",
    "                right_bound = eccs[-1] + (eccs[-1] - eccs[-2])/2\n",
    "            else:\n",
    "                right_bound = (eccs[i] + eccs[i+1])/2\n",
    "            \n",
    "            ecc_width = right_bound - left_bound\n",
    "            \n",
    "            # Horizontal dimming\n",
    "            if not sig_mask_x[i]:\n",
    "                if distance_from_center >= bar_width/2:\n",
    "                    dim_h = Rectangle((left_bound, -bar_width/2), \n",
    "                                     ecc_width, bar_width, \n",
    "                                     facecolor='black', alpha=0.4, zorder=3)\n",
    "                else:\n",
    "                    dim_h = Rectangle((left_bound, -current_width/2), \n",
    "                                     ecc_width, current_width,\n",
    "                                     facecolor='black', alpha=0.4, zorder=3)\n",
    "                ax.add_patch(dim_h)\n",
    "            \n",
    "            # Vertical dimming\n",
    "            if not sig_mask_y[i]:\n",
    "                if distance_from_center >= bar_width/2:\n",
    "                    dim_v = Rectangle((-bar_width/2, left_bound), \n",
    "                                     bar_width, ecc_width,\n",
    "                                     facecolor='black', alpha=0.4, zorder=3)\n",
    "                else:\n",
    "                    dim_v = Rectangle((-current_width/2, left_bound), \n",
    "                                     current_width, ecc_width,\n",
    "                                     facecolor='black', alpha=0.4, zorder=3)\n",
    "                ax.add_patch(dim_v)\n",
    "        \n",
    "        # Add circular region boundaries if requested\n",
    "        if show_regions:\n",
    "            # Define unique radii for region boundaries\n",
    "            region_radii = [\n",
    "                0.175/MM_PER_DEGREE,   # Foveola boundary\n",
    "                0.25/MM_PER_DEGREE,    # FAZ boundary\n",
    "                0.75/MM_PER_DEGREE,    # Fovea boundary\n",
    "                1.25/MM_PER_DEGREE,    # Parafovea boundary\n",
    "            ]\n",
    "            \n",
    "            # Draw white circles at each radius\n",
    "            for radius in region_radii:\n",
    "                circle = plt.Circle((0, 0), radius, \n",
    "                                   fill=False, \n",
    "                                   edgecolor='white', \n",
    "                                   linewidth=1.0, \n",
    "                                   alpha=0.7,\n",
    "                                   linestyle='--',\n",
    "                                   zorder=1)\n",
    "                ax.add_patch(circle)\n",
    "            \n",
    "            # Add region labels\n",
    "            label_positions = [\n",
    "                (0, 0.1/MM_PER_DEGREE, 'Foveola', 0.7),\n",
    "                (0, 0.21/MM_PER_DEGREE, 'FAZ', 0.7),\n",
    "                (0, 0.5/MM_PER_DEGREE, 'Fovea', 0.7),\n",
    "                (0, 1.0/MM_PER_DEGREE, 'Parafovea', 0.7),\n",
    "                (0, 3.0, 'Perifovea', 0.7),\n",
    "            ]\n",
    "            \n",
    "            for x, y, label, alpha in label_positions:\n",
    "                ax.text(x, y, label, \n",
    "                       color='white', \n",
    "                       fontsize=8, \n",
    "                       ha='center', \n",
    "                       va='bottom',\n",
    "                       alpha=alpha,\n",
    "                       zorder=1)\n",
    "        \n",
    "        # Set axis limits\n",
    "        max_ecc = eccs.max()\n",
    "        ax.set_xlim(-max_ecc, max_ecc)\n",
    "        ax.set_ylim(-max_ecc, max_ecc)\n",
    "        \n",
    "        # Add labels\n",
    "        ax.set_xlabel('Horizontal Meridian (X-axis) Eccentricity [°]', fontsize=12, color='white')\n",
    "        ax.set_ylabel('Vertical Meridian (Y-axis) Eccentricity [°]', fontsize=12, color='white')\n",
    "        \n",
    "        # Format layer name for title\n",
    "        layer_name = layer if layer != 'nonfit' else 'Non-fit'\n",
    "        \n",
    "        # Add smoothing info to title if applicable\n",
    "        title_parts = [f'{layer_name} - {corr_name} Correlation with Cone Density']\n",
    "        if smoothing:\n",
    "            if smoothing is True or smoothing == 'gaussian':\n",
    "                title_parts.append(f'(Gaussian smoothing, σ={smoothing_sigma})')\n",
    "            elif smoothing == 'savgol':\n",
    "                title_parts.append(f'(Savitzky-Golay smoothing, w={smoothing_window})')\n",
    "            elif smoothing == 'moving_average':\n",
    "                title_parts.append(f'(Moving average, w={smoothing_window})')\n",
    "        \n",
    "        ax.set_title(' '.join(title_parts), fontsize=14, pad=20, color='white')\n",
    "        \n",
    "        # Style axes\n",
    "        ax.tick_params(colors='white', which='both')\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_color('white')\n",
    "        \n",
    "        # Add grid\n",
    "        ax.grid(True, alpha=0.2, linestyle=':', linewidth=0.5)\n",
    "        \n",
    "        # Add colorbar\n",
    "        # Create a mappable for the colorbar\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap_obj, norm=norm)\n",
    "        sm.set_array([])\n",
    "        cbar = plt.colorbar(sm, ax=ax, orientation='vertical', pad=0.02, fraction=0.046)\n",
    "        cbar.set_label(f'{corr_name} Correlation Coefficient', fontsize=11, color='white')\n",
    "        cbar.ax.tick_params(labelsize=10, colors='white')\n",
    "        \n",
    "        # Make the plot square\n",
    "        ax.set_aspect('equal')\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save if path provided\n",
    "        if save_path:\n",
    "            smoothing_suffix = \"\"\n",
    "            if smoothing:\n",
    "                if smoothing is True or smoothing == 'gaussian':\n",
    "                    smoothing_suffix = f\"_gaussian_s{smoothing_sigma}\"\n",
    "                elif smoothing == 'savgol':\n",
    "                    smoothing_suffix = f\"_savgol_w{smoothing_window}\"\n",
    "                elif smoothing == 'moving_average':\n",
    "                    smoothing_suffix = f\"_movavg_w{smoothing_window}\"\n",
    "            \n",
    "            filename = f\"{save_path}_{layer}_cross_heatmap_tapered{smoothing_suffix}.png\"\n",
    "            fig.savefig(filename, dpi=dpi, bbox_inches='tight', facecolor='black')\n",
    "            print(f\"Saved: {filename}\")\n",
    "        \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Utility function to plot a single layer\n",
    "def plot_single_layer_cross(results_x: Dict[str, Tuple[np.ndarray]], \n",
    "                           results_y: Dict[str, Tuple[np.ndarray]], \n",
    "                           eccs: np.ndarray,\n",
    "                           layer: str,\n",
    "                           **kwargs):\n",
    "    \"\"\"\n",
    "    Convenience function to plot a single layer's cross heatmap.\n",
    "    \"\"\"\n",
    "    plot_cross_heatmap(results_x, results_y, eccs, layers_to_plot=[layer], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4c0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_heatmap(resultsX, resultsY, eccs, pv_threshold=1, smoothing='gaussian', smoothing_sigma=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad20725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aoslo12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
